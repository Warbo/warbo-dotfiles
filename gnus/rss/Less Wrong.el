;; -*- coding: utf-8-emacs; -*-
(setq nnrss-group-data '((63 (20954 42632 398298) "http://lesswrong.com/lw/hvw/how_i_became_more_ambitious/" "How I Became More Ambitious" nil "Thu, 04 Jul 2013 23:34:15 +0000" "Submitted by <a href=\"http://lesswrong.com/user/Swimmer963\">Swimmer963</a>
#
44 votes
#
<a href=\"http://lesswrong.com/lw/hvw/how_i_became_more_ambitious/#comments\">30 comments</a>
<div><p>Follow-up to <a href=\"/lw/9j1/how_i_ended_up_nonambitious/\">How I Ended Up Non-Ambitious</a></p>
<p>Living with yourself is a bit like having a preteen and watching them get taller; the changes happen so slowly that it's almost impossible to notice them, until you stumble across an old point of comparison and it becomes blindingly obvious. I hit that point a few days ago, while planning what I might want to talk about during an OkCupid date. My brain produced the following thought: \"well, if this topic comes up, it might sound like I'm trying to take over the world, and that's intimidating- Wait. What?\"</p>
<p>I'm not trying to take over the world. It sounds like a lot of work, and not my comparative advantage. If it seemed necessary, I would point out the problems that needed solving and delegate them to CFAR alumni with more domain-specific expertise than me.</p>
<p>However, I went back and reread the post linked at the beginning, and I no longer feel much kinship with that person. This is a change that happened maybe 25-50% deliberately, and the rest by drift, but I still changed my mind, so I will try to detail the particular changes, and what I think led to them. <a href=\"/lw/5sk/inferring_our_desires/\">Introspection is unreliable</a>, so I'll probably be at least 50% wrong, but what can you do?</p>
<h2><strong>1. Idealism versus practicality</strong></h2>
<p>I would still call myself practical, but I no longer think that this comes at the expense of idealism. Idealism is absolutely essential, if you want to have a world that changes because someone wanted it to, as opposed to just by drift. Lately in the rationalist/CFAR/LW community, there's been a lot of emphasis on <a href=\"/tag/agency/\">agency</a> and agentiness, which basically mean the ability to change the world and/or yourself deliberately, on purpose, through planned actions. This is hard. The first step is idealism-being able to imagine a state of affairs that is different and better. Then comes practicality, the part where you sit down and work hard and actually get something done.</p>
<p>It's still true that idealism without practicality doesn't get much done, and practicality without idealism can get a lot done, but it matters what problems you're working on, too. Are you being <a href=\"/lw/2p5/humans_are_not_automatically_strategic/\">strategic</a>? Are you even thinking, at all, about whether your actions are helping to accomplish your goals? One of the big things I've learned, a year and a half and two CFAR workshops later, is how automatic and easy this lack of strategy really is.</p>
<p>I had a limited sort of idealism in high school; I wanted to do work that was important and relevant; but I was lazy about it. I wanted someone to tell me what was important to be doing right now. Nursing seemed like an awesome solution. It still seems like a solution, but recently I've admitted to myself, with a painful twinge, that it might not be the best way to leverage my comparative advantage and help the most people. It's <a href=\"/lw/8j4/5second_level_case_study_value_of_information/\">worth spending a few minutes or hours</a> looking for interesting and important problems to work on.</p>
<p>I don't think I had the mental vocabulary to think that thought a year and a half ago. Some of the change comes from having dated an economics student. Come to think of it, I expect some of his general ambition rubbed off on me, too. The rest of the change comes from hanging out with the effective altruism and similar communities.</p>
<p>I'm still practical. I exercise, eat well, go to bed on time, work lots of hours, spend my money wisely, and maintain my social circle mostly on autopilot; it requires effort but not deliberate effort. I'm lucky to have this skill. But I no longer think it's a virtue over and above idealism. Practical idealists make the biggest difference, and they're pretty cool to hang out with. I want to be one when I grow up.</p>
<h2><span style=\"mso-ansi-language: EN-US;\">2. Fear of failure</span></h2>
<p><span style=\"mso-ansi-language: EN-US;\">Don't get me wrong. If there's one deep, gripping, soul-crushing terror in my life, one thing that gives me literal nightmares, it's failure. Making mistakes. Not being good enough. Et cetera. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">In the past few years, the main change has been admitting to myself that this terror doesn't make a lot of sense. First of all, it's completely miscalibrated. As Eliezer pointed out during a conversation on this, I don't fail at things very often. Far from being a success, this is likely a sign that the things I'm trying aren't nearly challenging enough. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">My threshold for what constitutes failure is also fairly low. I made a couple of embarrassing mistakes during my spring clinical. Some part of my brain is convinced that this equals <em>permanent </em>failure; I wasn't perfect during the placement, and I can't go back and change the past, thus I have failed. Forever. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I passed the clinical, wrote the provincial exam (results aren't in but I'm >99% confident I passed), and I'm currently working in the intensive care unit, which has been my dream since I was about fifteen. The part of my brain that keeps telling me I failed permanently obviously isn't saying anything useful. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I think 'embarrassing' is a keyword here. The first thing I thought, on the several occasions that I made mistakes, was \"oh my god did I just kill someone... Phew, no, no harm done.\" The second thought was \"oh my god, my preceptor will think I'm stupid forever and she'll never respect me and no one wants me around, I'm not good enough...\" This line of thought never goes anywhere good. It says something about me, though, that \"I'm not good enough\" is very directly connected to people wanting me around, to belonging somewhere. For several personality-formative years of my life, people <em>didn't </em>want me around. Probably for good reason; my ten-year-old self was prickly and socially inept and miserable. I think a lot of my determination not to seek status comes from the \"uncool kids trying to be cool are pathetic\" meme that was so rampant when I was in sixth grade. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">Oh, and then there's the traumatic swim team experience. Somewhere, in a part of my brain where I don't go very often nowadays, there a bottomless whirlpool of powerless rage and despair around the phrase \"no matter how hard I try, I'll never be good enough.\" So when I make an embarrassing mistake, my ten-year-old self is screaming at me \"no wonder everyone hates you!\" and my fourteen-year-old self is sadly muttering that \"you know, maybe you just don't have enough natural talent,\" and none of it is at all useful. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">The thing about those phrases is that they refer to complex and value-laden concepts, in a way that makes them seem like innate attributes, Ã  la <a href=\"/lw/hz/correspondence_bias/\">Fundamental Attribution Error</a>. \"Not good enough\" isn't a yes-or-no attribute of a person; it's a <a href=\"/lw/td/magical_categories/\">magical category</a> that only sounds simple because it's a three-word phrase. I've gotten somewhat better at propagating this to my emotional self. Slightly. It's a work in progress.</span></p>
<p><span style=\"mso-ansi-language: EN-US;\">During a conversation about this with <a href=\"http://annasalamon.com/\">Anna Salamon</a>, she noted that she likes to approach her own emotions and ask them what they want. It sounds weird, but it's helpful. \"Dear crushing sense of despair and unworthiness, what do you want? ...Oh, you're worried that you're going to end up an outcast from your tribe and starve to death in the wilderness because you accidentally gave an extra dose of digoxin? You want to signal remorse and regret and make sure everyone knows you're taking your failure seriously so that maybe they'll forgive you? Thank you for trying to protect me. But really, you don't need to worry about the starving-outcast thing. No one was harmed and no one is mad at you personally. Your friends and family couldn't care less. This mistake is data, but it's just as much data about the environment as it is about your attributes. These hand-copied medication records are the perfect medium for human error. Instead of signalling remorse, let's put some mental energy into getting rid of the environmental conditions that led to this mistake.\"</span></p>
<p><a href=\"http://en.wikipedia.org/wiki/Rejection_Therapy\">Rejection therapy</a> and having a general CoZE [Comfort Zone Expansion] mindset helped remove some of the sting of \"but I'll look stupid if I try something too hard and fail at it!\" I still worry about the pain of future embarrassment, but I'm more likely to point out to myself that it's not a valid objection and I should do X anyway. Making \"<a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">I want to become stronger</a>\" an explicit motto is new to the last year and a half, too, and helps by giving me ammunition for why potential embarrassment isn't a reason not to do something.</p>
<p>In conclusion: failure still sucks. I'm a perfectionist. But I failed in a lot of small ways during my <a href=\"/lw/gnv/learning_critical_thinking_a_personal_example/\">spring clinical</a>, and passed/got a job anyway, which seems to have helped me propagate to my emotional self that<em> it's okay to try hard things</em>, where I'm almost certain to make mistakes, because mistakes don't equal instant damnation and hatred from all of my friends.</p>
<h2><span style=\"mso-ansi-language: EN-US;\"><strong>3. The morality of ambition</strong></span></h2>
<p><span style=\"mso-ansi-language: EN-US;\">While I was in San Francisco a month ago, volunteering at the CFAR workshop and generally spending my time surrounded by smart, passionate, and ambitious people (thus convincing my emotional system that this is normal and okay), I had a conversation with Eliezer. He asked me to list ten areas where I had a comparative advantage; ten things in which I was above average. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">This was a lot more painful than it had any reason to be. After bouncing off various poorly-formed objections in my mind, I said to myself \"you know, having trouble admitting what you're good at doesn't make you virtuous.\" This was painful; losing a source of feeling-virtuous always is. But it was helpful. Yeah, talking all the time about how awesome you are at X, Y, Z makes you a bit of a bore. People might even avoid you (oh! the horror!). However, this doesn't mean that blocking even the<em> thought </em>of being above average makes you a good person. In fact, it's counterproductive. How are you supposed to know what problems you're capable of solving in the world if you can't be honest with yourself about your capabilities? </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">This conversation helped. (Even if some of the effect was \"high status person says X -> I believe X,\" who cares? I endorsed myself changing my mind about this a year and a half ago. It's about time.)</span></p>
<p><span style=\"mso-ansi-language: EN-US;\"><a href=\"http://hpmor.com/\">HPMOR</a> helped, too; specifically, the idea that there are four houses which have different positive qualities. Slytherins are demonized in canon, but in HPMOR their skills are recognized as essential. I can easily recognize the Ravenclaw and Hufflepuff and even the Gryffindor in myself, but not much of Slytherin. Having a word for the ambition-cunning-strategic concept cluster is helpful. I can ask myself \"now what would a Slytherin do with this information:?\" I can think thoughts that feel very un-virtuous. \"I'm young and prettier than average. What's a Slytherin way to use this... Oh, I suppose I can leverage it into a comparative advantage for getting high-status men to pay attention to me long enough for me to explain the merits of an idea I have.\" This thought feels yuck, but the universe doesn't explode. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">Probably the biggest factor was going to the CFAR workshops in the first place. Not from any of the curriculum, particularly, although the mindset of goal factoring helped me to realize that the mental action of \"feeling unvirtuous for thinking in ambitious or calculating ways\" wasn't accomplishing anything I wanted. Mostly the change came from social normalization, from hanging out with people who talked openly about their strengths and weaknesses, and no one got shunned. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">[Silly plan for taking over the world: Arrange to meet high status-people and offer to give their children swimming lessons. Gain their trust. Proceed from there.]</span></p>
<h2><span style=\"mso-ansi-language: EN-US;\"><strong>4. Laziness</strong></span></h2>
<p><span style=\"mso-ansi-language: EN-US;\">Nope. Still lazy. If anything, akrasia and procrastination are more of a problem now that I'm trying to do harder things more deliberately. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I've been keeping written goals for about a year now. This means I actually notice when I don't accomplish them. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I use Remember the Milk as a GTD system, and some other productivity/organization software (rescuetime, Mint.com, etc). I finally switched to Gmail, where I can use Boomerang and other useful tools. My current openness to trying new organization methods is high. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">My general interest in <a href=\"/lw/5a5/no_seriously_just_try_it/\">trying things</a> is higher, mainly because I have lots of community-endorsed-warm-fuzzies positive affect around that phrase. I want to be someone who's open to new experiences; I've had enough new experiences to realize how exhilarating they can be.Â </span></p>
<h2><span style=\"mso-ansi-language: EN-US;\"><strong>Conclusion</strong></span></h2>
<p><span style=\"mso-ansi-language: EN-US;\">I now have a wider range of potentially high-value personal projects ongoing. I now have an explicit goal of being well-known for non-fiction writing, probably in a blog form, in the next five years. (Do I have enough interesting things to say to make this a reality? We'll see. Is this goal vague? Yes. Working on it. I used to reject goals if they weren't utterly concrete, but even vague goals are something to build on). </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I'm more explicit with myself about what I want from CFAR curriculum skills. (The general problem of critical thinking in nursing? Solvable! Why not?) </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I think I've finally admitted to myself that \"well, I'll just live in a cozy little house near my parents and work in the ICU and raise kids for the next forty years\" might not be particularly virtuous <em>or </em>fun. There are things I would prefer to be different in the world, even if I can only completely specify a few of them. There are exciting scary opportunities happening all the time. I'm lucky enough to belong to a community of people that can help me find them. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I don't have plans for much beyond the next year. But here's to the next decade being interesting!<br></span></p></div>
<a href=\"http://lesswrong.com/lw/hvw/how_i_became_more_ambitious/#comments\">30 comments</a>
" nil nil "a93f44ba1a28e96adb9ef4981ae3000b") (62 (20954 39855 404010) "http://lesswrong.com/lw/hvw/how_i_became_more_ambitious/" "How I Became More Ambitious" nil "Thu, 04 Jul 2013 23:34:15 +0000" "Submitted by <a href=\"http://lesswrong.com/user/Swimmer963\">Swimmer963</a>
#
44 votes
#
<a href=\"http://lesswrong.com/lw/hvw/how_i_became_more_ambitious/#comments\">29 comments</a>
<div><p>Follow-up to <a href=\"/lw/9j1/how_i_ended_up_nonambitious/\">How I Ended Up Non-Ambitious</a></p>
<p>Living with yourself is a bit like having a preteen and watching them get taller; the changes happen so slowly that it's almost impossible to notice them, until you stumble across an old point of comparison and it becomes blindingly obvious. I hit that point a few days ago, while planning what I might want to talk about during an OkCupid date. My brain produced the following thought: \"well, if this topic comes up, it might sound like I'm trying to take over the world, and that's intimidating- Wait. What?\"</p>
<p>I'm not trying to take over the world. It sounds like a lot of work, and not my comparative advantage. If it seemed necessary, I would point out the problems that needed solving and delegate them to CFAR alumni with more domain-specific expertise than me.</p>
<p>However, I went back and reread the post linked at the beginning, and I no longer feel much kinship with that person. This is a change that happened maybe 25-50% deliberately, and the rest by drift, but I still changed my mind, so I will try to detail the particular changes, and what I think led to them. <a href=\"/lw/5sk/inferring_our_desires/\">Introspection is unreliable</a>, so I'll probably be at least 50% wrong, but what can you do?</p>
<h2><strong>1. Idealism versus practicality</strong></h2>
<p>I would still call myself practical, but I no longer think that this comes at the expense of idealism. Idealism is absolutely essential, if you want to have a world that changes because someone wanted it to, as opposed to just by drift. Lately in the rationalist/CFAR/LW community, there's been a lot of emphasis on <a href=\"/tag/agency/\">agency</a> and agentiness, which basically mean the ability to change the world and/or yourself deliberately, on purpose, through planned actions. This is hard. The first step is idealism-being able to imagine a state of affairs that is different and better. Then comes practicality, the part where you sit down and work hard and actually get something done.</p>
<p>It's still true that idealism without practicality doesn't get much done, and practicality without idealism can get a lot done, but it matters what problems you're working on, too. Are you being <a href=\"/lw/2p5/humans_are_not_automatically_strategic/\">strategic</a>? Are you even thinking, at all, about whether your actions are helping to accomplish your goals? One of the big things I've learned, a year and a half and two CFAR workshops later, is how automatic and easy this lack of strategy really is.</p>
<p>I had a limited sort of idealism in high school; I wanted to do work that was important and relevant; but I was lazy about it. I wanted someone to tell me what was important to be doing right now. Nursing seemed like an awesome solution. It still seems like a solution, but recently I've admitted to myself, with a painful twinge, that it might not be the best way to leverage my comparative advantage and help the most people. It's <a href=\"/lw/8j4/5second_level_case_study_value_of_information/\">worth spending a few minutes or hours</a> looking for interesting and important problems to work on.</p>
<p>I don't think I had the mental vocabulary to think that thought a year and a half ago. Some of the change comes from having dated an economics student. Come to think of it, I expect some of his general ambition rubbed off on me, too. The rest of the change comes from hanging out with the effective altruism and similar communities.</p>
<p>I'm still practical. I exercise, eat well, go to bed on time, work lots of hours, spend my money wisely, and maintain my social circle mostly on autopilot; it requires effort but not deliberate effort. I'm lucky to have this skill. But I no longer think it's a virtue over and above idealism. Practical idealists make the biggest difference, and they're pretty cool to hang out with. I want to be one when I grow up.</p>
<h2><span style=\"mso-ansi-language: EN-US;\">2. Fear of failure</span></h2>
<p><span style=\"mso-ansi-language: EN-US;\">Don't get me wrong. If there's one deep, gripping, soul-crushing terror in my life, one thing that gives me literal nightmares, it's failure. Making mistakes. Not being good enough. Et cetera. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">In the past few years, the main change has been admitting to myself that this terror doesn't make a lot of sense. First of all, it's completely miscalibrated. As Eliezer pointed out during a conversation on this, I don't fail at things very often. Far from being a success, this is likely a sign that the things I'm trying aren't nearly challenging enough. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">My threshold for what constitutes failure is also fairly low. I made a couple of embarrassing mistakes during my spring clinical. Some part of my brain is convinced that this equals <em>permanent </em>failure; I wasn't perfect during the placement, and I can't go back and change the past, thus I have failed. Forever. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I passed the clinical, wrote the provincial exam (results aren't in but I'm >99% confident I passed), and I'm currently working in the intensive care unit, which has been my dream since I was about fifteen. The part of my brain that keeps telling me I failed permanently obviously isn't saying anything useful. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I think 'embarrassing' is a keyword here. The first thing I thought, on the several occasions that I made mistakes, was \"oh my god did I just kill someone... Phew, no, no harm done.\" The second thought was \"oh my god, my preceptor will think I'm stupid forever and she'll never respect me and no one wants me around, I'm not good enough...\" This line of thought never goes anywhere good. It says something about me, though, that \"I'm not good enough\" is very directly connected to people wanting me around, to belonging somewhere. For several personality-formative years of my life, people <em>didn't </em>want me around. Probably for good reason; my ten-year-old self was prickly and socially inept and miserable. I think a lot of my determination not to seek status comes from the \"uncool kids trying to be cool are pathetic\" meme that was so rampant when I was in sixth grade. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">Oh, and then there's the traumatic swim team experience. Somewhere, in a part of my brain where I don't go very often nowadays, there a bottomless whirlpool of powerless rage and despair around the phrase \"no matter how hard I try, I'll never be good enough.\" So when I make an embarrassing mistake, my ten-year-old self is screaming at me \"no wonder everyone hates you!\" and my fourteen-year-old self is sadly muttering that \"you know, maybe you just don't have enough natural talent,\" and none of it is at all useful. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">The thing about those phrases is that they refer to complex and value-laden concepts, in a way that makes them seem like innate attributes, Ã  la <a href=\"/lw/hz/correspondence_bias/\">Fundamental Attribution Error</a>. \"Not good enough\" isn't a yes-or-no attribute of a person; it's a <a href=\"/lw/td/magical_categories/\">magical category</a> that only sounds simple because it's a three-word phrase. I've gotten somewhat better at propagating this to my emotional self. Slightly. It's a work in progress.</span></p>
<p><span style=\"mso-ansi-language: EN-US;\">During a conversation about this with <a href=\"http://annasalamon.com/\">Anna Salamon</a>, she noted that she likes to approach her own emotions and ask them what they want. It sounds weird, but it's helpful. \"Dear crushing sense of despair and unworthiness, what do you want? ...Oh, you're worried that you're going to end up an outcast from your tribe and starve to death in the wilderness because you accidentally gave an extra dose of digoxin? You want to signal remorse and regret and make sure everyone knows you're taking your failure seriously so that maybe they'll forgive you? Thank you for trying to protect me. But really, you don't need to worry about the starving-outcast thing. No one was harmed and no one is mad at you personally. Your friends and family couldn't care less. This mistake is data, but it's just as much data about the environment as it is about your attributes. These hand-copied medication records are the perfect medium for human error. Instead of signalling remorse, let's put some mental energy into getting rid of the environmental conditions that led to this mistake.\"</span></p>
<p><a href=\"http://en.wikipedia.org/wiki/Rejection_Therapy\">Rejection therapy</a> and having a general CoZE [Comfort Zone Expansion] mindset helped remove some of the sting of \"but I'll look stupid if I try something too hard and fail at it!\" I still worry about the pain of future embarrassment, but I'm more likely to point out to myself that it's not a valid objection and I should do X anyway. Making \"<a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">I want to become stronger</a>\" an explicit motto is new to the last year and a half, too, and helps by giving me ammunition for why potential embarrassment isn't a reason not to do something.</p>
<p>In conclusion: failure still sucks. I'm a perfectionist. But I failed in a lot of small ways during my <a href=\"/lw/gnv/learning_critical_thinking_a_personal_example/\">spring clinical</a>, and passed/got a job anyway, which seems to have helped me propagate to my emotional self that<em> it's okay to try hard things</em>, where I'm almost certain to make mistakes, because mistakes don't equal instant damnation and hatred from all of my friends.</p>
<h2><span style=\"mso-ansi-language: EN-US;\"><strong>3. The morality of ambition</strong></span></h2>
<p><span style=\"mso-ansi-language: EN-US;\">While I was in San Francisco a month ago, volunteering at the CFAR workshop and generally spending my time surrounded by smart, passionate, and ambitious people (thus convincing my emotional system that this is normal and okay), I had a conversation with Eliezer. He asked me to list ten areas where I had a comparative advantage; ten things in which I was above average. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">This was a lot more painful than it had any reason to be. After bouncing off various poorly-formed objections in my mind, I said to myself \"you know, having trouble admitting what you're good at doesn't make you virtuous.\" This was painful; losing a source of feeling-virtuous always is. But it was helpful. Yeah, talking all the time about how awesome you are at X, Y, Z makes you a bit of a bore. People might even avoid you (oh! the horror!). However, this doesn't mean that blocking even the<em> thought </em>of being above average makes you a good person. In fact, it's counterproductive. How are you supposed to know what problems you're capable of solving in the world if you can't be honest with yourself about your capabilities? </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">This conversation helped. (Even if some of the effect was \"high status person says X -> I believe X,\" who cares? I endorsed myself changing my mind about this a year and a half ago. It's about time.)</span></p>
<p><span style=\"mso-ansi-language: EN-US;\"><a href=\"http://hpmor.com/\">HPMOR</a> helped, too; specifically, the idea that there are four houses which have different positive qualities. Slytherins are demonized in canon, but in HPMOR their skills are recognized as essential. I can easily recognize the Ravenclaw and Hufflepuff and even the Gryffindor in myself, but not much of Slytherin. Having a word for the ambition-cunning-strategic concept cluster is helpful. I can ask myself \"now what would a Slytherin do with this information:?\" I can think thoughts that feel very un-virtuous. \"I'm young and prettier than average. What's a Slytherin way to use this... Oh, I suppose I can leverage it into a comparative advantage for getting high-status men to pay attention to me long enough for me to explain the merits of an idea I have.\" This thought feels yuck, but the universe doesn't explode. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">Probably the biggest factor was going to the CFAR workshops in the first place. Not from any of the curriculum, particularly, although the mindset of goal factoring helped me to realize that the mental action of \"feeling unvirtuous for thinking in ambitious or calculating ways\" wasn't accomplishing anything I wanted. Mostly the change came from social normalization, from hanging out with people who talked openly about their strengths and weaknesses, and no one got shunned. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">[Silly plan for taking over the world: Arrange to meet high status-people and offer to give their children swimming lessons. Gain their trust. Proceed from there.]</span></p>
<h2><span style=\"mso-ansi-language: EN-US;\"><strong>4. Laziness</strong></span></h2>
<p><span style=\"mso-ansi-language: EN-US;\">Nope. Still lazy. If anything, akrasia and procrastination are more of a problem now that I'm trying to do harder things more deliberately. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I've been keeping written goals for about a year now. This means I actually notice when I don't accomplish them. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I use Remember the Milk as a GTD system, and some other productivity/organization software (rescuetime, Mint.com, etc). I finally switched to Gmail, where I can use Boomerang and other useful tools. My current openness to trying new organization methods is high. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">My general interest in <a href=\"/lw/5a5/no_seriously_just_try_it/\">trying things</a> is higher, mainly because I have lots of community-endorsed-warm-fuzzies positive affect around that phrase. I want to be someone who's open to new experiences; I've had enough new experiences to realize how exhilarating they can be.Â </span></p>
<h2><span style=\"mso-ansi-language: EN-US;\"><strong>Conclusion</strong></span></h2>
<p><span style=\"mso-ansi-language: EN-US;\">I now have a wider range of potentially high-value personal projects ongoing. I now have an explicit goal of being well-known for non-fiction writing, probably in a blog form, in the next five years. (Do I have enough interesting things to say to make this a reality? We'll see. Is this goal vague? Yes. Working on it. I used to reject goals if they weren't utterly concrete, but even vague goals are something to build on). </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I'm more explicit with myself about what I want from CFAR curriculum skills. (The general problem of critical thinking in nursing? Solvable! Why not?) </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I think I've finally admitted to myself that \"well, I'll just live in a cozy little house near my parents and work in the ICU and raise kids for the next forty years\" might not be particularly virtuous <em>or </em>fun. There are things I would prefer to be different in the world, even if I can only completely specify a few of them. There are exciting scary opportunities happening all the time. I'm lucky enough to belong to a community of people that can help me find them. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I don't have plans for much beyond the next year. But here's to the next decade being interesting!<br></span></p></div>
<a href=\"http://lesswrong.com/lw/hvw/how_i_became_more_ambitious/#comments\">29 comments</a>
" nil nil "1aa06ad4df3a05aca948c0bb6fe6c8d5") (61 (20954 39122 534571) "http://lesswrong.com/lw/hvw/how_i_became_more_ambitious/" "How I Became More Ambitious" nil "Thu, 04 Jul 2013 23:34:15 +0000" "Submitted by <a href=\"http://lesswrong.com/user/Swimmer963\">Swimmer963</a>
#
43 votes
#
<a href=\"http://lesswrong.com/lw/hvw/how_i_became_more_ambitious/#comments\">29 comments</a>
<div><p>Follow-up to <a href=\"/lw/9j1/how_i_ended_up_nonambitious/\">How I Ended Up Non-Ambitious</a></p>
<p>Living with yourself is a bit like having a preteen and watching them get taller; the changes happen so slowly that it's almost impossible to notice them, until you stumble across an old point of comparison and it becomes blindingly obvious. I hit that point a few days ago, while planning what I might want to talk about during an OkCupid date. My brain produced the following thought: \"well, if this topic comes up, it might sound like I'm trying to take over the world, and that's intimidating- Wait. What?\"</p>
<p>I'm not trying to take over the world. It sounds like a lot of work, and not my comparative advantage. If it seemed necessary, I would point out the problems that needed solving and delegate them to CFAR alumni with more domain-specific expertise than me.</p>
<p>However, I went back and reread the post linked at the beginning, and I no longer feel much kinship with that person. This is a change that happened maybe 25-50% deliberately, and the rest by drift, but I still changed my mind, so I will try to detail the particular changes, and what I think led to them. <a href=\"/lw/5sk/inferring_our_desires/\">Introspection is unreliable</a>, so I'll probably be at least 50% wrong, but what can you do?</p>
<h2><strong>1. Idealism versus practicality</strong></h2>
<p>I would still call myself practical, but I no longer think that this comes at the expense of idealism. Idealism is absolutely essential, if you want to have a world that changes because someone wanted it to, as opposed to just by drift. Lately in the rationalist/CFAR/LW community, there's been a lot of emphasis on <a href=\"/tag/agency/\">agency</a> and agentiness, which basically mean the ability to change the world and/or yourself deliberately, on purpose, through planned actions. This is hard. The first step is idealism-being able to imagine a state of affairs that is different and better. Then comes practicality, the part where you sit down and work hard and actually get something done.</p>
<p>It's still true that idealism without practicality doesn't get much done, and practicality without idealism can get a lot done, but it matters what problems you're working on, too. Are you being <a href=\"/lw/2p5/humans_are_not_automatically_strategic/\">strategic</a>? Are you even thinking, at all, about whether your actions are helping to accomplish your goals? One of the big things I've learned, a year and a half and two CFAR workshops later, is how automatic and easy this lack of strategy really is.</p>
<p>I had a limited sort of idealism in high school; I wanted to do work that was important and relevant; but I was lazy about it. I wanted someone to tell me what was important to be doing right now. Nursing seemed like an awesome solution. It still seems like a solution, but recently I've admitted to myself, with a painful twinge, that it might not be the best way to leverage my comparative advantage and help the most people. It's <a href=\"/lw/8j4/5second_level_case_study_value_of_information/\">worth spending a few minutes or hours</a> looking for interesting and important problems to work on.</p>
<p>I don't think I had the mental vocabulary to think that thought a year and a half ago. Some of the change comes from having dated an economics student. Come to think of it, I expect some of his general ambition rubbed off on me, too. The rest of the change comes from hanging out with the effective altruism and similar communities.</p>
<p>I'm still practical. I exercise, eat well, go to bed on time, work lots of hours, spend my money wisely, and maintain my social circle mostly on autopilot; it requires effort but not deliberate effort. I'm lucky to have this skill. But I no longer think it's a virtue over and above idealism. Practical idealists make the biggest difference, and they're pretty cool to hang out with. I want to be one when I grow up.</p>
<h2><span style=\"mso-ansi-language: EN-US;\">2. Fear of failure</span></h2>
<p><span style=\"mso-ansi-language: EN-US;\">Don't get me wrong. If there's one deep, gripping, soul-crushing terror in my life, one thing that gives me literal nightmares, it's failure. Making mistakes. Not being good enough. Et cetera. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">In the past few years, the main change has been admitting to myself that this terror doesn't make a lot of sense. First of all, it's completely miscalibrated. As Eliezer pointed out during a conversation on this, I don't fail at things very often. Far from being a success, this is likely a sign that the things I'm trying aren't nearly challenging enough. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">My threshold for what constitutes failure is also fairly low. I made a couple of embarrassing mistakes during my spring clinical. Some part of my brain is convinced that this equals <em>permanent </em>failure; I wasn't perfect during the placement, and I can't go back and change the past, thus I have failed. Forever. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I passed the clinical, wrote the provincial exam (results aren't in but I'm >99% confident I passed), and I'm currently working in the intensive care unit, which has been my dream since I was about fifteen. The part of my brain that keeps telling me I failed permanently obviously isn't saying anything useful. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I think 'embarrassing' is a keyword here. The first thing I thought, on the several occasions that I made mistakes, was \"oh my god did I just kill someone... Phew, no, no harm done.\" The second thought was \"oh my god, my preceptor will think I'm stupid forever and she'll never respect me and no one wants me around, I'm not good enough...\" This line of thought never goes anywhere good. It says something about me, though, that \"I'm not good enough\" is very directly connected to people wanting me around, to belonging somewhere. For several personality-formative years of my life, people <em>didn't </em>want me around. Probably for good reason; my ten-year-old self was prickly and socially inept and miserable. I think a lot of my determination not to seek status comes from the \"uncool kids trying to be cool are pathetic\" meme that was so rampant when I was in sixth grade. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">Oh, and then there's the traumatic swim team experience. Somewhere, in a part of my brain where I don't go very often nowadays, there a bottomless whirlpool of powerless rage and despair around the phrase \"no matter how hard I try, I'll never be good enough.\" So when I make an embarrassing mistake, my ten-year-old self is screaming at me \"no wonder everyone hates you!\" and my fourteen-year-old self is sadly muttering that \"you know, maybe you just don't have enough natural talent,\" and none of it is at all useful. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">The thing about those phrases is that they refer to complex and value-laden concepts, in a way that makes them seem like innate attributes, Ã  la <a href=\"/lw/hz/correspondence_bias/\">Fundamental Attribution Error</a>. \"Not good enough\" isn't a yes-or-no attribute of a person; it's a <a href=\"/lw/td/magical_categories/\">magical category</a> that only sounds simple because it's a three-word phrase. I've gotten somewhat better at propagating this to my emotional self. Slightly. It's a work in progress.</span></p>
<p><span style=\"mso-ansi-language: EN-US;\">During a conversation about this with <a href=\"http://annasalamon.com/\">Anna Salamon</a>, she noted that she likes to approach her own emotions and ask them what they want. It sounds weird, but it's helpful. \"Dear crushing sense of despair and unworthiness, what do you want? ...Oh, you're worried that you're going to end up an outcast from your tribe and starve to death in the wilderness because you accidentally gave an extra dose of digoxin? You want to signal remorse and regret and make sure everyone knows you're taking your failure seriously so that maybe they'll forgive you? Thank you for trying to protect me. But really, you don't need to worry about the starving-outcast thing. No one was harmed and no one is mad at you personally. Your friends and family couldn't care less. This mistake is data, but it's just as much data about the environment as it is about your attributes. These hand-copied medication records are the perfect medium for human error. Instead of signalling remorse, let's put some mental energy into getting rid of the environmental conditions that led to this mistake.\"</span></p>
<p><a href=\"http://en.wikipedia.org/wiki/Rejection_Therapy\">Rejection therapy</a> and having a general CoZE [Comfort Zone Expansion] mindset helped remove some of the sting of \"but I'll look stupid if I try something too hard and fail at it!\" I still worry about the pain of future embarrassment, but I'm more likely to point out to myself that it's not a valid objection and I should do X anyway. Making \"<a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">I want to become stronger</a>\" an explicit motto is new to the last year and a half, too, and helps by giving me ammunition for why potential embarrassment isn't a reason not to do something.</p>
<p>In conclusion: failure still sucks. I'm a perfectionist. But I failed in a lot of small ways during my <a href=\"/lw/gnv/learning_critical_thinking_a_personal_example/\">spring clinical</a>, and passed/got a job anyway, which seems to have helped me propagate to my emotional self that<em> it's okay to try hard things</em>, where I'm almost certain to make mistakes, because mistakes don't equal instant damnation and hatred from all of my friends.</p>
<h2><span style=\"mso-ansi-language: EN-US;\"><strong>3. The morality of ambition</strong></span></h2>
<p><span style=\"mso-ansi-language: EN-US;\">While I was in San Francisco a month ago, volunteering at the CFAR workshop and generally spending my time surrounded by smart, passionate, and ambitious people (thus convincing my emotional system that this is normal and okay), I had a conversation with Eliezer. He asked me to list ten areas where I had a comparative advantage; ten things in which I was above average. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">This was a lot more painful than it had any reason to be. After bouncing off various poorly-formed objections in my mind, I said to myself \"you know, having trouble admitting what you're good at doesn't make you virtuous.\" This was painful; losing a source of feeling-virtuous always is. But it was helpful. Yeah, talking all the time about how awesome you are at X, Y, Z makes you a bit of a bore. People might even avoid you (oh! the horror!). However, this doesn't mean that blocking even the<em> thought </em>of being above average makes you a good person. In fact, it's counterproductive. How are you supposed to know what problems you're capable of solving in the world if you can't be honest with yourself about your capabilities? </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">This conversation helped. (Even if some of the effect was \"high status person says X -> I believe X,\" who cares? I endorsed myself changing my mind about this a year and a half ago. It's about time.)</span></p>
<p><span style=\"mso-ansi-language: EN-US;\"><a href=\"http://hpmor.com/\">HPMOR</a> helped, too; specifically, the idea that there are four houses which have different positive qualities. Slytherins are demonized in canon, but in HPMOR their skills are recognized as essential. I can easily recognize the Ravenclaw and Hufflepuff and even the Gryffindor in myself, but not much of Slytherin. Having a word for the ambition-cunning-strategic concept cluster is helpful. I can ask myself \"now what would a Slytherin do with this information:?\" I can think thoughts that feel very un-virtuous. \"I'm young and prettier than average. What's a Slytherin way to use this... Oh, I suppose I can leverage it into a comparative advantage for getting high-status men to pay attention to me long enough for me to explain the merits of an idea I have.\" This thought feels yuck, but the universe doesn't explode. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">Probably the biggest factor was going to the CFAR workshops in the first place. Not from any of the curriculum, particularly, although the mindset of goal factoring helped me to realize that the mental action of \"feeling unvirtuous for thinking in ambitious or calculating ways\" wasn't accomplishing anything I wanted. Mostly the change came from social normalization, from hanging out with people who talked openly about their strengths and weaknesses, and no one got shunned. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">[Silly plan for taking over the world: Arrange to meet high status-people and offer to give their children swimming lessons. Gain their trust. Proceed from there.]</span></p>
<h2><span style=\"mso-ansi-language: EN-US;\"><strong>4. Laziness</strong></span></h2>
<p><span style=\"mso-ansi-language: EN-US;\">Nope. Still lazy. If anything, akrasia and procrastination are more of a problem now that I'm trying to do harder things more deliberately. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I've been keeping written goals for about a year now. This means I actually notice when I don't accomplish them. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I use Remember the Milk as a GTD system, and some other productivity/organization software (rescuetime, Mint.com, etc). I finally switched to Gmail, where I can use Boomerang and other useful tools. My current openness to trying new organization methods is high. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">My general interest in <a href=\"/lw/5a5/no_seriously_just_try_it/\">trying things</a> is higher, mainly because I have lots of community-endorsed-warm-fuzzies positive affect around that phrase. I want to be someone who's open to new experiences; I've had enough new experiences to realize how exhilarating they can be.Â </span></p>
<h2><span style=\"mso-ansi-language: EN-US;\"><strong>Conclusion</strong></span></h2>
<p><span style=\"mso-ansi-language: EN-US;\">I now have a wider range of potentially high-value personal projects ongoing. I now have an explicit goal of being well-known for non-fiction writing, probably in a blog form, in the next five years. (Do I have enough interesting things to say to make this a reality? We'll see. Is this goal vague? Yes. Working on it. I used to reject goals if they weren't utterly concrete, but even vague goals are something to build on). </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I'm more explicit with myself about what I want from CFAR curriculum skills. (The general problem of critical thinking in nursing? Solvable! Why not?) </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I think I've finally admitted to myself that \"well, I'll just live in a cozy little house near my parents and work in the ICU and raise kids for the next forty years\" might not be particularly virtuous <em>or </em>fun. There are things I would prefer to be different in the world, even if I can only completely specify a few of them. There are exciting scary opportunities happening all the time. I'm lucky enough to belong to a community of people that can help me find them. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I don't have plans for much beyond the next year. But here's to the next decade being interesting!<br></span></p></div>
<a href=\"http://lesswrong.com/lw/hvw/how_i_became_more_ambitious/#comments\">29 comments</a>
" nil nil "bdef315ca53a58587dd028bb027b9808") (60 (20954 33480 109369) "http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/" "Rationality Quotes July 2013" nil "Tue, 02 Jul 2013 16:21:59 +0000" "Submitted by <a href=\"http://lesswrong.com/user/Vaniver\">Vaniver</a>
#
3 votes
#
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">189 comments</a>
<div><div id=\"entry_t3_hlk\" class=\"content clear\">
<div class=\"md\">
<div>
<div>
<p>Another month has passed and here is a new rationality quotes thread. The usual rules are:</p>
<ul>
<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>
<li>Do not quote yourself.</li>
<li>Do not quote from Less Wrong itself, HPMoR, Eliezer Yudkowsky, or Robin Hanson.</li>
<li>No more than 5 quotes per person per monthly thread, please.</li>
</ul>
</div>
</div>
</div>
</div></div>
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">189 comments</a>
" nil nil "8e4b0d5fec51ea98a914b9b1e30a74a5") (59 (20954 30944 145025) "http://lesswrong.com/lw/hvw/how_i_became_more_ambitious/" "How I Became More Ambitious" nil "Thu, 04 Jul 2013 23:34:15 +0000" "Submitted by <a href=\"http://lesswrong.com/user/Swimmer963\">Swimmer963</a>
#
42 votes
#
<a href=\"http://lesswrong.com/lw/hvw/how_i_became_more_ambitious/#comments\">29 comments</a>
<div><p>Follow-up to <a href=\"/lw/9j1/how_i_ended_up_nonambitious/\">How I Ended Up Non-Ambitious</a></p>
<p>Living with yourself is a bit like having a preteen and watching them get taller; the changes happen so slowly that it's almost impossible to notice them, until you stumble across an old point of comparison and it becomes blindingly obvious. I hit that point a few days ago, while planning what I might want to talk about during an OkCupid date. My brain produced the following thought: \"well, if this topic comes up, it might sound like I'm trying to take over the world, and that's intimidating- Wait. What?\"</p>
<p>I'm not trying to take over the world. It sounds like a lot of work, and not my comparative advantage. If it seemed necessary, I would point out the problems that needed solving and delegate them to CFAR alumni with more domain-specific expertise than me.</p>
<p>However, I went back and reread the post linked at the beginning, and I no longer feel much kinship with that person. This is a change that happened maybe 25-50% deliberately, and the rest by drift, but I still changed my mind, so I will try to detail the particular changes, and what I think led to them. <a href=\"/lw/5sk/inferring_our_desires/\">Introspection is unreliable</a>, so I'll probably be at least 50% wrong, but what can you do?</p>
<h2><strong>1. Idealism versus practicality</strong></h2>
<p>I would still call myself practical, but I no longer think that this comes at the expense of idealism. Idealism is absolutely essential, if you want to have a world that changes because someone wanted it to, as opposed to just by drift. Lately in the rationalist/CFAR/LW community, there's been a lot of emphasis on <a href=\"/tag/agency/\">agency</a> and agentiness, which basically mean the ability to change the world and/or yourself deliberately, on purpose, through planned actions. This is hard. The first step is idealism-being able to imagine a state of affairs that is different and better. Then comes practicality, the part where you sit down and work hard and actually get something done.</p>
<p>It's still true that idealism without practicality doesn't get much done, and practicality without idealism can get a lot done, but it matters what problems you're working on, too. Are you being <a href=\"/lw/2p5/humans_are_not_automatically_strategic/\">strategic</a>? Are you even thinking, at all, about whether your actions are helping to accomplish your goals? One of the big things I've learned, a year and a half and two CFAR workshops later, is how automatic and easy this lack of strategy really is.</p>
<p>I had a limited sort of idealism in high school; I wanted to do work that was important and relevant; but I was lazy about it. I wanted someone to tell me what was important to be doing right now. Nursing seemed like an awesome solution. It still seems like a solution, but recently I've admitted to myself, with a painful twinge, that it might not be the best way to leverage my comparative advantage and help the most people. It's <a href=\"/lw/8j4/5second_level_case_study_value_of_information/\">worth spending a few minutes or hours</a> looking for interesting and important problems to work on.</p>
<p>I don't think I had the mental vocabulary to think that thought a year and a half ago. Some of the change comes from having dated an economics student. Come to think of it, I expect some of his general ambition rubbed off on me, too. The rest of the change comes from hanging out with the effective altruism and similar communities.</p>
<p>I'm still practical. I exercise, eat well, go to bed on time, work lots of hours, spend my money wisely, and maintain my social circle mostly on autopilot; it requires effort but not deliberate effort. I'm lucky to have this skill. But I no longer think it's a virtue over and above idealism. Practical idealists make the biggest difference, and they're pretty cool to hang out with. I want to be one when I grow up.</p>
<h2><span style=\"mso-ansi-language: EN-US;\">2. Fear of failure</span></h2>
<p><span style=\"mso-ansi-language: EN-US;\">Don't get me wrong. If there's one deep, gripping, soul-crushing terror in my life, one thing that gives me literal nightmares, it's failure. Making mistakes. Not being good enough. Et cetera. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">In the past few years, the main change has been admitting to myself that this terror doesn't make a lot of sense. First of all, it's completely miscalibrated. As Eliezer pointed out during a conversation on this, I don't fail at things very often. Far from being a success, this is likely a sign that the things I'm trying aren't nearly challenging enough. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">My threshold for what constitutes failure is also fairly low. I made a couple of embarrassing mistakes during my spring clinical. Some part of my brain is convinced that this equals <em>permanent </em>failure; I wasn't perfect during the placement, and I can't go back and change the past, thus I have failed. Forever. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I passed the clinical, wrote the provincial exam (results aren't in but I'm >99% confident I passed), and I'm currently working in the intensive care unit, which has been my dream since I was about fifteen. The part of my brain that keeps telling me I failed permanently obviously isn't saying anything useful. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I think 'embarrassing' is a keyword here. The first thing I thought, on the several occasions that I made mistakes, was \"oh my god did I just kill someone... Phew, no, no harm done.\" The second thought was \"oh my god, my preceptor will think I'm stupid forever and she'll never respect me and no one wants me around, I'm not good enough...\" This line of thought never goes anywhere good. It says something about me, though, that \"I'm not good enough\" is very directly connected to people wanting me around, to belonging somewhere. For several personality-formative years of my life, people <em>didn't </em>want me around. Probably for good reason; my ten-year-old self was prickly and socially inept and miserable. I think a lot of my determination not to seek status comes from the \"uncool kids trying to be cool are pathetic\" meme that was so rampant when I was in sixth grade. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">Oh, and then there's the traumatic swim team experience. Somewhere, in a part of my brain where I don't go very often nowadays, there a bottomless whirlpool of powerless rage and despair around the phrase \"no matter how hard I try, I'll never be good enough.\" So when I make an embarrassing mistake, my ten-year-old self is screaming at me \"no wonder everyone hates you!\" and my fourteen-year-old self is sadly muttering that \"you know, maybe you just don't have enough natural talent,\" and none of it is at all useful. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">The thing about those phrases is that they refer to complex and value-laden concepts, in a way that makes them seem like innate attributes, Ã  la <a href=\"/lw/hz/correspondence_bias/\">Fundamental Attribution Error</a>. \"Not good enough\" isn't a yes-or-no attribute of a person; it's a <a href=\"/lw/td/magical_categories/\">magical category</a> that only sounds simple because it's a three-word phrase. I've gotten somewhat better at propagating this to my emotional self. Slightly. It's a work in progress.</span></p>
<p><span style=\"mso-ansi-language: EN-US;\">During a conversation about this with <a href=\"http://annasalamon.com/\">Anna Salamon</a>, she noted that she likes to approach her own emotions and ask them what they want. It sounds weird, but it's helpful. \"Dear crushing sense of despair and unworthiness, what do you want? ...Oh, you're worried that you're going to end up an outcast from your tribe and starve to death in the wilderness because you accidentally gave an extra dose of digoxin? You want to signal remorse and regret and make sure everyone knows you're taking your failure seriously so that maybe they'll forgive you? Thank you for trying to protect me. But really, you don't need to worry about the starving-outcast thing. No one was harmed and no one is mad at you personally. Your friends and family couldn't care less. This mistake is data, but it's just as much data about the environment as it is about your attributes. These hand-copied medication records are the perfect medium for human error. Instead of signalling remorse, let's put some mental energy into getting rid of the environmental conditions that led to this mistake.\"</span></p>
<p><a href=\"http://en.wikipedia.org/wiki/Rejection_Therapy\">Rejection therapy</a> and having a general CoZE [Comfort Zone Expansion] mindset helped remove some of the sting of \"but I'll look stupid if I try something too hard and fail at it!\" I still worry about the pain of future embarrassment, but I'm more likely to point out to myself that it's not a valid objection and I should do X anyway. Making \"<a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">I want to become stronger</a>\" an explicit motto is new to the last year and a half, too, and helps by giving me ammunition for why potential embarrassment isn't a reason not to do something.</p>
<p>In conclusion: failure still sucks. I'm a perfectionist. But I failed in a lot of small ways during my <a href=\"/lw/gnv/learning_critical_thinking_a_personal_example/\">spring clinical</a>, and passed/got a job anyway, which seems to have helped me propagate to my emotional self that<em> it's okay to try hard things</em>, where I'm almost certain to make mistakes, because mistakes don't equal instant damnation and hatred from all of my friends.</p>
<h2><span style=\"mso-ansi-language: EN-US;\"><strong>3. The morality of ambition</strong></span></h2>
<p><span style=\"mso-ansi-language: EN-US;\">While I was in San Francisco a month ago, volunteering at the CFAR workshop and generally spending my time surrounded by smart, passionate, and ambitious people (thus convincing my emotional system that this is normal and okay), I had a conversation with Eliezer. He asked me to list ten areas where I had a comparative advantage; ten things in which I was above average. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">This was a lot more painful than it had any reason to be. After bouncing off various poorly-formed objections in my mind, I said to myself \"you know, having trouble admitting what you're good at doesn't make you virtuous.\" This was painful; losing a source of feeling-virtuous always is. But it was helpful. Yeah, talking all the time about how awesome you are at X, Y, Z makes you a bit of a bore. People might even avoid you (oh! the horror!). However, this doesn't mean that blocking even the<em> thought </em>of being above average makes you a good person. In fact, it's counterproductive. How are you supposed to know what problems you're capable of solving in the world if you can't be honest with yourself about your capabilities? </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">This conversation helped. (Even if some of the effect was \"high status person says X -> I believe X,\" who cares? I endorsed myself changing my mind about this a year and a half ago. It's about time.)</span></p>
<p><span style=\"mso-ansi-language: EN-US;\"><a href=\"http://hpmor.com/\">HPMOR</a> helped, too; specifically, the idea that there are four houses which have different positive qualities. Slytherins are demonized in canon, but in HPMOR their skills are recognized as essential. I can easily recognize the Ravenclaw and Hufflepuff and even the Gryffindor in myself, but not much of Slytherin. Having a word for the ambition-cunning-strategic concept cluster is helpful. I can ask myself \"now what would a Slytherin do with this information:?\" I can think thoughts that feel very un-virtuous. \"I'm young and prettier than average. What's a Slytherin way to use this... Oh, I suppose I can leverage it into a comparative advantage for getting high-status men to pay attention to me long enough for me to explain the merits of an idea I have.\" This thought feels yuck, but the universe doesn't explode. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">Probably the biggest factor was going to the CFAR workshops in the first place. Not from any of the curriculum, particularly, although the mindset of goal factoring helped me to realize that the mental action of \"feeling unvirtuous for thinking in ambitious or calculating ways\" wasn't accomplishing anything I wanted. Mostly the change came from social normalization, from hanging out with people who talked openly about their strengths and weaknesses, and no one got shunned. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">[Silly plan for taking over the world: Arrange to meet high status-people and offer to give their children swimming lessons. Gain their trust. Proceed from there.]</span></p>
<h2><span style=\"mso-ansi-language: EN-US;\"><strong>4. Laziness</strong></span></h2>
<p><span style=\"mso-ansi-language: EN-US;\">Nope. Still lazy. If anything, akrasia and procrastination are more of a problem now that I'm trying to do harder things more deliberately. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I've been keeping written goals for about a year now. This means I actually notice when I don't accomplish them. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I use Remember the Milk as a GTD system, and some other productivity/organization software (rescuetime, Mint.com, etc). I finally switched to Gmail, where I can use Boomerang and other useful tools. My current openness to trying new organization methods is high. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">My general interest in <a href=\"/lw/5a5/no_seriously_just_try_it/\">trying things</a> is higher, mainly because I have lots of community-endorsed-warm-fuzzies positive affect around that phrase. I want to be someone who's open to new experiences; I've had enough new experiences to realize how exhilarating they can be.Â </span></p>
<h2><span style=\"mso-ansi-language: EN-US;\"><strong>Conclusion</strong></span></h2>
<p><span style=\"mso-ansi-language: EN-US;\">I now have a wider range of potentially high-value personal projects ongoing. I now have an explicit goal of being well-known for non-fiction writing, probably in a blog form, in the next five years. (Do I have enough interesting things to say to make this a reality? We'll see. Is this goal vague? Yes. Working on it. I used to reject goals if they weren't utterly concrete, but even vague goals are something to build on). </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I'm more explicit with myself about what I want from CFAR curriculum skills. (The general problem of critical thinking in nursing? Solvable! Why not?) </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I think I've finally admitted to myself that \"well, I'll just live in a cozy little house near my parents and work in the ICU and raise kids for the next forty years\" might not be particularly virtuous <em>or </em>fun. There are things I would prefer to be different in the world, even if I can only completely specify a few of them. There are exciting scary opportunities happening all the time. I'm lucky enough to belong to a community of people that can help me find them. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I don't have plans for much beyond the next year. But here's to the next decade being interesting!<br></span></p></div>
<a href=\"http://lesswrong.com/lw/hvw/how_i_became_more_ambitious/#comments\">29 comments</a>
" nil nil "907c45abae7ea5452bff39259ddcf912") (58 (20954 30944 142788) "http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/" "Rationality Quotes July 2013" nil "Tue, 02 Jul 2013 16:21:59 +0000" "Submitted by <a href=\"http://lesswrong.com/user/Vaniver\">Vaniver</a>
#
3 votes
#
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">188 comments</a>
<div><div id=\"entry_t3_hlk\" class=\"content clear\">
<div class=\"md\">
<div>
<div>
<p>Another month has passed and here is a new rationality quotes thread. The usual rules are:</p>
<ul>
<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>
<li>Do not quote yourself.</li>
<li>Do not quote from Less Wrong itself, HPMoR, Eliezer Yudkowsky, or Robin Hanson.</li>
<li>No more than 5 quotes per person per monthly thread, please.</li>
</ul>
</div>
</div>
</div>
</div></div>
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">188 comments</a>
" nil nil "fd9b294f74bc77336fe6ae6201a5562b") (57 (20954 30944 140238) "http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/" "Tiling Agents for Self-Modifying AI (OPFAI #2)" nil "Fri, 07 Jun 2013 06:24:25 +1000" "Submitted by <a href=\"http://lesswrong.com/user/Eliezer_Yudkowsky\">Eliezer_Yudkowsky</a>
#
51 votes
#
<a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/#comments\">251 comments</a>
<div><p>An early draft of publication #2 in the Open Problems in Friendly AI series is now available: Â <a href=\"http://intelligence.org/files/TilingAgents.pdf\">Tiling Agents for Self-Modifying AI, and the Lobian Obstacle</a>.Â  ~20,000 words, aimed at mathematicians or the highly mathematically literate.Â  The research reported on was conducted by Yudkowsky and Herreshoff, substantially refined at the November 2012 MIRI Workshop with Mihaly Barasz and Paul Christiano, and refined further at the April 2013 MIRI Workshop.</p>
<p style=\"padding-left: 60px;\"><strong>Abstract:</strong></p>
<p style=\"padding-left: 30px;\">We model self-modication in AI by introducing 'tiling' agents whose decision systems will approve the construction of highly similar agents, creating a repeating pattern (including similarity of the offspring's goals). Â Constructing a formalism in the most straightforward way produces a Godelian difficulty, the Lobian obstacle. Â By technical methods we demonstrate the possibility of avoiding this obstacle, but the underlying puzzles of rational coherence are thus only partially addressed. Â We extend the formalism to partially unknown deterministic environments, and show a very crude extension to probabilistic environments and expected utility; but the problem of finding a fundamental decision criterion for self-modifying probabilistic agents remains open.</p>
<p>Commenting here is the preferred venue for discussion of the paper. Â This is an early draft and has not been reviewed, so it may contain mathematical errors, and reporting of these will be much appreciated.</p>
<p>The overall agenda of the paper is introduce the conceptual notion of a self-reproducing decision pattern which includes reproduction of the goal or utility function, by exposing a particular possible problem with a tiling logical decision pattern and coming up with some partial technical solutions. Â This then makes it conceptually much clearer to point out the even deeper problems with \"We can't yet describe a probabilistic way to do this because of non-monotonicity\" and \"We don't have a good bounded way to do this because maximization is impossible, satisficing is too weak and Schmidhuber's swapping criterion is underspecified.\" Â The paper uses first-order logic (FOL) because FOL has a lot of useful standard machinery for reflection which we can then invoke; in real life, FOL is of course a poor representational fit to most real-world environments outside a human-constructed computer chip with thermodynamically expensive crisp variable states.</p>
<p>As further background, the idea that something-like-proof might be relevant to Friendly AI is not about achieving some chimera of absolute safety-feeling, but rather about the idea that the total probability of catastrophic failure should not have a significant conditionally independent component on each self-modification, and that self-modification will (at least in initial stages) take place within the highly deterministic environment of a computer chip. Â This means that statistical testing methods (e.g. an evolutionary algorithm's evaluation of average fitness on a set of test problems) are not suitable for self-modifications which can potentially induce catastrophic failure (e.g. of parts of code that can affect the representation or interpretation of the goals). Â Mathematical proofs have the property that they are as strong as their axioms and have no significant conditionally independent per-step failure probability if their axioms are semantically true, which suggests that something like mathematical reasoning may be appropriate for certain particular types of self-modification during some developmental stages.</p>
<p>Thus the content of the paper is very far off from how a realistic AI would work, but conversely, if you can't even answer the kinds of simple problems posed within the paper (both those we partially solve and those we only pose) then you must be very far off from being able to build a stable self-modifying AI. Â Being able to say how to build a theoretical device that would play perfect chess given infinite computing power, is very far off from the ability to build Deep Blue. Â However, if you can't even say how to play perfect chess given infinite computing power, you are confused about the rules of the chess or the structure of chess-playing computation in a way that would make it entirelyÂ hopeless for you to figure out how to build a bounded chess-player. Â Thus \"In real life we're always bounded\" is no excuse for not being able to solve the much simpler unbounded form of the problem, and being able to describe the infinite chess-player would be substantial and useful conceptual progress compared to <em>not </em>being able to do that. Â We can't be absolutely certain that an analogous situation holds between solving the challenges posed in the paper, and realistic self-modifying AIs with stable goal systems, but every line of investigation has to start somewhere.</p>
<p>Parts of the paper will be easier to understand if you've read <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\">Highly Advanced Epistemology 101 For Beginners</a> including the parts on correspondence theories of truth (relevant to section 6) and model-theoretic semantics of logic (relevant to 3, 4, and 6), and there are footnotes intended to make the paper somewhat more accessible than usual, but the paper is still essentially aimed at mathematically sophisticated readers.</p></div>
<a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/#comments\">251 comments</a>
" nil nil "0beebe40ebccbe6cbb44af3461e9bad1") (56 (20954 29003 414854) "http://lesswrong.com/lw/hvw/how_i_became_more_ambitious/" "How I Became More Ambitious" nil "Thu, 04 Jul 2013 23:34:15 +0000" "Submitted by <a href=\"http://lesswrong.com/user/Swimmer963\">Swimmer963</a>
#
42 votes
#
<a href=\"http://lesswrong.com/lw/hvw/how_i_became_more_ambitious/#comments\">29 comments</a>
<div><p>Follow-up to <a href=\"/lw/9j1/how_i_ended_up_nonambitious/\">How I Ended Up Non-Ambitious</a></p>
<p>Living with yourself is a bit like having a preteen and watching them get taller; the changes happen so slowly that it's almost impossible to notice them, until you stumble across an old point of comparison and it becomes blindingly obvious. I hit that point a few days ago, while planning what I might want to talk about during an OkCupid date. My brain produced the following thought: \"well, if this topic comes up, it might sound like I'm trying to take over the world, and that's intimidating- Wait. What?\"</p>
<p>I'm not trying to take over the world. It sounds like a lot of work, and not my comparative advantage. If it seemed necessary, I would point out the problems that needed solving and delegate them to CFAR alumni with more domain-specific expertise than me.</p>
<p>However, I went back and reread the post linked at the beginning, and I no longer feel much kinship with that person. This is a change that happened maybe 25-50% deliberately, and the rest by drift, but I still changed my mind, so I will try to detail the particular changes, and what I think led to them. <a href=\"/lw/5sk/inferring_our_desires/\">Introspection is unreliable</a>, so I'll probably be at least 50% wrong, but what can you do?</p>
<h2><strong>1. Idealism versus practicality</strong></h2>
<p>I would still call myself practical, but I no longer think that this comes at the expense of idealism. Idealism is absolutely essential, if you want to have a world that changes because someone wanted it to, as opposed to just by drift. Lately in the rationalist/CFAR/LW community, there's been a lot of emphasis on <a href=\"/tag/agency/\">agency</a> and agentiness, which basically mean the ability to change the world and/or yourself deliberately, on purpose, through planned actions. This is hard. The first step is idealism-being able to imagine a state of affairs that is different and better. Then comes practicality, the part where you sit down and work hard and actually get something done.</p>
<p>It's still true that idealism without practicality doesn't get much done, and practicality without idealism can get a lot done, but it matters what problems you're working on, too. Are you being <a href=\"/lw/2p5/humans_are_not_automatically_strategic/\">strategic</a>? Are you even thinking, at all, about whether your actions are helping to accomplish your goals? One of the big things I've learned, a year and a half and two CFAR workshops later, is how automatic and easy this lack of strategy really is.</p>
<p>I had a limited sort of idealism in high school; I wanted to do work that was important and relevant; but I was lazy about it. I wanted someone to tell me what was important to be doing right now. Nursing seemed like an awesome solution. It still seems like a solution, but recently I've admitted to myself, with a painful twinge, that it might not be the best way to leverage my comparative advantage and help the most people. It's <a href=\"/lw/8j4/5second_level_case_study_value_of_information/\">worth spending a few minutes or hours</a> looking for interesting and important problems to work on.</p>
<p>I don't think I had the mental vocabulary to think that thought a year and a half ago. Some of the change comes from having dated an economics student. Come to think of it, I expect some of his general ambition rubbed off on me, too. The rest of the change comes from hanging out with the effective altruism and similar communities.</p>
<p>I'm still practical. I exercise, eat well, go to bed on time, work lots of hours, spend my money wisely, and maintain my social circle mostly on autopilot; it requires effort but not deliberate effort. I'm lucky to have this skill. But I no longer think it's a virtue over and above idealism. Practical idealists make the biggest difference, and they're pretty cool to hang out with. I want to be one when I grow up.</p>
<h2><span style=\"mso-ansi-language: EN-US;\">2. Fear of failure</span></h2>
<p><span style=\"mso-ansi-language: EN-US;\">Don't get me wrong. If there's one deep, gripping, soul-crushing terror in my life, one thing that gives me literal nightmares, it's failure. Making mistakes. Not being good enough. Et cetera. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">In the past few years, the main change has been admitting to myself that this terror doesn't make a lot of sense. First of all, it's completely miscalibrated. As Eliezer pointed out during a conversation on this, I don't fail at things very often. Far from being a success, this is likely a sign that the things I'm trying aren't nearly challenging enough. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">My threshold for what constitutes failure is also fairly low. I made a couple of embarrassing mistakes during my spring clinical. Some part of my brain is convinced that this equals <em>permanent </em>failure; I wasn't perfect during the placement, and I can't go back and change the past, thus I have failed. Forever. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I passed the clinical, wrote the provincial exam (results aren't in but I'm >99% confident I passed), and I'm currently working in the intensive care unit, which has been my dream since I was about fifteen. The part of my brain that keeps telling me I failed permanently obviously isn't saying anything useful. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I think 'embarrassing' is a keyword here. The first thing I thought, on the several occasions that I made mistakes, was \"oh my god did I just kill someone... Phew, no, no harm done.\" The second thought was \"oh my god, my preceptor will think I'm stupid forever and she'll never respect me and no one wants me around, I'm not good enough...\" This line of thought never goes anywhere good. It says something about me, though, that \"I'm not good enough\" is very directly connected to people wanting me around, to belonging somewhere. For several personality-formative years of my life, people <em>didn't </em>want me around. Probably for good reason; my ten-year-old self was prickly and socially inept and miserable. I think a lot of my determination not to seek status comes from the \"uncool kids trying to be cool are pathetic\" meme that was so rampant when I was in sixth grade. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">Oh, and then there's the traumatic swim team experience. Somewhere, in a part of my brain where I don't go very often nowadays, there a bottomless whirlpool of powerless rage and despair around the phrase \"no matter how hard I try, I'll never be good enough.\" So when I make an embarrassing mistake, my ten-year-old self is screaming at me \"no wonder everyone hates you!\" and my fourteen-year-old self is sadly muttering that \"you know, maybe you just don't have enough natural talent,\" and none of it is at all useful. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">The thing about those phrases is that they refer to complex and value-laden concepts, in a way that makes them seem like innate attributes, Ã  la <a href=\"/lw/hz/correspondence_bias/\">Fundamental Attribution Error</a>. \"Not good enough\" isn't a yes-or-no attribute of a person; it's a <a href=\"/lw/td/magical_categories/\">magical category</a> that only sounds simple because it's a three-word phrase. I've gotten somewhat better at propagating this to my emotional self. Slightly. It's a work in progress.</span></p>
<p><span style=\"mso-ansi-language: EN-US;\">During a conversation about this with <a href=\"http://annasalamon.com/\">Anna Salamon</a>, she noted that she likes to approach her own emotions and ask them what they want. It sounds weird, but it's helpful. \"Dear crushing sense of despair and unworthiness, what do you want? ...Oh, you're worried that you're going to end up an outcast from your tribe and starve to death in the wilderness because you accidentally gave an extra dose of digoxin? You want to signal remorse and regret and make sure everyone knows you're taking your failure seriously so that maybe they'll forgive you? Thank you for trying to protect me. But really, you don't need to worry about the starving-outcast thing. No one was harmed and no one is mad at you personally. Your friends and family couldn't care less. This mistake is data, but it's just as much data about the environment as it is about your attributes. These hand-copied medication records are the perfect medium for human error. Instead of signalling remorse, let's put some mental energy into getting rid of the environmental conditions that led to this mistake.\"</span></p>
<p><a href=\"http://en.wikipedia.org/wiki/Rejection_Therapy\">Rejection therapy</a> and having a general CoZE [Comfort Zone Expansion] mindset helped remove some of the sting of \"but I'll look stupid if I try something too hard and fail at it!\" I still worry about the pain of future embarrassment, but I'm more likely to point out to myself that it's not a valid objection and I should do X anyway. Making \"<a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">I want to become stronger</a>\" an explicit motto is new to the last year and a half, too, and helps by giving me ammunition for why potential embarrassment isn't a reason not to do something.</p>
<p>In conclusion: failure still sucks. I'm a perfectionist. But I failed in a lot of small ways during my <a href=\"/lw/gnv/learning_critical_thinking_a_personal_example/\">spring clinical</a>, and passed/got a job anyway, which seems to have helped me propagate to my emotional self that<em> it's okay to try hard things</em>, where I'm almost certain to make mistakes, because mistakes don't equal instant damnation and hatred from all of my friends.</p>
<h2><span style=\"mso-ansi-language: EN-US;\"><strong>3. The morality of ambition</strong></span></h2>
<p><span style=\"mso-ansi-language: EN-US;\">While I was in San Francisco a month ago, volunteering at the CFAR workshop and generally spending my time surrounded by smart, passionate, and ambitious people (thus convincing my emotional system that this is normal and okay), I had a conversation with Eliezer. He asked me to list ten areas where I had a comparative advantage; ten things in which I was above average. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">This was a lot more painful than it had any reason to be. After bouncing off various poorly-formed objections in my mind, I said to myself \"you know, having trouble admitting what you're good at doesn't make you virtuous.\" This was painful; losing a source of feeling-virtuous always is. But it was helpful. Yeah, talking all the time about how awesome you are at X, Y, Z makes you a bit of a bore. People might even avoid you (oh! the horror!). However, this doesn't mean that blocking even the<em> thought </em>of being above average makes you a good person. In fact, it's counterproductive. How are you supposed to know what problems you're capable of solving in the world if you can't be honest with yourself about your capabilities? </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">This conversation helped. (Even if some of the effect was \"high status person says X -> I believe X,\" who cares? I endorsed myself changing my mind about this a year and a half ago. It's about time.)</span></p>
<p><span style=\"mso-ansi-language: EN-US;\"><a href=\"http://hpmor.com/\">HPMOR</a> helped, too; specifically, the idea that there are four houses which have different positive qualities. Slytherins are demonized in canon, but in HPMOR their skills are recognized as essential. I can easily recognize the Ravenclaw and Hufflepuff and even the Gryffindor in myself, but not much of Slytherin. Having a word for the ambition-cunning-strategic concept cluster is helpful. I can ask myself \"now what would a Slytherin do with this information:?\" I can think thoughts that feel very un-virtuous. \"I'm young and prettier than average. What's a Slytherin way to use this... Oh, I suppose I can leverage it into a comparative advantage for getting high-status men to pay attention to me long enough for me to explain the merits of an idea I have.\" This thought feels yuck, but the universe doesn't explode. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">Probably the biggest factor was going to the CFAR workshops in the first place. Not from any of the curriculum, particularly, although the mindset of goal factoring helped me to realize that the mental action of \"feeling unvirtuous for thinking in ambitious or calculating ways\" wasn't accomplishing anything I wanted. Mostly the change came from social normalization, from hanging out with people who talked openly about their strengths and weaknesses, and no one got shunned. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">[Silly plan for taking over the world: Arrange to meet high status-people and offer to give their children swimming lessons. Gain their trust. Proceed from there.]</span></p>
<h2><span style=\"mso-ansi-language: EN-US;\"><strong>4. Laziness</strong></span></h2>
<p><span style=\"mso-ansi-language: EN-US;\">Nope. Still lazy. If anything, akrasia and procrastination are more of a problem now that I'm trying to do harder things more deliberately. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I've been keeping written goals for about a year now. This means I actually notice when I don't accomplish them. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I use Remember the Milk as a GTD system, and some other productivity/organization software (rescuetime, Mint.com, etc). I finally switched to Gmail, where I can use Boomerang and other useful tools. My current openness to trying new organization methods is high. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">My general interest in <a href=\"/lw/5a5/no_seriously_just_try_it/\">trying things</a> is higher, mainly because I have lots of community-endorsed-warm-fuzzies positive affect around that phrase. I want to be someone who's open to new experiences; I've had enough new experiences to realize how exhilarating they can be.Â </span></p>
<h2><span style=\"mso-ansi-language: EN-US;\"><strong>Conclusion</strong></span></h2>
<p><span style=\"mso-ansi-language: EN-US;\">I now have a wider range of potentially high-value personal projects ongoing. I now have an explicit goal of being well-known for non-fiction writing, probably in a blog form, in the next five years. (Do I have enough interesting things to say to make this a reality? We'll see. Is this goal vague? Yes. Working on it. I used to reject goals if they weren't utterly concrete, but even vague goals are something to build on). </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I'm more explicit with myself about what I want from CFAR curriculum skills. (The general problem of critical thinking in nursing? Solvable! Why not?) </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I think I've finally admitted to myself that \"well, I'll just live in a cozy little house near my parents and work in the ICU and raise kids for the next forty years\" might not be particularly virtuous <em>or </em>fun. There are things I would prefer to be different in the world, even if I can only completely specify a few of them. There are exciting scary opportunities happening all the time. I'm lucky enough to belong to a community of people that can help me find them. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I don't have plans for much beyond the next year. But here's to the next decade being interesting!<br></span></p></div>
<a href=\"http://lesswrong.com/lw/hvw/how_i_became_more_ambitious/#comments\">29 comments</a>
" nil nil "7ebe86d5a452db896f6b937c70d8fc73") (55 (20954 29003 412745) "http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/" "Rationality Quotes July 2013" nil "Tue, 02 Jul 2013 16:21:59 +0000" "Submitted by <a href=\"http://lesswrong.com/user/Vaniver\">Vaniver</a>
#
3 votes
#
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">187 comments</a>
<div><div id=\"entry_t3_hlk\" class=\"content clear\">
<div class=\"md\">
<div>
<div>
<p>Another month has passed and here is a new rationality quotes thread. The usual rules are:</p>
<ul>
<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>
<li>Do not quote yourself.</li>
<li>Do not quote from Less Wrong itself, HPMoR, Eliezer Yudkowsky, or Robin Hanson.</li>
<li>No more than 5 quotes per person per monthly thread, please.</li>
</ul>
</div>
</div>
</div>
</div></div>
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">187 comments</a>
" nil nil "4597c38b013e4bf4d242a7b426e8ee7b") (54 (20954 29003 410377) "http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/" "Tiling Agents for Self-Modifying AI (OPFAI #2)" nil "Fri, 07 Jun 2013 06:24:25 +1000" "Submitted by <a href=\"http://lesswrong.com/user/Eliezer_Yudkowsky\">Eliezer_Yudkowsky</a>
#
51 votes
#
<a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/#comments\">251 comments</a>
<div><p>An early draft of publication #2 in the Open Problems in Friendly AI series is now available: Â <a href=\"http://intelligence.org/files/TilingAgents.pdf\">Tiling Agents for Self-Modifying AI, and the Lobian Obstacle</a>.Â  ~20,000 words, aimed at mathematicians or the highly mathematically literate.Â  The research reported on was conducted by Yudkowsky and Herreshoff, substantially refined at the November 2012 MIRI Workshop with Mihaly Barasz and Paul Christiano, and refined further at the April 2013 MIRI Workshop.</p>
<p style=\"padding-left: 60px;\"><strong>Abstract:</strong></p>
<p style=\"padding-left: 30px;\">We model self-modication in AI by introducing 'tiling' agents whose decision systems will approve the construction of highly similar agents, creating a repeating pattern (including similarity of the offspring's goals). Â Constructing a formalism in the most straightforward way produces a Godelian difficulty, the Lobian obstacle. Â By technical methods we demonstrate the possibility of avoiding this obstacle, but the underlying puzzles of rational coherence are thus only partially addressed. Â We extend the formalism to partially unknown deterministic environments, and show a very crude extension to probabilistic environments and expected utility; but the problem of finding a fundamental decision criterion for self-modifying probabilistic agents remains open.</p>
<p>Commenting here is the preferred venue for discussion of the paper. Â This is an early draft and has not been reviewed, so it may contain mathematical errors, and reporting of these will be much appreciated.</p>
<p>The overall agenda of the paper is introduce the conceptual notion of a self-reproducing decision pattern which includes reproduction of the goal or utility function, by exposing a particular possible problem with a tiling logical decision pattern and coming up with some partial technical solutions. Â This then makes it conceptually much clearer to point out the even deeper problems with \"We can't yet describe a probabilistic way to do this because of non-monotonicity\" and \"We don't have a good bounded way to do this because maximization is impossible, satisficing is too weak and Schmidhuber's swapping criterion is underspecified.\" Â The paper uses first-order logic (FOL) because FOL has a lot of useful standard machinery for reflection which we can then invoke; in real life, FOL is of course a poor representational fit to most real-world environments outside a human-constructed computer chip with thermodynamically expensive crisp variable states.</p>
<p>As further background, the idea that something-like-proof might be relevant to Friendly AI is not about achieving some chimera of absolute safety-feeling, but rather about the idea that the total probability of catastrophic failure should not have a significant conditionally independent component on each self-modification, and that self-modification will (at least in initial stages) take place within the highly deterministic environment of a computer chip. Â This means that statistical testing methods (e.g. an evolutionary algorithm's evaluation of average fitness on a set of test problems) are not suitable for self-modifications which can potentially induce catastrophic failure (e.g. of parts of code that can affect the representation or interpretation of the goals). Â Mathematical proofs have the property that they are as strong as their axioms and have no significant conditionally independent per-step failure probability if their axioms are semantically true, which suggests that something like mathematical reasoning may be appropriate for certain particular types of self-modification during some developmental stages.</p>
<p>Thus the content of the paper is very far off from how a realistic AI would work, but conversely, if you can't even answer the kinds of simple problems posed within the paper (both those we partially solve and those we only pose) then you must be very far off from being able to build a stable self-modifying AI. Â Being able to say how to build a theoretical device that would play perfect chess given infinite computing power, is very far off from the ability to build Deep Blue. Â However, if you can't even say how to play perfect chess given infinite computing power, you are confused about the rules of the chess or the structure of chess-playing computation in a way that would make it entirelyÂ hopeless for you to figure out how to build a bounded chess-player. Â Thus \"In real life we're always bounded\" is no excuse for not being able to solve the much simpler unbounded form of the problem, and being able to describe the infinite chess-player would be substantial and useful conceptual progress compared to <em>not </em>being able to do that. Â We can't be absolutely certain that an analogous situation holds between solving the challenges posed in the paper, and realistic self-modifying AIs with stable goal systems, but every line of investigation has to start somewhere.</p>
<p>Parts of the paper will be easier to understand if you've read <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\">Highly Advanced Epistemology 101 For Beginners</a> including the parts on correspondence theories of truth (relevant to section 6) and model-theoretic semantics of logic (relevant to 3, 4, and 6), and there are footnotes intended to make the paper somewhat more accessible than usual, but the paper is still essentially aimed at mathematically sophisticated readers.</p></div>
<a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/#comments\">251 comments</a>
" nil nil "64cebb1d35d8d22f2aacc3f28cde672e") (53 (20950 61536 373942) "http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/" "Rationality Quotes July 2013" nil "Tue, 02 Jul 2013 16:21:59 +0000" "Submitted by <a href=\"http://lesswrong.com/user/Vaniver\">Vaniver</a>
#
3 votes
#
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">156 comments</a>
<div><div id=\"entry_t3_hlk\" class=\"content clear\">
<div class=\"md\">
<div>
<div>
<p>Another month has passed and here is a new rationality quotes thread. The usual rules are:</p>
<ul>
<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>
<li>Do not quote yourself.</li>
<li>Do not quote from Less Wrong itself, HPMoR, Eliezer Yudkowsky, or Robin Hanson.</li>
<li>No more than 5 quotes per person per monthly thread, please.</li>
</ul>
</div>
</div>
</div>
</div></div>
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">156 comments</a>
" nil nil "e7d35c6e4a7655c515aacbf6a9ccacef") (52 (20950 47994 356101) "http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/" "Rationality Quotes July 2013" nil "Tue, 02 Jul 2013 16:21:59 +0000" "Submitted by <a href=\"http://lesswrong.com/user/Vaniver\">Vaniver</a>
#
3 votes
#
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">155 comments</a>
<div><div id=\"entry_t3_hlk\" class=\"content clear\">
<div class=\"md\">
<div>
<div>
<p>Another month has passed and here is a new rationality quotes thread. The usual rules are:</p>
<ul>
<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>
<li>Do not quote yourself.</li>
<li>Do not quote from Less Wrong itself, HPMoR, Eliezer Yudkowsky, or Robin Hanson.</li>
<li>No more than 5 quotes per person per monthly thread, please.</li>
</ul>
</div>
</div>
</div>
</div></div>
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">155 comments</a>
" nil nil "f1026b14e9fc161c97c8cffb7028c8e6") (51 (20950 36074 239052) "http://lesswrong.com/lw/hvw/how_i_became_more_ambitious/" "How I Became More Ambitious" nil "Thu, 04 Jul 2013 23:34:15 +0000" "Submitted by <a href=\"http://lesswrong.com/user/Swimmer963\">Swimmer963</a>
#
32 votes
#
<a href=\"http://lesswrong.com/lw/hvw/how_i_became_more_ambitious/#comments\">18 comments</a>
<div><p>Follow-up to <a href=\"/lw/9j1/how_i_ended_up_nonambitious/\">How I Ended Up Non-Ambitious</a></p>
<p>Living with yourself is a bit like having a preteen and watching them get taller; the changes happen so slowly that it's almost impossible to notice them, until you stumble across an old point of comparison and it becomes blindingly obvious. I hit that point a few days ago, while planning what I might want to talk about during an OkCupid date. My brain produced the following thought: \"well, if this topic comes up, it might sound like I'm trying to take over the world, and that's intimidating- Wait. What?\"</p>
<p>I'm not trying to take over the world. It sounds like a lot of work, and not my comparative advantage. If it seemed necessary, I would point out the problems that needed solving and delegate them to CFAR alumni with more domain-specific expertise than me.</p>
<p>However, I went back and reread the post linked at the beginning, and I no longer feel much kinship with that person. This is a change that happened maybe 25-50% deliberately, and the rest by drift, but I still changed my mind, so I will try to detail the particular changes, and what I think led to them. <a href=\"/lw/5sk/inferring_our_desires/\">Introspection is unreliable</a>, so I'll probably be at least 50% wrong, but what can you do?</p>
<h2><strong>1. Idealism versus practicality</strong></h2>
<p>I would still call myself practical, but I no longer think that this comes at the expense of idealism. Idealism is absolutely essential, if you want to have a world that changes because someone wanted it to, as opposed to just by drift. Lately in the rationalist/CFAR/LW community, there's been a lot of emphasis on <a href=\"/tag/agency/\">agency</a> and agentiness, which basically mean the ability to change the world and/or yourself deliberately, on purpose, through planned actions. This is hard. The first step is idealism-being able to imagine a state of affairs that is different and better. Then comes practicality, the part where you sit down and work hard and actually get something done.</p>
<p>It's still true that idealism without practicality doesn't get much done, and practicality without idealism can get a lot done, but it matters what problems you're working on, too. Are you being <a href=\"/lw/2p5/humans_are_not_automatically_strategic/\">strategic</a>? Are you even thinking, at all, about whether your actions are helping to accomplish your goals? One of the big things I've learned, a year and a half and two CFAR workshops later, is how automatic and easy this lack of strategy really is.</p>
<p>I had a limited sort of idealism in high school; I wanted to do work that was important and relevant; but I was lazy about it. I wanted someone to tell me what was important to be doing right now. Nursing seemed like an awesome solution. It still seems like a solution, but recently I've admitted to myself, with a painful twinge, that it might not be the best way to leverage my comparative advantage and help the most people. It's <a href=\"/lw/8j4/5second_level_case_study_value_of_information/\">worth spending a few minutes or hours</a> looking for interesting and important problems to work on.</p>
<p>I don't think I had the mental vocabulary to think that thought a year and a half ago. Some of the change comes from having dated an economics student. Come to think of it, I expect some of his general ambition rubbed off on me, too. The rest of the change comes from hanging out with the effective altruism and similar communities.</p>
<p>I'm still practical. I exercise, eat well, go to bed on time, work lots of hours, spend my money wisely, and maintain my social circle mostly on autopilot; it requires effort but not deliberate effort. I'm lucky to have this skill. But I no longer think it's a virtue over and above idealism. Practical idealists make the biggest difference, and they're pretty cool to hang out with. I want to be one when I grow up.</p>
<h2><span style=\"mso-ansi-language: EN-US;\">2. Fear of failure</span></h2>
<p><span style=\"mso-ansi-language: EN-US;\">Don't get me wrong. If there's one deep, gripping, soul-crushing terror in my life, one thing that gives me literal nightmares, it's failure. Making mistakes. Not being good enough. Et cetera. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">In the past few years, the main change has been admitting to myself that this terror doesn't make a lot of sense. First of all, it's completely miscalibrated. As Eliezer pointed out during a conversation on this, I don't fail at things very often. Far from being a success, this is likely a sign that the things I'm trying aren't nearly challenging enough. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">My threshold for what constitutes failure is also fairly low. I made a couple of embarrassing mistakes during my spring clinical. Some part of my brain is convinced that this equals <em>permanent </em>failure; I wasn't perfect during the placement, and I can't go back and change the past, thus I have failed. Forever. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I passed the clinical, wrote the provincial exam (results aren't in but I'm >99% confident I passed), and I'm currently working in the intensive care unit, which has been my dream since I was about fifteen. The part of my brain that keeps telling me I failed permanently obviously isn't saying anything useful. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I think 'embarrassing' is a keyword here. The first thing I thought, on the several occasions that I made mistakes, was \"oh my god did I just kill someone... Phew, no, no harm done.\" The second thought was \"oh my god, my preceptor will think I'm stupid forever and she'll never respect me and no one wants me around, I'm not good enough...\" This line of thought never goes anywhere good. It says something about me, though, that \"I'm not good enough\" is very directly connected to people wanting me around, to belonging somewhere. For several personality-formative years of my life, people <em>didn't </em>want me around. Probably for good reason; my ten-year-old self was prickly and socially inept and miserable. I think a lot of my determination not to seek status comes from the \"uncool kids trying to be cool are pathetic\" meme that was so rampant when I was in sixth grade. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">Oh, and then there's the traumatic swim team experience. Somewhere, in a part of my brain where I don't go very often nowadays, there a bottomless whirlpool of powerless rage and despair around the phrase \"no matter how hard I try, I'll never be good enough.\" So when I make an embarrassing mistake, my ten-year-old self is screaming at me \"no wonder everyone hates you!\" and my fourteen-year-old self is sadly muttering that \"you know, maybe you just don't have enough natural talent,\" and none of it is at all useful. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">The thing about those phrases is that they refer to complex and value-laden concepts, in a way that makes them seem like innate attributes, Ã  la <a href=\"/lw/hz/correspondence_bias/\">Fundamental Attribution Error</a>. \"Not good enough\" isn't a yes-or-no attribute of a person; it's a <a href=\"/lw/td/magical_categories/\">magical category</a> that only sounds simple because it's a three-word phrase. I've gotten somewhat better at propagating this to my emotional self. Slightly. It's a work in progress.</span></p>
<p><span style=\"mso-ansi-language: EN-US;\">During a conversation about this with <a href=\"http://annasalamon.com/\">Anna Salamon</a>, she noted that she likes to approach her own emotions and ask them what they want. It sounds weird, but it's helpful. \"Dear crushing sense of despair and unworthiness, what do you want? ...Oh, you're worried that you're going to end up an outcast from your tribe and starve to death in the wilderness because you accidentally gave an extra dose of digoxin? You want to signal remorse and regret and make sure everyone knows you're taking your failure seriously so that maybe they'll forgive you? Thank you for trying to protect me. But really, you don't need to worry about the starving-outcast thing. No one was harmed and no one is mad at you personally. Your friends and family couldn't care less. This mistake is data, but it's just as much data about the environment as it is about your attributes. These hand-copied medication records are the perfect medium for human error. Instead of signalling remorse, let's put some mental energy into getting rid of the environmental conditions that led to this mistake.\"</span></p>
<p><a href=\"http://en.wikipedia.org/wiki/Rejection_Therapy\">Rejection therapy</a> and having a general CoZE [Comfort Zone Expansion] mindset helped remove some of the sting of \"but I'll look stupid if I try something too hard and fail at it!\" I still worry about the pain of future embarrassment, but I'm more likely to point out to myself that it's not a valid objection and I should do X anyway. Making \"<a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">I want to become stronger</a>\" an explicit motto is new to the last year and a half, too, and helps by giving me ammunition for why potential embarrassment isn't a reason not to do something.</p>
<p>In conclusion: failure still sucks. I'm a perfectionist. But I failed in a lot of small ways during my <a href=\"/lw/gnv/learning_critical_thinking_a_personal_example/\">spring clinical</a>, and passed/got a job anyway, which seems to have helped me propagate to my emotional self that<em> it's okay to try hard things</em>, where I'm almost certain to make mistakes, because mistakes don't equal instant damnation and hatred from all of my friends.</p>
<h2><span style=\"mso-ansi-language: EN-US;\"><strong>3. The morality of ambition</strong></span></h2>
<p><span style=\"mso-ansi-language: EN-US;\">While I was in San Francisco a month ago, volunteering at the CFAR workshop and generally spending my time surrounded by smart, passionate, and ambitious people (thus convincing my emotional system that this is normal and okay), I had a conversation with Eliezer. He asked me to list ten areas where I had a comparative advantage; ten things in which I was above average. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">This was a lot more painful than it had any reason to be. After bouncing off various poorly-formed objections in my mind, I said to myself \"you know, having trouble admitting what you're good at doesn't make you virtuous.\" This was painful; losing a source of feeling-virtuous always is. But it was helpful. Yeah, talking all the time about how awesome you are at X, Y, Z makes you a bit of a bore. People might even avoid you (oh! the horror!). However, this doesn't mean that blocking even the<em> thought </em>of being above average makes you a good person. In fact, it's counterproductive. How are you supposed to know what problems you're capable of solving in the world if you can't be honest with yourself about your capabilities? </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">This conversation helped. (Even if some of the effect was \"high status person says X -> I believe X,\" who cares? I endorsed myself changing my mind about this a year and a half ago. It's about time.)</span></p>
<p><span style=\"mso-ansi-language: EN-US;\"><a href=\"http://hpmor.com/\">HPMOR</a> helped, too; specifically, the idea that there are four houses which have different positive qualities. Slytherins are demonized in canon, but in HPMOR their skills are recognized as essential. I can easily recognize the Ravenclaw and Hufflepuff and even the Gryffindor in myself, but not much of Slytherin. Having a word for the ambition-cunning-strategic concept cluster is helpful. I can ask myself \"now what would a Slytherin do with this information:?\" I can think thoughts that feel very un-virtuous. \"I'm young and prettier than average. What's a Slytherin way to use this... Oh, I suppose I can leverage it into a comparative advantage for getting high-status men to pay attention to me long enough for me to explain the merits of an idea I have.\" This thought feels yuck, but the universe doesn't explode. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">Probably the biggest factor was going to the CFAR workshops in the first place. Not from any of the curriculum, particularly, although the mindset of goal factoring helped me to realize that the mental action of \"feeling unvirtuous for thinking in ambitious or calculating ways\" wasn't accomplishing anything I wanted. Mostly the change came from social normalization, from hanging out with people who talked openly about their strengths and weaknesses, and no one got shunned. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">[Silly plan for taking over the world: Arrange to meet high status-people and offer to give their children swimming lessons. Gain their trust. Proceed from there.]</span></p>
<h2><span style=\"mso-ansi-language: EN-US;\"><strong>4. Laziness</strong></span></h2>
<p><span style=\"mso-ansi-language: EN-US;\">Nope. Still lazy. If anything, akrasia and procrastination are more of a problem now that I'm trying to do harder things more deliberately. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I've been keeping written goals for about a year now. This means I actually notice when I don't accomplish them. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I use Remember the Milk as a GTD system, and some other productivity/organization software (rescuetime, Mint.com, etc). I finally switched to Gmail, where I can use Boomerang and other useful tools. My current openness to trying new organization methods is high. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">My general interest in <a href=\"/lw/5a5/no_seriously_just_try_it/\">trying things</a> is higher, mainly because I have lots of community-endorsed-warm-fuzzies positive affect around that phrase. I want to be someone who's open to new experiences; I've had enough new experiences to realize how exhilarating they can be.Â </span></p>
<h2><span style=\"mso-ansi-language: EN-US;\"><strong>Conclusion</strong></span></h2>
<p><span style=\"mso-ansi-language: EN-US;\">I now have a wider range of potentially high-value personal projects ongoing. I now have an explicit goal of being well-known for non-fiction writing, probably in a blog form, in the next five years. (Do I have enough interesting things to say to make this a reality? We'll see. Is this goal vague? Yes. Working on it. I used to reject goals if they weren't utterly concrete, but even vague goals are something to build on). </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I'm more explicit with myself about what I want from CFAR curriculum skills. (The general problem of critical thinking in nursing? Solvable! Why not?) </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I think I've finally admitted to myself that \"well, I'll just live in a cozy little house near my parents and work in the ICU and raise kids for the next forty years\" might not be particularly virtuous <em>or </em>fun. There are things I would prefer to be different in the world, even if I can only completely specify a few of them. There are exciting scary opportunities happening all the time. I'm lucky enough to belong to a community of people that can help me find them. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I don't have plans for much beyond the next year. But here's to the next decade being interesting!<br></span></p></div>
<a href=\"http://lesswrong.com/lw/hvw/how_i_became_more_ambitious/#comments\">18 comments</a>
" nil nil "091cf0f014485cf6b87b4f8a6cc13f30") (50 (20950 32319 971076) "http://lesswrong.com/lw/hvw/how_i_became_more_ambitious/" "How I Became More Ambitious" nil "Thu, 04 Jul 2013 23:34:15 +0000" "Submitted by <a href=\"http://lesswrong.com/user/Swimmer963\">Swimmer963</a>
#
28 votes
#
<a href=\"http://lesswrong.com/lw/hvw/how_i_became_more_ambitious/#comments\">17 comments</a>
<div><p>Follow-up to <a href=\"/lw/9j1/how_i_ended_up_nonambitious/\">How I Ended Up Non-Ambitious</a></p>
<p>Living with yourself is a bit like having a preteen and watching them get taller; the changes happen so slowly that it's almost impossible to notice them, until you stumble across an old point of comparison and it becomes blindingly obvious. I hit that point a few days ago, while planning what I might want to talk about during an OkCupid date. My brain produced the following thought: \"well, if this topic comes up, it might sound like I'm trying to take over the world, and that's intimidating- Wait. What?\"</p>
<p>I'm not trying to take over the world. It sounds like a lot of work, and not my comparative advantage. If it seemed necessary, I would point out the problems that needed solving and delegate them to CFAR alumni with more domain-specific expertise than me.</p>
<p>However, I went back and reread the post linked at the beginning, and I no longer feel much kinship with that person. This is a change that happened maybe 25-50% deliberately, and the rest by drift, but I still changed my mind, so I will try to detail the particular changes, and what I think led to them. <a href=\"/lw/5sk/inferring_our_desires/\">Introspection is unreliable</a>, so I'll probably be at least 50% wrong, but what can you do?</p>
<h2><strong>1. Idealism versus practicality</strong></h2>
<p>I would still call myself practical, but I no longer think that this comes at the expense of idealism. Idealism is absolutely essential, if you want to have a world that changes because someone wanted it to, as opposed to just by drift. Lately in the rationalist/CFAR/LW community, there's been a lot of emphasis on <a href=\"/tag/agency/\">agency</a> and agentiness, which basically mean the ability to change the world and/or yourself deliberately, on purpose, through planned actions. This is hard. The first step is idealism-being able to imagine a state of affairs that is different and better. Then comes practicality, the part where you sit down and work hard and actually get something done.</p>
<p>It's still true that idealism without practicality doesn't get much done, and practicality without idealism can get a lot done, but it matters what problems you're working on, too. Are you being <a href=\"/lw/2p5/humans_are_not_automatically_strategic/\">strategic</a>? Are you even thinking, at all, about whether your actions are helping to accomplish your goals? One of the big things I've learned, a year and a half and two CFAR workshops later, is how automatic and easy this lack of strategy really is.</p>
<p>I had a limited sort of idealism in high school; I wanted to do work that was important and relevant; but I was lazy about it. I wanted someone to tell me what was important to be doing right now. Nursing seemed like an awesome solution. It still seems like a solution, but recently I've admitted to myself, with a painful twinge, that it might not be the best way to leverage my comparative advantage and help the most people. It's <a href=\"/lw/8j4/5second_level_case_study_value_of_information/\">worth spending a few minutes or hours</a> looking for interesting and important problems to work on.</p>
<p>I don't think I had the mental vocabulary to think that thought a year and a half ago. Some of the change comes from having dated an economics student. Come to think of it, I expect some of his general ambition rubbed off on me, too. The rest of the change comes from hanging out with the effective altruism and similar communities.</p>
<p>I'm still practical. I exercise, eat well, go to bed on time, work lots of hours, spend my money wisely, and maintain my social circle mostly on autopilot; it requires effort but not deliberate effort. I'm lucky to have this skill. But I no longer think it's a virtue over and above idealism. Practical idealists make the biggest difference, and they're pretty cool to hang out with. I want to be one when I grow up.</p>
<h2><span style=\"mso-ansi-language: EN-US;\">2. Fear of failure</span></h2>
<p><span style=\"mso-ansi-language: EN-US;\">Don't get me wrong. If there's one deep, gripping, soul-crushing terror in my life, one thing that gives me literal nightmares, it's failure. Making mistakes. Not being good enough. Et cetera. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">In the past few years, the main change has been admitting to myself that this terror doesn't make a lot of sense. First of all, it's completely miscalibrated. As Eliezer pointed out during a conversation on this, I don't fail at things very often. Far from being a success, this is likely a sign that the things I'm trying aren't nearly challenging enough. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">My threshold for what constitutes failure is also fairly low. I made a couple of embarrassing mistakes during my spring clinical. Some part of my brain is convinced that this equals <em>permanent </em>failure; I wasn't perfect during the placement, and I can't go back and change the past, thus I have failed. Forever. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I passed the clinical, wrote the provincial exam (results aren't in but I'm >99% confident I passed), and I'm currently working in the intensive care unit, which has been my dream since I was about fifteen. The part of my brain that keeps telling me I failed permanently obviously isn't saying anything useful. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I think 'embarrassing' is a keyword here. The first thing I thought, on the several occasions that I made mistakes, was \"oh my god did I just kill someone... Phew, no, no harm done.\" The second thought was \"oh my god, my preceptor will think I'm stupid forever and she'll never respect me and no one wants me around, I'm not good enough...\" This line of thought never goes anywhere good. It says something about me, though, that \"I'm not good enough\" is very directly connected to people wanting me around, to belonging somewhere. For several personality-formative years of my life, people <em>didn't </em>want me around. Probably for good reason; my ten-year-old self was prickly and socially inept and miserable. I think a lot of my determination not to seek status comes from the \"uncool kids trying to be cool are pathetic\" meme that was so rampant when I was in sixth grade. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">Oh, and then there's the traumatic swim team experience. Somewhere, in a part of my brain where I don't go very often nowadays, there a bottomless whirlpool of powerless rage and despair around the phrase \"no matter how hard I try, I'll never be good enough.\" So when I make an embarrassing mistake, my ten-year-old self is screaming at me \"no wonder everyone hates you!\" and my fourteen-year-old self is sadly muttering that \"you know, maybe you just don't have enough natural talent,\" and none of it is at all useful. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">The thing about those phrases is that they refer to complex and value-laden concepts, in a way that makes them seem like innate attributes, Ã  la <a href=\"/lw/hz/correspondence_bias/\">Fundamental Attribution Error</a>. \"Not good enough\" isn't a yes-or-no attribute of a person; it's a <a href=\"/lw/td/magical_categories/\">magical category</a> that only sounds simple because it's a three-word phrase. I've gotten somewhat better at propagating this to my emotional self. Slightly. It's a work in progress.</span></p>
<p><span style=\"mso-ansi-language: EN-US;\">During a conversation about this with <a href=\"http://annasalamon.com/\">Anna Salamon</a>, she noted that she likes to approach her own emotions and ask them what they want. It sounds weird, but it's helpful. \"Dear crushing sense of despair and unworthiness, what do you want? ...Oh, you're worried that you're going to end up an outcast from your tribe and starve to death in the wilderness because you accidentally gave an extra dose of digoxin? You want to signal remorse and regret and make sure everyone knows you're taking your failure seriously so that maybe they'll forgive you? Thank you for trying to protect me. But really, you don't need to worry about the starving-outcast thing. No one was harmed and no one is mad at you personally. Your friends and family couldn't care less. This mistake is data, but it's just as much data about the environment as it is about your attributes. These hand-copied medication records are the perfect medium for human error. Instead of signalling remorse, let's put some mental energy into getting rid of the environmental conditions that led to this mistake.\"</span></p>
<p><a href=\"http://en.wikipedia.org/wiki/Rejection_Therapy\">Rejection therapy</a> and having a general CoZE [Comfort Zone Expansion] mindset helped remove some of the sting of \"but I'll look stupid if I try something too hard and fail at it!\" I still worry about the pain of future embarrassment, but I'm more likely to point out to myself that it's not a valid objection and I should do X anyway. Making \"<a href=\"/lw/h8/tsuyoku_naritai_i_want_to_become_stronger/\">I want to become stronger</a>\" an explicit motto is new to the last year and a half, too, and helps by giving me ammunition for why potential embarrassment isn't a reason not to do something.</p>
<p>In conclusion: failure still sucks. I'm a perfectionist. But I failed in a lot of small ways during my <a href=\"/lw/gnv/learning_critical_thinking_a_personal_example/\">spring clinical</a>, and passed/got a job anyway, which seems to have helped me propagate to my emotional self that<em> it's okay to try hard things</em>, where I'm almost certain to make mistakes, because mistakes don't equal instant damnation and hatred from all of my friends.</p>
<h2><span style=\"mso-ansi-language: EN-US;\"><strong>3. The morality of ambition</strong></span></h2>
<p><span style=\"mso-ansi-language: EN-US;\">While I was in San Francisco a month ago, volunteering at the CFAR workshop and generally spending my time surrounded by smart, passionate, and ambitious people (thus convincing my emotional system that this is normal and okay), I had a conversation with Eliezer. He asked me to list ten areas where I had a comparative advantage; ten things in which I was above average. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">This was a lot more painful than it had any reason to be. After bouncing off various poorly-formed objections in my mind, I said to myself \"you know, having trouble admitting what you're good at doesn't make you virtuous.\" This was painful; losing a source of feeling-virtuous always is. But it was helpful. Yeah, talking all the time about how awesome you are at X, Y, Z makes you a bit of a bore. People might even avoid you (oh! the horror!). However, this doesn't mean that blocking even the<em> thought </em>of being above average makes you a good person. In fact, it's counterproductive. How are you supposed to know what problems you're capable of solving in the world if you can't be honest with yourself about your capabilities? </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">This conversation helped. (Even if some of the effect was \"high status person says X -> I believe X,\" who cares? I endorsed myself changing my mind about this a year and a half ago. It's about time.)</span></p>
<p><span style=\"mso-ansi-language: EN-US;\"><a href=\"http://hpmor.com/\">HPMOR</a> helped, too; specifically, the idea that there are four houses which have different positive qualities. Slytherins are demonized in canon, but in HPMOR their skills are recognized as essential. I can easily recognize the Ravenclaw and Hufflepuff and even the Gryffindor in myself, but not much of Slytherin. Having a word for the ambition-cunning-strategic concept cluster is helpful. I can ask myself \"now what would a Slytherin do with this information:?\" I can think thoughts that feel very un-virtuous. \"I'm young and prettier than average. What's a Slytherin way to use this... Oh, I suppose I can leverage it into a comparative advantage for getting high-status men to pay attention to me long enough for me to explain the merits of an idea I have.\" This thought feels yuck, but the universe doesn't explode. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">Probably the biggest factor was going to the CFAR workshops in the first place. Not from any of the curriculum, particularly, although the mindset of goal factoring helped me to realize that the mental action of \"feeling unvirtuous for thinking in ambitious or calculating ways\" wasn't accomplishing anything I wanted. Mostly the change came from social normalization, from hanging out with people who talked openly about their strengths and weaknesses, and no one got shunned. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">[Silly plan for taking over the world: Arrange to meet high status-people and offer to give their children swimming lessons. Gain their trust. Proceed from there.]</span></p>
<h2><span style=\"mso-ansi-language: EN-US;\"><strong>4. Laziness</strong></span></h2>
<p><span style=\"mso-ansi-language: EN-US;\">Nope. Still lazy. If anything, akrasia and procrastination are more of a problem now that I'm trying to do harder things more deliberately. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I've been keeping written goals for about a year now. This means I actually notice when I don't accomplish them. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I use Remember the Milk as a GTD system, and some other productivity/organization software (rescuetime, Mint.com, etc). I finally switched to Gmail, where I can use Boomerang and other useful tools. My current openness to trying new organization methods is high. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">My general interest in <a href=\"/lw/5a5/no_seriously_just_try_it/\">trying things</a> is higher, mainly because I have lots of community-endorsed-warm-fuzzies positive affect around that phrase. I want to be someone who's open to new experiences; I've had enough new experiences to realize how exhilarating they can be.Â </span></p>
<h2><span style=\"mso-ansi-language: EN-US;\"><strong>Conclusion</strong></span></h2>
<p><span style=\"mso-ansi-language: EN-US;\">I now have a wider range of potentially high-value personal projects ongoing. I now have an explicit goal of being well-known for non-fiction writing, probably in a blog form, in the next five years. (Do I have enough interesting things to say to make this a reality? We'll see. Is this goal vague? Yes. Working on it. I used to reject goals if they weren't utterly concrete, but even vague goals are something to build on). </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I'm more explicit with myself about what I want from CFAR curriculum skills. (The general problem of critical thinking in nursing? Solvable! Why not?) </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I think I've finally admitted to myself that \"well, I'll just live in a cozy little house near my parents and work in the ICU and raise kids for the next forty years\" might not be particularly virtuous <em>or </em>fun. There are things I would prefer to be different in the world, even if I can only completely specify a few of them. There are exciting scary opportunities happening all the time. I'm lucky enough to belong to a community of people that can help me find them. </span></p>
<p><span style=\"mso-ansi-language: EN-US;\">I don't have plans for much beyond the next year. But here's to the next decade being interesting!<br></span></p></div>
<a href=\"http://lesswrong.com/lw/hvw/how_i_became_more_ambitious/#comments\">17 comments</a>
" nil nil "03f47b7ecaffcf08d016668d49d8005f") (49 (20950 32319 968715) "http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/" "Rationality Quotes July 2013" nil "Tue, 02 Jul 2013 16:21:59 +0000" "Submitted by <a href=\"http://lesswrong.com/user/Vaniver\">Vaniver</a>
#
3 votes
#
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">154 comments</a>
<div><div id=\"entry_t3_hlk\" class=\"content clear\">
<div class=\"md\">
<div>
<div>
<p>Another month has passed and here is a new rationality quotes thread. The usual rules are:</p>
<ul>
<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>
<li>Do not quote yourself.</li>
<li>Do not quote from Less Wrong itself, HPMoR, Eliezer Yudkowsky, or Robin Hanson.</li>
<li>No more than 5 quotes per person per monthly thread, please.</li>
</ul>
</div>
</div>
</div>
</div></div>
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">154 comments</a>
" nil nil "77008aa4134cfa245dafa198b1b91b6b") (48 (20949 41317 441487) "http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/" "Rationality Quotes July 2013" nil "Tue, 02 Jul 2013 16:21:59 +0000" "Submitted by <a href=\"http://lesswrong.com/user/Vaniver\">Vaniver</a>
#
3 votes
#
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">142 comments</a>
<div><div id=\"entry_t3_hlk\" class=\"content clear\">
<div class=\"md\">
<div>
<div>
<p>Another month has passed and here is a new rationality quotes thread. The usual rules are:</p>
<ul>
<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>
<li>Do not quote yourself.</li>
<li>Do not quote from Less Wrong itself, HPMoR, Eliezer Yudkowsky, or Robin Hanson.</li>
<li>No more than 5 quotes per person per monthly thread, please.</li>
</ul>
</div>
</div>
</div>
</div></div>
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">142 comments</a>
" nil nil "e9a6850b3423081e4334a55cb630ae20") (47 (20949 39384 955797) "http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/" "Rationality Quotes July 2013" nil "Tue, 02 Jul 2013 16:21:59 +0000" "Submitted by <a href=\"http://lesswrong.com/user/Vaniver\">Vaniver</a>
#
3 votes
#
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">141 comments</a>
<div><div id=\"entry_t3_hlk\" class=\"content clear\">
<div class=\"md\">
<div>
<div>
<p>Another month has passed and here is a new rationality quotes thread. The usual rules are:</p>
<ul>
<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>
<li>Do not quote yourself.</li>
<li>Do not quote from Less Wrong itself, HPMoR, Eliezer Yudkowsky, or Robin Hanson.</li>
<li>No more than 5 quotes per person per monthly thread, please.</li>
</ul>
</div>
</div>
</div>
</div></div>
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">141 comments</a>
" nil nil "d16f8ed7b6046a80ef3c229db64f7b24") (46 (20949 35302 47480) "http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/" "Rationality Quotes July 2013" nil "Tue, 02 Jul 2013 16:21:59 +0000" "Submitted by <a href=\"http://lesswrong.com/user/Vaniver\">Vaniver</a>
#
3 votes
#
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">139 comments</a>
<div><div id=\"entry_t3_hlk\" class=\"content clear\">
<div class=\"md\">
<div>
<div>
<p>Another month has passed and here is a new rationality quotes thread. The usual rules are:</p>
<ul>
<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>
<li>Do not quote yourself.</li>
<li>Do not quote from Less Wrong itself, HPMoR, Eliezer Yudkowsky, or Robin Hanson.</li>
<li>No more than 5 quotes per person per monthly thread, please.</li>
</ul>
</div>
</div>
</div>
</div></div>
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">139 comments</a>
" nil nil "4591a647f192ef5eb6b8b7a8a8552512") (45 (20949 25625 815736) "http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/" "Rationality Quotes July 2013" nil "Tue, 02 Jul 2013 16:21:59 +0000" "Submitted by <a href=\"http://lesswrong.com/user/Vaniver\">Vaniver</a>
#
3 votes
#
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">136 comments</a>
<div><div id=\"entry_t3_hlk\" class=\"content clear\">
<div class=\"md\">
<div>
<div>
<p>Another month has passed and here is a new rationality quotes thread. The usual rules are:</p>
<ul>
<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>
<li>Do not quote yourself.</li>
<li>Do not quote from Less Wrong itself, HPMoR, Eliezer Yudkowsky, or Robin Hanson.</li>
<li>No more than 5 quotes per person per monthly thread, please.</li>
</ul>
</div>
</div>
</div>
</div></div>
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">136 comments</a>
" nil nil "f2cbf512b2c2ba783eb69fecdac11295") (44 (20949 25625 814428) "http://lesswrong.com/lw/hui/new_lw_meetup_lyon/" "New LW Meetup: Lyon" nil "Sat, 29 Jun 2013 08:41:08 +1000" "Submitted by <a href=\"http://lesswrong.com/user/FrankAdamek\">FrankAdamek</a>
#
4 votes
#
<a href=\"http://lesswrong.com/lw/hui/new_lw_meetup_lyon/#comments\">4 comments</a>
<div><p>New meetups (or meetups with a hiatus of more than a year) are happening in:</p>
<ul>
<li><a href=\"/meetups/o1\">[Lyon, France] LW Meetup in Lyon:Â <span class=\"date\">03 July 2013 06:00PM</span></a></li>
</ul>
<p>Other irregularly scheduled Less Wrong meetups are taking place in:</p>
<ul>
<li><a href=\"/meetups/ns\">Brussels meetup with HEALES:Â <span class=\"date\">13 July 2013 01:00PM</span></a></li>
<li><a href=\"/meetups/o2\">Frankfurt meetup:Â <span class=\"date\">30 June 2013 04:30PM</span></a></li>
<li><a href=\"/meetups/o5\">Israel LW meetup:Â <span class=\"date\">04 July 2013 07:00PM</span></a></li>
<li><a href=\"/meetups/o3\">[Moscow] The Goals We Set:Â <span class=\"date\">07 July 2013 04:00PM</span></a></li>
<li><a href=\"/meetups/o4\">[Munich] LW Munich Meetup in July:Â <span class=\"date\">06 July 2013 03:00PM</span></a></li>
<li><a href=\"/meetups/ny\">San Francisco: Effective Altruism:Â <span class=\"date\">28 June 2013 07:54PM</span></a></li>
<li><a href=\"/meetups/o8\">[Vienna] LW Vienna Meetup #4:Â <span class=\"date\">13 July 2013 03:00PM</span></a></li>
</ul>
<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>
<ul>
<li><a href=\"/meetups/bx\">Austin, TX:Â <span class=\"date\">29 June 2019 01:30PM</span></a></li>
<li><a href=\"http://lesswrong.com/meetups/o6\">London Practical - Sunday 7th July:Â <span class=\"date\">07 July 2013 02:00PM</span></a></li>
<li><a href=\"/meetups/o7\">Melbourne LW Outing: Astronomy evening in Eltham, Saturday 29th June, 5:30pm:Â <span class=\"date\">29 June 2013 05:30PM</span></a></li>
<li><a href=\"/meetups/nz\">Melbourne LW Outing: Indoor Rock Climbing, Sunday June 30th:Â <span class=\"date\">30 June 2013 02:00PM</span></a></li>
</ul>
<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>
<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>
<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>
<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>
<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetupÂ <em>before </em>the Friday before your meetup!</p>
<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong style=\"font-weight: bold;\"></strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,Â <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>
<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>
<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>
<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p></div>
<a href=\"http://lesswrong.com/lw/hui/new_lw_meetup_lyon/#comments\">4 comments</a>
" nil nil "abd7b0ae93276a38b3c79f0e3b181ada") (43 (20949 25625 812300) "http://lesswrong.com/lw/hmw/robust_cooperation_in_the_prisoners_dilemma/" "Robust Cooperation in the Prisoner's Dilemma" nil "Fri, 07 Jun 2013 18:30:25 +1000" "Submitted by <a href=\"http://lesswrong.com/user/orthonormal\">orthonormal</a>
#
64 votes
#
<a href=\"http://lesswrong.com/lw/hmw/robust_cooperation_in_the_prisoners_dilemma/#comments\">113 comments</a>
<div><p>I'm proud to announce the preprint of <a href=\"http://intelligence.org/files/RobustCooperation.pdf\" target=\"_blank\">Robust Cooperation in the Prisoner's Dilemma: Program Equilibrium via Provability Logic</a>, a joint paper with Patrick LaVictoire (me), Mihaly Barasz, Paul Christiano, Benja Fallenstein, Marcello Herreshoff, and Eliezer Yudkowsky.</p>
<p>This paper was one of three projects to come out of the <a href=\"http://intelligence.org/2013/03/07/upcoming-miri-research-workshops/\">2nd MIRI Workshop on Probability and Reflection</a> in April 2013, and had its genesis in ideas about formalizations of decision theory that have appeared on LessWrong. (At the end of this post, I'll include links for further reading.)</p>
<p>Below, I'll briefly outline the problem we considered, the results we proved, and the (many) open questions that remain. Thanks in advance for your thoughts and suggestions!</p>
<h2>Background: Writing programs to play the PD with source code swap</h2>
<p>(If you're not familiar with the Prisoner's Dilemma, <a href=\"http://wiki.lesswrong.com/wiki/Prisoner's_dilemma\">see here.</a>)</p>
<p>The paper concerns the following setup, <a href=\"/r/all/lw/duv/ai_cooperation_is_already_studied_in_academia_as/\">which has come up in academic research on game theory</a>: say that you have the chance to write a computer program <strong>X</strong>, which takes in one input and returns either <em>Cooperate</em> or <em>Defect</em>. This program will face off against some other computer program <strong>Y</strong>, but with a twist: <strong>X</strong> will receive the source code of <strong>Y</strong> as input, and <strong>Y</strong> will receive the source code of <strong>X</strong> as input. And you will be given your program's winnings, so you should think carefully about what sort of program you'd write!</p>
<p>Of course, you could simply write a program that defects regardless of its input; we call this program <strong>DefectBot</strong>, and call the program that cooperates on all inputs <strong>CooperateBot</strong>. But with the wealth of information afforded by the setup, you might wonder if there's some program that might be able to achieve mutual cooperation in situations where <strong>DefectBot</strong> achieves mutual defection, without thereby risking a sucker's payoff. (Douglas Hofstadter would call this a perfect opportunity for <a href=\"http://www.gwern.net/docs/1985-hofstadter\">superrationality</a>...)</p>
<h2>Previously known: CliqueBot and FairBot</h2>
<p>And indeed, there's a way to do this that's been known since at least the 1980s. You can write <a href=\"http://en.wikipedia.org/wiki/Quine_(computing)\">a computer program that knows its own source code</a>, compares it to the input, and returns <em>C</em> if and only if the two are identical (and <em>D</em> otherwise). Thus it achieves mutual cooperation in one important case where it intuitively ought to: when playing against itself! We call this program <strong>CliqueBot</strong>, since it cooperates only with the \"clique\" of agents identical to itself.</p>
<p>There's one particularly irksome issue with <strong>CliqueBot</strong>, and that's the fragility of its cooperation. If two people write functionally analogous but syntactically different versions of it, those programs will defect against one another! This problem can be patched somewhat, but not fully fixed. Moreover, mutual cooperation might be the best strategy against some agents that are not even functionally identical, and extending this approach requires you to explicitly delineate the list of programs that you're willing to cooperate with. Is there a more flexible and robust kind of program you could write instead?</p>
<p>As it turns out, there is: <a href=\"/lw/2ip/ai_cooperation_in_practice/\">in a 2010 post on LessWrong</a>, cousin_it introduced an algorithm that we now call <strong>FairBot</strong>. Given the source code of <strong>Y</strong>, <strong>FairBot</strong> searches for a proof (of less than some large fixed length) that <strong>Y</strong> returns <em>C</em> when given the source code of <strong>FairBot</strong>, and then returns <em>C</em> if and only if it discovers such a proof (otherwise it returns <em>D</em>). Clearly, if our proof system is consistent, <strong>FairBot</strong> only cooperates when that cooperation will be mutual. But the really fascinating thing is what happens when you play two versions of <strong>FairBot</strong> against each other. Intuitively, it seems that <em>either</em> mutual cooperation or mutual defection would be stable outcomes, but it turns out that if their limits on proof lengths are sufficiently high, they will achieve mutual cooperation!</p>
<p>The proof that they mutually cooperate follows from a bounded version of <a href=\"http://en.wikipedia.org/wiki/L%C3%B6b's_theorem\">LÃ¶b's Theorem</a>Â from mathematical logic. (If you're not familiar with this result, you might enjoy <a href=\"/lw/t6/the_cartoon_guide_to_l%C3%B6bs_theorem/\">Eliezer's Cartoon Guide to LÃ¶b's Theorem</a>, which is a correct formal proof written in much more intuitive notation.) Essentially, the asymmetry comes from the fact that both programs are searching for the same outcome, so that a short proof that one of them cooperates leads to a short proof that the other cooperates, and vice versa. (The opposite is not true, because <a href=\"/lw/t8/you_provably_cant_trust_yourself/\">the formal system can't know it won't find a contradiction</a>. This is a subtle but essential feature of mathematical logic!)</p>
<h2>Generalization: Modal Agents</h2>
<p>Unfortunately, <strong>FairBot</strong> isn't what I'd consider an ideal program to write: it happily cooperates with <strong>CooperateBot</strong>, when it could do better by defecting. ThisÂ is problematic because in real life, the world isn't separated into agents and non-agents, and any natural phenomenon that doesn't predict your actions can be thought of as aÂ <strong>CooperateBot</strong>Â (or aÂ <strong>DefectBot</strong>). You don't want your agent to be making concessions to rocks that happened not to fall on them. (There's an important caveat: some things have utility functions that you care about, but don't have sufficient ability to predicate their actions on yours. In that case, though, itÂ <a href=\"/lw/tn/the_true_prisoners_dilemma/\">wouldn't be a true Prisoner's Dilemma</a>Â if your values actually prefer the outcome (<em>C</em>,<em>C</em>) to (<em>D</em>,<em>C</em>).)</p>
<p>However, <strong>FairBot</strong> belongs to a promising class of algorithms: those that decide on their action by looking for short proofs of logical statements that concern their opponent's actions. In fact, there's a really convenient mathematical structure that's analogous to the class of such algorithms: the <a href=\"http://plato.stanford.edu/entries/logic-provability/\">modal logic of provability</a> (known as GL, for GÃ¶del-LÃ¶b).</p>
<p>So that's the subject of this preprint: <strong>what can we achieve in decision theory by considering agents defined by formulas of provability logic?</strong><a id=\"more\"></a></p>
<p>More formally <em>(skip the next two paragraphs if you're willing to trust me)</em>, we inductively define the class of \"modal agents\" as formulas using propositional variables and <a href=\"http://en.wikipedia.org/wiki/Logical_connective\">logical connectives</a> and the modal operatorÂ <img src=\"http://www.codecogs.com/png.latex?%5CBox\" alt=\"\">Â <span style=\"font-family: sans-serif; font-size: 13px; line-height: 19.1875px;\">(which represents provability in some base-level formal system like Peano Arithmetic), of the formÂ <img src=\"http://www.codecogs.com/png.latex?P%5Cleftrightarrow%20%5Cvarphi(P,Q,R_1,%5Cdots,R_N)\" alt=\"\">, whereÂ <img src=\"http://www.codecogs.com/png.latex?%5Cvarphi\" alt=\"\" height=\"15\" width=\"12\">Â is fully modalized (i.e. all instances of variables are contained in an expressionÂ <img src=\"http://www.codecogs.com/png.latex?%5CBox%5Cpsi\" alt=\"\">), and with eachÂ <img src=\"http://www.codecogs.com/png.latex?R_i\" alt=\"\" height=\"16\" width=\"18\">Â corresponding to a fixed modal agent of lower rank. For example, <strong>FairBot</strong> is represented by the modal formulaÂ <img src=\"http://www.codecogs.com/png.latex?P%5Cleftrightarrow%20%5CBox%20Q\" alt=\"\">.</span></p>
<p><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19.1875px;\">When two modal agents play against each other, the outcome is given by the unique fixed point of the system of modal statements, where the variables are identified with each other so thatÂ <img src=\"http://www.codecogs.com/png.latex?P\" alt=\"\" height=\"13\" width=\"14\">Â represents the expressionÂ <img src=\"http://www.codecogs.com/png.latex?X(Y)=C\" alt=\"\" height=\"19\" width=\"83\">,Â <img src=\"http://www.codecogs.com/png.latex?Q\" alt=\"\" height=\"18\" width=\"15\">Â representsÂ <img src=\"http://www.codecogs.com/png.latex?Y(X)=C\" alt=\"\" height=\"19\" width=\"83\">, and theÂ <img src=\"http://www.codecogs.com/png.latex?R_i\" alt=\"\" height=\"16\" width=\"18\">Â represent the actions of lower-rank modal agents againstÂ <img src=\"http://www.codecogs.com/png.latex?Y\" alt=\"\" height=\"13\" width=\"14\">Â and vice-versa. (Modal rank is defined as a natural number, so this always bottoms out in a finite number of modal statements; also, we interpret outcomes as statements of provability in Peano Arithmetic, evaluated in the model where PA is consistent, PA+Con(PA) is consistent, and so on. See the paper for the actual details.)</span></p>
<p><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19.1875px;\">The nice part about modal agents is that there are <a href=\"http://en.wikipedia.org/wiki/Kripke_semantics\">simple tools</a> for finding the fixed points without having to search through proofs; in fact, Mihaly and Marcello wrote up a computer program to deduce the outcome of the source-code-swap Prisoner's Dilemma between any two (reasonably simple) modal agents. These tools also made it much easier to prove general theorems about such agents.</span></p>
<h2>PrudentBot: The best of both worlds?</h2>
<p>Can we find a modal agent that seems to improve on <strong>FairBot</strong>? In particular, we should want at least the following properties:</p>
<ul>
<li>It should be un-exploitable: if our axioms are consistent in the first place, then it had better only end up cooperating when it's mutual.</li>
<li>It should cooperate with itself, and also mutually cooperate with <strong>FairBot</strong> (both are, common-sensically, the best actions in those cases).</li>
<li>It should defect, however, against <strong>CooperateBot</strong> and lots of similarly exploitable modal agents.</li>
</ul>
<p>It's nontrivial that such an agent exists: you may remember the post I wrote aboutÂ <a href=\"/lw/ebx/decision_theories_part_375_hang_on_i_think_this/\">the Masquerade agent</a>, which is a modal agent that does <em>almost</em> all of those things (it doesn't cooperate with the original <strong>FairBot</strong>, though it does cooperate with some more complicated variants), and indeed we didn't find anything better until after we had Mihaly and Marcello's modal-agent-evaluator to help us.</p>
<p>But as it turns out, there is such an agent, and it's pretty elegant: we call it <strong>PrudentBot</strong>, and its modal version cooperates with another agent <strong>Y</strong> if and only if (there's a proof in Peano Arithmetic that <strong>Y</strong> cooperates with <strong>PrudentBot</strong> and there's a proof in PA+Con(PA) that <strong>Y</strong> defects against <strong>DefectBot</strong>). This agent can be seen to satisfy all of our criteria. But is it <em>optimal</em> among modal agents, by any reasonable criterion?</p>
<h2>Results: Obstacles to Optimality</h2>
<p>It turns out that, even within the class of modal agents, it's hard to formulate a definition of optimality that's actually true of something, and which meaningfully corresponds to our intuitions about the \"right\" decisions on decision-theoretic problems. (This intuition is not formally defined, so I'm using scare quotes.)</p>
<p>There are agents that give preferential treatment to <strong>DefectBot</strong>, <strong>FairBot</strong>, or even <strong>CooperateBot</strong>, compared to <strong>PrudentBot</strong>, though these agents are not ones you'd program in an attempt to win at the Prisoner's Dilemma. (For instance, one agent that rewards <strong>CooperateBotÂ </strong>over <strong>PrudentBot</strong> is the agent that cooperates with <strong>Y</strong> iff PA proves that <strong>Y</strong> cooperates against <strong>DefectBot</strong>; we've taken to jokingly calling that agent <strong>TrollBot</strong>.) One might well suppose that a modal agent could still be optimal in the sense of making the \"right\" decision in every case, regardless of whether it's being punished for some other decision. However, this is not the only obstacle to a useful concept of optimality.</p>
<p>The second obstacle is that any modal agent only checks proofs at some finite number of levels on the hierarchy of formal systems, and agents that appear indistinguishable at all those levels may have obviously different \"right\" decisions. And thirdly, an agent might mimic another agent in such a way that the \"right\" decision is to treat the mimic differently from the agent it imitates, but in some cases one can prove that no modal agent can treat the two differently.</p>
<p>These three strikes appear to indicate that if we're looking to formalize <a href=\"http://wiki.lesswrong.com/wiki/Decision_theory\">more advanced decision theories</a>, modal agents are too restrictive of a class to work with. We might instead allow things like quantifiers over agents, which would invalidate these specific obstacles, but may well introduce new ones (and certainly would make for more complicated proofs). But for a \"good enough\" algorithm on the original problem (assuming that the computer will have lots of computational resources), one could definitely do worse than submit a finite version of <strong>PrudentBot</strong>.</p>
<h2>Why is this awesome, and what's next?</h2>
<p>In my opinion, the result of LÃ¶bian cooperation deserves to be published for its illustration of Hofstadterian superrationality in action, apart from anything else! It's <em>really cool</em> that two agents reasoning about each other can in theory come to mutual cooperation for genuine reasons that don't have to involve being clones of each other (or other anthropic dodges). It's a far cry from a practical approach, of course, but it's a start: mathematicians always begin with a simplified and artificial model to see what happens, then add complications one at a time.</p>
<p>As for what's next: First, we don't <em>actually</em> know that there's no meaningful non-vacuous concept of optimality for modal agents; it would be nice to know that one way or another. Secondly, we'd like to see if some other class of agents contains a simple example with really nice properties (the way that classical game theory doesn't always have a pure Nash equilibrium, but always has a mixed one). Thirdly, we might hope that there's an actual implementation of a decision theory (<a href=\"http://wiki.lesswrong.com/wiki/Timeless_decision_theory\">TDT</a>, <a href=\"http://wiki.lesswrong.com/wiki/Updateless_decision_theory\">UDT</a>, etc) in the context of program equilibrium.</p>
<p>If we succeed in the positive direction on any of those, we'd next want to extend them in several important ways: using probabilistic information rather than certainty, considering more general games than the Prisoner's Dilemma (bargaining games have many further challenges, and games of more than two players could be more convoluted still), etc. I personally hope to work on such topics in future MIRI workshops.</p>
<h2>Further Reading on LessWrong</h2>
<p>Here are some LessWrong posts that have tackled similar material to the preprint:</p>
<ul>
<li><a href=\"/lw/2ip/ai_cooperation_in_practice/\">AI cooperation in practice</a>, cousin_it, 2010</li>
<li><a href=\"/lw/2tq/notion_of_preference_in_ambient_control/\">Notion of Preference in Ambient Control</a>, Vladimir_Nesov, 2010</li>
<li><a href=\"/lw/8wc/a_model_of_udt_with_a_halting_oracle/\">A model of UDT with a halting oracle</a>, cousin_it, 2011</li>
<li><a href=\"/lw/9o7/formulas_of_arithmetic_that_behave_like_decision/\">Formulas of arithmetic that behave like decision agents</a>, Nisan, 2012</li>
<li><a href=\"/lw/b0e/a_model_of_udt_without_proof_limits/\">A model of UDT without proof limits</a>, <a href=\"/lw/b5t/an_example_of_selffulfilling_spurious_proofs_in/\">An example of self-fulfilling spurious proofs in UDT</a>, <a href=\"/lw/crx/loebian_cooperation_version_2/\">LÃ¶bian cooperation, version 2</a>, <a href=\"/lw/dba/bounded_versions_of_g%C3%B6dels_and_l%C3%B6bs_theorems/\">Bounded versions of GÃ¶del's and LÃ¶b's theorems</a>, cousin_it, 2012</li>
<li><a href=\"/lw/ap3/predictability_of_decisions_and_the_diagonal/\">Predictability of decisions and the diagonal method</a>, <a href=\"/lw/ca5/consequentialist_formal_systems/\">Consequentialist formal systems</a>, Vladimir_Nesov, 2012</li>
<li>Decision Theories: A Semi-Formal Analysis: <a href=\"/lw/aq9/decision_theories_a_less_wrong_primer/\">Part 0 (A LessWrong Primer)</a>, <a href=\"/lw/axl/decision_theories_a_semiformal_analysis_part_i\">Part 1 (The Problem with Naive Decision Theory)</a>, <a href=\"/lw/az6/decision_theories_a_semiformal_analysis_part_ii/\">Part 2 (Causal Decision Theory and Substitution)</a>, <a href=\"/lw/b7w/decision_theories_a_semiformal_analysis_part_iii/\">Part 3 (Formalizing Timeless Decision Theory)</a>, <a href=\"/lw/e94/decision_theories_part_35_halt_melt_and_catch_fire/\">Part 3.5 (Halt, Melt, and Catch Fire)</a>, <a href=\"/lw/ebx/decision_theories_part_375_hang_on_i_think_this/\">Part 3.75 (Hang On, I Think This Works After All)</a>, orthonormal, 2012</li>
</ul></div>
<a href=\"http://lesswrong.com/lw/hmw/robust_cooperation_in_the_prisoners_dilemma/#comments\">113 comments</a>
" nil nil "e489791eec1a1bb74d75218b9882ca72") (42 (20949 25625 809577) "http://lesswrong.com/lw/hmx/prisoners_dilemma_with_visible_source_code/" "Prisoner's Dilemma (with visible source code) Tournament" nil "Fri, 07 Jun 2013 18:30:20 +1000" "Submitted by <a href=\"http://lesswrong.com/user/AlexMennen\">AlexMennen</a>
#
47 votes
#
<a href=\"http://lesswrong.com/lw/hmx/prisoners_dilemma_with_visible_source_code/#comments\">229 comments</a>
<div><p style=\"margin-bottom: 0in\">After the <a href=\"/lw/7f2/prisoners_dilemma_tournament_results/\">iterated prisoner's dilemma tournament</a>Â organized by prase two years ago, there was discussion of running tournaments for several variants, including one in which two players submit programs, each of which are given the source code of the other player's program, and outputs either âcooperateâ or âdefectâ. However, as far as I know, no such tournament has been run until now.</p>
<p style=\"margin-bottom: 0in\">Here's how it's going to work: Each player will submit a file containing a single Scheme lambda-function. The function should take one input. Your program will play exactly one round against each other program submitted (not including itself). In each round, two programs will be run, each given the source code of the other as input, and will be expected to return either of the symbols âCâ or âDâ (for \"cooperate\" and \"defect\", respectively). The programs will receive points based on the following payoff matrix:</p>
<p style=\"margin-bottom: 0in\"><img src=\"http://www.codecogs.com/png.latex?%5Cbegin%7Barray%7D%7Bcccc%7D%20&%20C%20&%20D%20&%20other%5C%5C%20C%20&%20(2,%5C,2)%20&%20(0,%5C,3)%20&%20(0,%5C,2)%5C%5C%20D%20&%20(3,%5C,0)%20&%20(1,%5C,1)%20&%20(1,%5C,0)%5C%5C%20other%20&%20(2,%5C,0)%20&%20(0,%5C,1)%20&%20(0,%5C,0)%20%5Cend%7Barray%7D\" alt=\"\" height=\"84\" width=\"215\"></p>
<p style=\"margin-bottom: 0in\">âOtherâ includes any result other than returning âCâ or âDâ, including failing to terminate, throwing an exception, and even returning the string âCooperateâ. Notice that âOtherâ results in a worst-of-both-worlds scenario where you get the same payoff as you would have if you cooperated, but the other player gets the same payoff as if you had defected. This is an attempt to ensure that no one ever has incentive for their program to fail to run properly, or to trick another program into doing so.</p>
<p style=\"margin-bottom: 0in\">Your score is the sum of the number of points you earn in each round. The player with the highest score wins the tournament. <strong>Edit: There is a <a href=\"/lw/hmx/prisoners_dilemma_with_visible_source_code/94no\">0.5 bitcoin prize</a> being offered for the winner. Thanks, VincentYu!</strong></p>
<p style=\"margin-bottom: 0in\">Details:<br>All submissions must be emailed to <a href=\"mailto:wardenPD@gmail.com\">wardenPD@gmail.com</a> by July 5, at noon PDT (Edit: that's 19:00 UTC). Your email should also say how you would like to be identified when I announce the tournament results.<br>Each program will be allowed to run for 10 seconds. If it has not returned either âCâ or âDâ by then, it will be stopped, and treated as returning âOtherâ. For consistency, I will have Scheme collect garbage right before each run.<br>One submission per person or team. No person may contribute to more than one entry. <strong>Edit: This also means no copying from each others' source code. Describing the behavior of your program to others is okay.</strong><br>I will be running the submissions in Racket. You may be interested in how Racket handles <a href=\"http://docs.racket-lang.org/reference/time.html\">time</a>Â (especially the (current-milliseconds) function), <a href=\"http://docs.racket-lang.org/reference/threads.html\">threads</a>Â (in particular, âthreadâ, âkill-threadâ, âsleepâ, and âthread-dead?â), and possibly <a href=\"http://docs.racket-lang.org/reference/generic-numbers.html#%28def._%28%28quote._~23~25kernel%29._random%29%29\">randomness</a>.<br>Don't try to open the file you wrote your program in (or any other file, for that matter). I'll add code to the file before running it, so if you want your program to use a copy of your source code, you will need to use a quine. <strong>Edit: No I/O of any sort.</strong><br>Unless you tell me otherwise, I assume I have permission to publish your code after the contest.<br>You are encouraged to discuss strategies for achieving mutual cooperation in the comments thread.<br>I'm hoping to get as many entries as possible. If you know someone who might be interested in this, please tell them.<br>It's possible that I've said something stupid that I'll have to change or clarify, so you might want to come back to this page again occasionally to look for changes to the rules. Any edits will be bolded, and I'll try not to change anything too drastically, or make any edits late in the contest.</p>
<p style=\"margin-bottom: 0in\">Here is an example of a correct entry, which cooperates with you if and only if you would cooperate with a program that always cooperates (actually, if and only if you would cooperate with one particular program that always cooperates):</p>
<blockquote>
<p style=\"margin-bottom: 0in;\">(lambda (x)<br>Â  Â  (if (eq? ((eval x) '(lambda (y) 'C)) 'C)<br>Â  Â  Â  Â  'C<br>Â  Â  Â  Â  'D))</p>
</blockquote></div>
<a href=\"http://lesswrong.com/lw/hmx/prisoners_dilemma_with_visible_source_code/#comments\">229 comments</a>
" nil nil "1fa4b2756c4bd69b4a1dcd15c19372b1") (41 (20949 25625 808617) "http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/" "Tiling Agents for Self-Modifying AI (OPFAI #2)" nil "Fri, 07 Jun 2013 06:24:25 +1000" "Submitted by <a href=\"http://lesswrong.com/user/Eliezer_Yudkowsky\">Eliezer_Yudkowsky</a>
#
51 votes
#
<a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/#comments\">250 comments</a>
<div><p>An early draft of publication #2 in the Open Problems in Friendly AI series is now available: Â <a href=\"http://intelligence.org/files/TilingAgents.pdf\">Tiling Agents for Self-Modifying AI, and the Lobian Obstacle</a>.Â  ~20,000 words, aimed at mathematicians or the highly mathematically literate.Â  The research reported on was conducted by Yudkowsky and Herreshoff, substantially refined at the November 2012 MIRI Workshop with Mihaly Barasz and Paul Christiano, and refined further at the April 2013 MIRI Workshop.</p>
<p style=\"padding-left: 60px;\"><strong>Abstract:</strong></p>
<p style=\"padding-left: 30px;\">We model self-modication in AI by introducing 'tiling' agents whose decision systems will approve the construction of highly similar agents, creating a repeating pattern (including similarity of the offspring's goals). Â Constructing a formalism in the most straightforward way produces a Godelian difficulty, the Lobian obstacle. Â By technical methods we demonstrate the possibility of avoiding this obstacle, but the underlying puzzles of rational coherence are thus only partially addressed. Â We extend the formalism to partially unknown deterministic environments, and show a very crude extension to probabilistic environments and expected utility; but the problem of finding a fundamental decision criterion for self-modifying probabilistic agents remains open.</p>
<p>Commenting here is the preferred venue for discussion of the paper. Â This is an early draft and has not been reviewed, so it may contain mathematical errors, and reporting of these will be much appreciated.</p>
<p>The overall agenda of the paper is introduce the conceptual notion of a self-reproducing decision pattern which includes reproduction of the goal or utility function, by exposing a particular possible problem with a tiling logical decision pattern and coming up with some partial technical solutions. Â This then makes it conceptually much clearer to point out the even deeper problems with \"We can't yet describe a probabilistic way to do this because of non-monotonicity\" and \"We don't have a good bounded way to do this because maximization is impossible, satisficing is too weak and Schmidhuber's swapping criterion is underspecified.\" Â The paper uses first-order logic (FOL) because FOL has a lot of useful standard machinery for reflection which we can then invoke; in real life, FOL is of course a poor representational fit to most real-world environments outside a human-constructed computer chip with thermodynamically expensive crisp variable states.</p>
<p>As further background, the idea that something-like-proof might be relevant to Friendly AI is not about achieving some chimera of absolute safety-feeling, but rather about the idea that the total probability of catastrophic failure should not have a significant conditionally independent component on each self-modification, and that self-modification will (at least in initial stages) take place within the highly deterministic environment of a computer chip. Â This means that statistical testing methods (e.g. an evolutionary algorithm's evaluation of average fitness on a set of test problems) are not suitable for self-modifications which can potentially induce catastrophic failure (e.g. of parts of code that can affect the representation or interpretation of the goals). Â Mathematical proofs have the property that they are as strong as their axioms and have no significant conditionally independent per-step failure probability if their axioms are semantically true, which suggests that something like mathematical reasoning may be appropriate for certain particular types of self-modification during some developmental stages.</p>
<p>Thus the content of the paper is very far off from how a realistic AI would work, but conversely, if you can't even answer the kinds of simple problems posed within the paper (both those we partially solve and those we only pose) then you must be very far off from being able to build a stable self-modifying AI. Â Being able to say how to build a theoretical device that would play perfect chess given infinite computing power, is very far off from the ability to build Deep Blue. Â However, if you can't even say how to play perfect chess given infinite computing power, you are confused about the rules of the chess or the structure of chess-playing computation in a way that would make it entirelyÂ hopeless for you to figure out how to build a bounded chess-player. Â Thus \"In real life we're always bounded\" is no excuse for not being able to solve the much simpler unbounded form of the problem, and being able to describe the infinite chess-player would be substantial and useful conceptual progress compared to <em>not </em>being able to do that. Â We can't be absolutely certain that an analogous situation holds between solving the challenges posed in the paper, and realistic self-modifying AIs with stable goal systems, but every line of investigation has to start somewhere.</p>
<p>Parts of the paper will be easier to understand if you've read <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\">Highly Advanced Epistemology 101 For Beginners</a> including the parts on correspondence theories of truth (relevant to section 6) and model-theoretic semantics of logic (relevant to 3, 4, and 6), and there are footnotes intended to make the paper somewhat more accessible than usual, but the paper is still essentially aimed at mathematically sophisticated readers.</p></div>
<a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/#comments\">250 comments</a>
" nil nil "a1a5bfddc1228b89bb142f56c4015d13") (40 (20949 25625 807719) "http://lesswrong.com/lw/hlk/rationality_quotes_june_2013/" "Rationality Quotes June 2013" nil "Mon, 03 Jun 2013 13:08:50 +1000" "Submitted by <a href=\"http://lesswrong.com/user/Thomas\">Thomas</a>
#
3 votes
#
<a href=\"http://lesswrong.com/lw/hlk/rationality_quotes_june_2013/#comments\">751 comments</a>
<div><p>Another month has passed and here is a new rationality quotes thread. The usual rules are:</p>
<ul>
<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>
<li>Do not quote yourself.</li>
<li>Do not quote from Less Wrong itself, Overcoming Bias, or HPMoR.</li>
<li>No more than 5 quotes per person per monthly thread, please.</li>
</ul></div>
<a href=\"http://lesswrong.com/lw/hlk/rationality_quotes_june_2013/#comments\">751 comments</a>
" nil nil "f11b90678015b47dd452d707990effc5") (39 (20949 25625 806653) "http://lesswrong.com/lw/hjn/earning_to_give_vs_altruistic_career_choice/" "Earning to Give vs. Altruistic Career Choice Revisited" nil "Sun, 02 Jun 2013 12:55:23 +1000" "Submitted by <a href=\"http://lesswrong.com/user/JonahSinick\">JonahSinick</a>
#
31 votes
#
<a href=\"http://lesswrong.com/lw/hjn/earning_to_give_vs_altruistic_career_choice/#comments\">134 comments</a>
<div><p class=\"MsoNormal\">A commonly voiced sentiment in the effective altruist community is that the best way to do the most good is generally to make as much money as possible, with a view toward donating to the most cost-effective charities. This is often referred to as âearning to give.â In the article <a href=\"http://qz.com/57254/to-save-the-world-dont-get-a-job-at-a-charity-go-work-on-wall-street/\">To save the world, donât get a job at a charity; go work on Wall Street</a> William MacAskill wrote:</p>
<p style=\"padding-left: 30px;\" class=\"MsoNormal\"><em style=\"mso-bidi-font-style:normal\">Top undergraduates who want to âmake a differenceâ are encouraged to forgo the allure of Wall Street and work in the charity sector ...Â </em><em style=\"mso-bidi-font-style:normal\">while researching ethical career choice, I concluded that itâs in fact better to earn a lot of money and donate a good chunk of it to the most cost-effective charities, a path that I call âearning to give.â </em><span style=\"mso-bidi-font-style:normal\">...Â </span><em style=\"mso-bidi-font-style:normal\">In general, the charitable sector is people-rich but money-poor. Adding another person to the labor pool just isnât as valuable as providing more money, so that more workers can be hired.</em></p>
<p class=\"MsoNormal\">In private correspondence, MacAskill clarified that he wasnât arguing that âearning to giveâ is the <em style=\"mso-bidi-font-style: normal\">best</em> way to do good, only that itâs often better than working at a given nonprofit. <span style=\"mso-spacerun:yes\">Â </span>In <a href=\"https://www.facebook.com/jefftk/posts/613456690752?comment_id=713258\">a recent comment</a> MacAskill wrote</p>
<p style=\"padding-left: 30px;\" class=\"MsoNormal\"><em style=\"mso-bidi-font-style:normal\">I think there's too much emphasis on âearning to giveâ as the *best* option rather than as the *baseline* option</em>Â </p>
<p class=\"MsoNormal\">and raises a number of counter-considerations againstÂ âearning to give.<em style=\"mso-bidi-font-style:normal\">â</em>Â Despite this, the idea that âearning to giveâ is optimal has caught on in the effective altruist community, and so itâs important to discuss it.</p>
<p class=\"MsoNormal\">Over the past three years, I myself have shifted from the position thatÂ <em style=\"mso-bidi-font-style:normal\">â</em>earning to give<em style=\"mso-bidi-font-style:normal\">â</em>Â is philanthropically optimal, to the position that <strong style=\"mso-bidi-font-weight:normal\">itâs generally the case that one can do more good by choosing a career with high direct social value than by choosing a lucrative career with a view toward donating as much as possible</strong>.Â </p>
<p class=\"MsoNormal\">In this post Iâll outline some arguments in favor of this view.<a id=\"more\"></a></p>
<p class=\"MsoNormal\"><strong style=\"mso-bidi-font-weight:normal\"><span style=\"font-size:14.0pt;mso-bidi-font-size:12.0pt\">Responses to MacAskillâs Considerations</span></strong></p>
<p class=\"MsoNormal\">In the article <a href=\"http://qz.com/57254/to-save-the-world-dont-get-a-job-at-a-charity-go-work-on-wall-street/\">To save the world, donât get a job at a charity; go work on Wall Street</a>, MacAskill gives three considerations in favor of âearning to give.â I respond to these considerations below. What I write should be read as a response to the article, rather than to MacAskillâs views.Â </p>
<p class=\"MsoNormal\"><strong style=\"mso-bidi-font-weight:normal\">Variance in cost-effectiveness of charities</strong></p>
<p class=\"MsoNormal\">MacAskill wrote</p>
<p style=\"padding-left: 30px;\" class=\"MsoNormal\"><em style=\"mso-bidi-font-style:normal\">â¦ charities vary tremendously in the amount of good they do with the money they receive. For example, it costs about $40,000 to train and provide a guide dog for one person, but it costs less than $25 to cure one person of sight-destroying trachoma. For the cost of improving the life of one person with blindness, you can cure 1,000 people of itâ¦itâs unlikely that you can work for only the very best charities. In contrast, if you earn to give, you can donate anywhere, preferably to the most cost-effective charities, and change your donations as often as you like.</em></p>
<p class=\"MsoNormal\">GiveWell has spent about five years looking for the best giving opportunities in global health, and its current #1 ranked charity is <a href=\"http://www.givewell.org/international/top-charities/AMF\">Against Malaria Foundation</a> (AMF). GiveWell estimates that AMF <a href=\"http://www.givewell.org/international/top-charities/AMF#Costperlifesaved\">saves an infantâs life for ~ $2,300</a>, not counting other benefits. These other benefits not withstanding, AMFâs cost per <a href=\"http://www.givewell.org/international/technical/additional/DALY\">DALY saved</a> is much higher than the implied cost per DALY saved associated with the figure cited for curing sight-destroying trachoma.</p>
<p class=\"MsoNormal\">GiveWell may have missed giving opportunities in global health that are much more cost-effective than AMF is, but given the amount of time, energy and attention that GiveWell spent on its search, one should have a strong prior against the possibility that one can easily find a better giving opportunity in global health. So a plausible estimate of the cost-effectiveness of donating to the best charity that delivers direct global health interventions is much lower than the above quotation suggests.</p>
<p class=\"MsoNormal\">Furthermore, the phenomenon of the <a href=\"/lw/hif/robustness_of_costeffectiveness_estimates_and/91ia\">optimizerâs curse</a> suggests that all charities with robust case for fairly high cost-effectiveness are closer in cost-effectiveness to AMF than explicit cost-effectiveness calculations indicate. This narrows the variance in cost-effectiveness amongst charities.Â </p>
<p class=\"MsoNormal\">So the advantage of being able to choose a charity to support and change at any time is smaller than the above quotation suggests.</p>
<p class=\"MsoNormal\"><strong style=\"mso-bidi-font-weight:normal\">Discrepancy in earnings</strong></p>
<p class=\"MsoNormal\">MacAskill wrote:</p>
<p style=\"padding-left: 30px;\" class=\"MsoNormal\"><em style=\"mso-bidi-font-style:normal\">Annual salaries in banking or investment start at $80,000 and grow to over $500,000 if you do well. A lifetime salary of over $10 million is typical. Careers in nonprofits start at about $40,000, and donât typically exceed $100,000, even for executive directors </em><span style=\"mso-bidi-font-style:normal\"><em>... </em></span><em style=\"mso-bidi-font-style:normal\">By entering finance and donating 50% of your lifetime earnings, you could pay for two nonprofit workers in your placeâwhile still living on double what you would have if youâd chosen that route.</em>Â </p>
<p class=\"MsoNormal\">The assumption âif you do wellâ is a very strong one. Only about 1% of Americans make ~$500k/year. There are some people who have a strong comparative advantage in finance, for whom âearning to giveâ to give may be especially compelling. But people who are able to make ~$500k/year in finance who <strong style=\"mso-bidi-font-weight:normal\">donât</strong> have a large comparative advantage in finance have <strong style=\"mso-bidi-font-weight:normal\">very strong transferable skills</strong>. Such people are significantly more capable than the average non-profit worker, and can plausibly have a bigger impact than 2 or 3 such workers by working directly on something with high social value.Â </p>
<p class=\"MsoNormal\"><strong style=\"mso-bidi-font-weight: normal\">Replaceability</strong></p>
<p class=\"MsoNormal\">MacAskill wrote:</p>
<p style=\"padding-left: 30px;\" class=\"MsoNormal\"><em><span style=\"color: #404040; background-position: initial initial; background-repeat: initial initial;\">â¦âmaking a differenceâ requires<span>Â </span></span></em><em><a style=\"-webkit-tap-highlight-color: rgba(0, 0, 0, 0);\" href=\"http://80000hours.org/blog/18-just-what-is-making-a-difference-counterfactuals-and-career-choice\"><span style=\"color: #168dd9; background-position: initial initial; background-repeat: initial initial;\">doing something that wouldnât have happened</span></a><span><span style=\"color: #404040; background-position: initial initial; background-repeat: initial initial;\">Â </span><span style=\"color: #404040; background-position: initial initial; background-repeat: initial initial;\">anywayâ¦</span></span></em><em><span style=\"color: #404040; background-position: initial initial; background-repeat: initial initial;\">The competition for not-for-profit jobs is fierce, and if someone else takes the job instead of you, he or she likely wonât be much worse at it than you would have been. So the difference you make by taking the job is only the difference between the good you would do, and the good that the other person would have done.</span></em>Â </p>
<p class=\"MsoNormal\">I would guess that there are some highly cost-effective humanitarian interventions that are sufficiently easy to implement that the implementers are easily replaceable. I could easily imagine that this is the case for vaccination efforts.Â </p>
<p class=\"MsoNormal\">But funding opportunities for these interventions can be thought of as âlow hanging fruit.â <a href=\"http://blog.givewell.org/2013/05/02/broad-market-efficiency/\">Broad market efficiency</a> suggests that such interventions will be funded. And indeed, GiveWell has found that straightforward immunization efforts <a href=\"http://blog.givewell.org/2013/03/21/trying-and-failing-to-find-more-funding-gaps-for-delivering-proven-cost-effective-interventions/\">are already largely funded</a>, to the point that GiveWell has been unable to find giving opportunities for individual donors in this area.Â </p>
<p class=\"MsoNormal\">This suggests that at the margin, <strong style=\"mso-bidi-font-weight: normal\">very high value humanitarian efforts require highly skilled and highly motivated laborers</strong>.</p>
<p class=\"MsoNormal\">High skilled laborers are a relatively small subset of laborers, so there are fewer people available to do these sorts of jobs than other jobs. Doing a hard, non-routine job well requires high motivation. <span style=\"mso-spacerun:yes\">Â </span>The collection of people who are sufficiently highly motivated to do a hard job with high social value that doesnât pay well, and who could otherwise be making much more money, largely consists of people who are trying to have a significant positive social impact.Â </p>
<p class=\"MsoNormal\">So suppose that youâre a highly skilled laborer deciding whether to âearn to giveâ or take a job with high social value that requires high skills and motivation. If you donât take the job with high social value, your counterfactual replacement is likely be one of the following:<br style=\"mso-special-character:line-break\"> </p>
<p style=\"margin-left:38.7pt;mso-add-space: auto;text-indent:-.25in;mso-list:l0 level1 lfo1\" class=\"MsoListParagraphCxSpFirst\"><span style=\"mso-fareast-font-family:Cambria;mso-fareast-theme-font:minor-latin; mso-bidi-font-family:Cambria;mso-bidi-theme-font:minor-latin\"><span style=\"mso-list:Ignore\">1.<span style='font:7.0pt \"Times New Roman\"'>Â Â Â Â  </span></span></span>Substantially less capable than you on account of having low skills, or low altruistic motivation.</p>
<p style=\"margin-left:38.7pt;mso-add-space: auto;text-indent:-.25in;mso-list:l0 level1 lfo1\" class=\"MsoListParagraphCxSpMiddle\"><span style=\"mso-fareast-font-family:Cambria;mso-fareast-theme-font:minor-latin; mso-bidi-font-family:Cambria;mso-bidi-theme-font:minor-latin\"><span style=\"mso-list:Ignore\">2.<span style='font:7.0pt \"Times New Roman\"'>Â Â Â Â  </span></span></span>A highly skilled person with high motivation, <em style=\"mso-bidi-font-style:normal\">who would be doing something else with high social value if you had taken the job, and who canât do this because they have to do the job that you would have done</em>.</p>
<p style=\"margin-left:38.7pt;mso-add-space:auto; text-indent:-.25in;mso-list:l0 level1 lfo1\" class=\"MsoListParagraphCxSpLast\"><span style=\"mso-fareast-font-family:Cambria;mso-fareast-theme-font:minor-latin; mso-bidi-font-family:Cambria;mso-bidi-theme-font:minor-latin\"><span style=\"mso-list:Ignore\">3.<span style='font:7.0pt \"Times New Roman\"'>Â Â Â Â  </span></span></span>Nonexistent.</p>
<p class=\"MsoNormal\">So the replaceability consideration carries less weight than it might seem.</p>
<p class=\"MsoNormal\">Admittedly thereâs a counterconsideration âÂ broad market efficiency cuts both ways, and one could imagine that the low hanging fruit in <em style=\"mso-bidi-font-style:normal\">working directly on projects with high social valu</em>e is also plucked, and this counter-consideration pushes in favor of âearning to give.â I have a fairly strong intuition that âif you donât fund it, somebody else willâ is more true than âif you donât do it, somebody else willâ so that this counter-consideration is outweighed. Itâs important to note that many projects of high social value are the first of their kind, and that finding somebody else to execute such a project is highly nontrivial. I think that itâs also relevant that <a href=\"http://givingpledge.org/\">114 billionaires</a> have signed the Giving Pledge, committing to giving 50+% of their wealth away in their lifetimes.<span style=\"mso-spacerun:yes\">Â </span></p>
<p class=\"MsoNormal\">In any case, there isnât a clear-cut, unconditional argument that favors âearning to giveâ: whether âearning to giveâ is the best option very much depends on nuanced empirical considerations rather than a general abstract argument.</p>
<p class=\"MsoNormal\"><strong style=\"mso-bidi-font-weight:normal\"><span style=\"font-size:14.0pt;mso-bidi-font-size:12.0pt\">Other important considerations that favor an altruistic career</span></strong></p>
<p class=\"MsoNormal\">There are additional important considerations that favor pursuing a career with high social value over âearning to giveâ:</p>
<p class=\"MsoNormal\"><strong style=\"mso-bidi-font-weight:normal\">Asymmetric implications of the existence of small probability failure modes</strong>Â </p>
<p class=\"MsoNormal\">In <a href=\"/lw/hif/robustness_of_costeffectiveness_estimates_and/\">Robustness of Cost-Effectiveness Estimates and Philanthropy</a>, I described how a large collection of small probability failure modes conspires to substantially reduce the expected value of a funding opportunity. The same issue applies to choosing a narrow career goal with a view toward directly having a high positive social impact. But <strong style=\"mso-bidi-font-weight:normal\">a worker has more capacity than a donor does to learn whether small probability failure modes prevail in practice, and can switch to a different job if he or she finds that such a failure mode prevails.</strong>Â </p>
<p class=\"MsoNormal\">Hereâs an example. Suppose that you go to medical school with a view toward the possibility of performing cleft palate surgeries in the developing world. Itâs probably the case that the opportunity isnât as promising as it seems. But if you try it, then youâll be able to see how effective the intervention is firsthand. If itâs highly effective, then you can keep doing it. If itâs not highly effective, then you can explore other possibilities, such asÂ </p>
<ul>
<li>Starting your own surgery organization.</li>
<li>Switching to doing a different kind of surgery in the developing world, such as cataract removal.</li>
<li>Working in a poor community in the developed world (which could have a bigger impact than working in the developing world owing to <a href=\"http://blog.givewell.org/2013/05/15/flow-through-effects/\">flow-through effects</a>).</li>
<li>Working for a biotech company.</li>
<li>Getting involved in clinical medical research.</li>
<li>Other things that haven't occurred to me.</li>
</ul>
<p class=\"MsoNormal\">By experimenting, one can hope to hone in on a job that has both high ostensible cost-effectiveness, and and a relatively small mass of small probability failure modes.</p>
<p class=\"MsoNormal\"><strong style=\"mso-bidi-font-weight:normal\">Altruistic careers extend beyond the nonprofit world</strong></p>
<p class=\"MsoNormal\">Even on the assumption that âearning to giveâ is better than working at a nonprofit, it doesnât follow that âearning to giveâ optimizes social impact. There are ways to have a positive social impact in the for-profit world, in scientific research, and in the government.</p>
<p class=\"MsoNormal\"><strong style=\"mso-bidi-font-weight:normal\">Historical Precedent</strong></p>
<p class=\"MsoNormal\">For the most part, the people who have had the biggest positive impact on the world havenât had their impact by âearning to give.â</p>
<p class=\"MsoNormal\">There are a few possible exceptions, such as Bill Gates and Warren Buffett, whose philanthropic activities could be having a huge impact (though itâs hard to tell from the outside) and could well outstrip the value that they contributed through their labor. But they appear to have an unusually high ratio of wealth to direct positive impact of their work, and so appear to be unrepresentative.</p>
<p class=\"MsoNormal\">Steve Jobsâ highest net worth was on the order of $10 billion, whereas Bill Gatesâ highest net worth was on the order of $100 billion. I donât think that Bill Gates contributed 10x as much as Steve Jobs to technology, and I donât think that Jobs could have had a bigger social impact by donating than through his work (which had massive positive <a href=\"http://blog.givewell.org/2013/05/15/flow-through-effects/\">flow-through effects</a>). I acknowledge that Jobs is a cherry picked example, but I think that the general principle still holds.Â </p>
<p class=\"MsoNormal\"><strong style=\"mso-bidi-font-weight:normal\">Mainstream consensus</strong></p>
<p class=\"MsoNormal\">Few people think that âearning to giveâ is the best way to make the world a better place. This could be attributable to irrationality or to low altruism, but my experience is that there are many people who care about global welfare, or just welfare within a specific cause, and many people who are highly intelligent. In light of the existence ofÂ <a href=\"https://en.wikipedia.org/wiki/Illusory_superiority\">illusory superiority</a>, one should be wary of holding an implicit view that one knows more about how to make the world a better place than the vast majority of the population.</p>
<p class=\"MsoNormal\"><strong style=\"mso-bidi-font-weight:normal\"><span style=\"font-size:14.0pt;mso-bidi-font-size:12.0pt\">Steelmanning wealth maximization</span></strong></p>
<p class=\"MsoNormal\">Itâs worth highlighting some factors that <em style=\"mso-bidi-font-style:normal\">favor</em> choosing a career with a view toward maximizing wealth in some situations:</p>
<ul>
<li><strong style=\"mso-bidi-font-weight:normal\">Comparative advantage</strong> âÂ Some people are unusually good at making money relative to doing other things. Such people may do better to âearn to giveâ than to try to choose a job that has a direct positive impact (which theyâre relatively bad at).<br></li>
<li><strong style=\"mso-bidi-font-weight:normal\">The market mechanism</strong>Â âÂ In the for-profit world, maximizing wealth is often correlated with maximizing positive social impact, and so can be used as a proxy goal for maximizing positive social impact.<br><strong style=\"mso-bidi-font-weight:normal\"><br></strong></li>
<li><span style=\"mso-bidi-font-weight:normal\"><strong>Connections and personal growth</strong></span>Â â People with high earnings are generally more capable and more knowledgeable than people in other contexts, and tend to be well connected, so positioning oneself among such people can increase oneâs prospects of soaring to greater heights. Jeff Bezos <a href=\"http://en.wikipedia.org/wiki/Jeff_Bezos#Business_career\">started his career</a> in finance, and later created Amazon, which has had massive positive social impact (both direct, and via <a href=\"http://blog.givewell.org/2013/05/15/flow-through-effects/\">flow-through effects</a>).<br><strong style=\"mso-bidi-font-weight:normal\"><br></strong></li>
<li><span style=\"mso-bidi-font-weight:normal\"><strong>Unusual values</strong></span> âÂ If one cares about causes that very few people care about, then it could be difficult to find funding for work on them, so âearning to giveâ could be necessary. I donât believe this to be the case, but itâs a consideration that's been raised by others, and so is worth mentioning.</li>
</ul>
<p class=\"MsoNormal\"><strong style=\"mso-bidi-font-weight:normal\"><span style=\"font-size:14.0pt;mso-bidi-font-size:12.0pt\">Closing summary</span></strong><strong style=\"mso-bidi-font-weight:normal\"><span style=\"font-size:14.0pt;mso-bidi-font-size:12.0pt\">Â </span></strong></p>
<p class=\"MsoNormal\"> </p>
<p class=\"MsoNormal\"><span style=\"font-size:14.0pt;mso-bidi-font-size:12.0pt\">Â­</span>There are many arguments against the claim that âearning to giveâ is generally the best way to maximize oneâs positive social impact, and I believe that choosing a job where one can do as much good as possible through oneâs work is generally the best way to maximize oneâs positive social impact. However, for some people in unusual situations, âearning to giveâ may be the best way to have a positive social impact.</p>
<p class=\"MsoNormal\"><strong>Note:<em>Â </em></strong>I formerly worked as a research analyst atÂ <a href=\"http://www.givewell.org/\">GiveWell</a>. All views expressed here are my own.</p>
<p class=\"MsoNormal\"><strong>Acknowledgements:</strong><em style=\"font-weight: bold;\">Â </em>I thank Nick Beckstead, ModusPonies and Will Crouch for helpful feedback on an earlier version of this article.</p>
<div style=\"mso-element:comment-list\">
<div style=\"mso-element:comment\">
<div id=\"_com_4\" class=\"msocomtxt\"></div>
</div>
</div></div>
<a href=\"http://lesswrong.com/lw/hjn/earning_to_give_vs_altruistic_career_choice/#comments\">134 comments</a>
" nil nil "c8373e61a4566f5c59d88673ce4de29c") (38 (20949 25625 803744) "http://lesswrong.com/lw/hhy/reductionism_sequence_now_available_in_audio/" "Reductionism sequence now available in audio format" nil "Sun, 02 Jun 2013 12:55:00 +1000" "Submitted by <a href=\"http://lesswrong.com/user/Rick_from_Castify\">Rick_from_Castify</a>
#
18 votes
#
<a href=\"http://lesswrong.com/lw/hhy/reductionism_sequence_now_available_in_audio/#comments\">6 comments</a>
<div><p>The sequence \"<a href=\"http://wiki.lesswrong.com/wiki/Reductionism_(sequence)\">Reductionism</a>\", which includes the subsequences \"<a href=\"http://wiki.lesswrong.com/wiki/Joy_in_the_Merely_Real\">Joy in the Merely Real</a>\" and \"<a href=\"http://wiki.lesswrong.com/wiki/Zombies_(sequence)\">Zombies</a>\", is now available as a <a href=\"http://castify.co/channels/43-reductionism\">professionally read podcast</a>.</p>
<p>Thanks to those who've been listening, let us know how your experience has been thus far and what you think of the service by dropping an email to support@castify.co. Â </p></div>
<a href=\"http://lesswrong.com/lw/hhy/reductionism_sequence_now_available_in_audio/#comments\">6 comments</a>
" nil nil "c07d09d60c2c83aa1ee957cddf3e4f4c") (37 (20949 25625 803107) "http://lesswrong.com/lw/hiv/the_centre_for_applied_rationality_a_year_later/" "The Centre for Applied Rationality: a year later from a (somewhat) outside perspective" nil "Tue, 28 May 2013 04:31:41 +1000" "Submitted by <a href=\"http://lesswrong.com/user/Swimmer963\">Swimmer963</a>
#
39 votes
#
<a href=\"http://lesswrong.com/lw/hiv/the_centre_for_applied_rationality_a_year_later/#comments\">102 comments</a>
<div><p>I recently had the privilege of being a CFAR alumni volunteering at a later workshop, which is a fascinating thing to do, and put me in a position both to evaluate how much of a difference the first workshop actually made in my life, and to see how the workshops themselves have evolved.Â </p>
<p>Exactly a year ago, I attended one of the first <a href=\"/lw/b98/minicamps_on_rationality_and_awesomeness_may_1113/\">workshops</a>, back when they were still inexplicably called âminicampsâ. I wasn't sure what to expect, and I especially wasn't sure why I had been accepted. But I bravely bullied the nursing faculty staff until they reluctantly let me switch a day of clinical around, and laterÂ stumbled off my plane into the San Francisco airport in a haze of exhaustion. The workshop spat me out three days later, twice as exhausted, with teetering piles of ideas and very little time or energy to apply them. I left with a list of annual goals, which I had never bothered to have before, and a feeling that more was possibleâthis included the feeling that more would have been possible if the workshop had been longer and less chaotic, if I had slept more the week before, if I hadn't had to rush out on Sunday evening to catch a plane and miss the social.Â </p>
<p>Like I frequently do on Less Wrong the website, I left the minicamp feeling a bit like an outsider, but also a bit like I had come home. As well as my written goals, I made an unwrittenÂ pre-commitmentÂ to come back to San Francisco later, for longer, and see whether I could make the \"more is possible\" in my head more specific. Of my thirteen written goals on my list, I fully accomplished only four and partially accomplished five, but I did make it back to San Francisco, at the opportunity cost of four weeks of sacrificed hospital shifts.Â </p>
<p>A week or so into my stay, while I shifted around between different rationalist shared houses and attempted to max out interesting-conversations-for-day, I found out that CFAR was holding another May workshop. I offered to volunteer, proved my sincerity by spending 6 hours printing and sticking nametags, and lived on site for another 4-day weekend of delightful information overload and limited sleep.Â </p>
<p>Before the May 2012 workshop, I had a low prior that any four-day workshop could be life-changing in a major way. A four-year nursing degree, okayâI've successfully retrained my social skills and my <a href=\"/lw/4fo/ability_to_react/\">ability to react under pressure</a> by putting myself in particular situations over and over and over and over again. Four days? Nah. Brains don't work that way.Â </p>
<p>In my experience, it's exceedingly hard for the human brain to doÂ <em>anythingÂ </em>deliberately. In Kahneman-speak, habits are System 1, effortless and automatic. Doing things on purpose involves System 2, effortful and a bit aversive. I could have had aÂ <em>muchÂ </em>better experience in my <a href=\"/lw/gnv/learning_critical_thinking_a_personal_example/\">final intensive care clinical</a> if I'd though to open up my workshop notes and tried to address the causes of aversions, or use offline time to train habits, or, y'know, doÂ <em>anythingÂ </em>on purpose instead of floundering around trying things at random until they worked.Â </p>
<p>(The again, I didn't apply concepts like System 1 and System 2 to myself a year ago. I read 'Thinking Fast and Slow' by Kahneman and 'Rationality and the Reflective Mind' by Stanovich as part of my minicamp goal 'read 12 hard nonfiction books this year', most of which came from the <a href=\"http://rationality.org/recommended-reading-on-rationality/\">CFAR recommended reading list</a>. If my preceptor had had any idea what I was saying when I explained to her that she was running particular nursing skills on System 1, because they were engrained on the level of habit, and I was running the same tasks on System 2 in working memory because they were new and confusing to me, and that was why I appeared to have poor time management, because System 2 takes forever to do anything, this terminology might have helped. Oh, for the world where everyone knows all jargon!)</p>
<p>...And here I am, setting aside a month of my life to think only about rationality. I can't imagine that my counterfactual self-who-didn't-attend-in-May-2012 would be here. I can't imagine that being here now will have <em>zeroÂ </em>effect on what I'm doing in a year, or ten years. Bingo. I did one thing deliberately!</p>
<p><strong>So what was the May 2013 workshop actually like?</strong></p>
<p>The curriculum has shifted around a lot in the past year, and I think with 95% probability that it's now more concretely useful. (Speaking of probabilities, the prediction markets during the workshop seemed to flow better and be more fun and interesting this time, although this may just show that I was more averse to games in general and betting in particular. In that case, yay for partly-cured aversions!)</p>
<p>The classes are grouped in an order that allows them to build on each other usefully, and they've been honed by practice into forms that successfully teach skills, instead of just putting words in the air and on flipcharts. For example, having a personal productivity system like GTD came across as a culturally prestigious thing at the last workshop, but there wasn't a lot of useful curriculum on it. Of course, I left on this trip wanting to spend my offline month creating with a GTD system better than paper to-do lists taped to walls, so I have both motivation and a low threshold for improvement.Â </p>
<p>There are also some completely new classes, including \"Againstness training\" by <a href=\"/user/Valentine/overview/\">Valentine</a>, which seem to relate to some of the 'reacting under pressure' stuff in interesting ways, and gave me vocabulary and techniques for something I've been doing inefficiently by trial and error for a good part of my life.</p>
<p>In general, there are more classes about emotions, both how to deal with them when they're in the way and how to use them when they're the best tool available. Given that none of us are Spock, I think this is useful.Â </p>
<p>Rejection therapy has morphed into a less terrifying and more helpful form with the awesome name of CoZE (Comfort Zone Expansion). I didn't personally find the original rejection therapy all that awful, but some people did, and that problem is largely solved.Â </p>
<p>The workshops are vastly more orderly and organized. (I like to think I contributed to this slightly with my volunteer skills of keeping the fridge stocked with water bottles and calling restaurants to confirm orders and make sure food arrived on time.) Classes began and ended on time. The venue stayed tidy. The food was excellent. It was easier to get enough sleep. Etc. The May 2012 venue had a pool, and this one didn't, which made exercise harder for addicts like me. CFAR staff are talking about solving this.Â </p>
<p>The workshops still aren't an easy environment for introverts. The negative parts of my experience in May 2012 were mostly because of this. It was easier this time, because as a volunteer I could skip classes if I started to feel socially overloaded, but periods of quiet alone time had to be effortfully carved out of the day, and at an opportunity cost of missing interesting conversations. I'm not sure if this problem is solvable without either making the workshops longer, in order to space the material out, and thus less accessible for people with jobs, or by cutting out curriculum. Either would impose a cost on the extroverts who don't want an hour at lunch to meditate or go running alone or read a sci-fi book, etc.Â </p>
<p>In general, I found the May 2012 workshop too short and intenseâwe had material thrown at us at a rate far exceeding the usual human idea-digestion rate. Keeping in touch via Skype chats with other participants helped. CFAR now does official followups with participants for six weeks following the workshop.Â </p>
<p>Meeting the other participants was, as usual, the best part of the weekend. The group was quite diverse, although I was still the only health care professional there. (Whyyy???? The health care system needs more rationality <em>so </em>badly!)Â The conversations were engaging. Many of the participants seem eager to stay in touch. The May 2012 workshop has a total of six people still on the Skype chats list, which is a 75% attrition rate. CFAR is now working on strategies to help people who want to stay in touch do it successfully.Â </p>
<p><strong>Conclusions?</strong></p>
<p>I thought the May 2012 workshop was awesome. I thought the May 2013 workshop was about an order of magnitude more awesome. I would say that now is a great time to attend a CFAR workshop...except that the organization is financially stable and likely to still be around in a year and producing even <em>better </em>workshops. So I'm not sure. Then again, rationality skills have compound interestâthe value of learning some new skills now, even if they amount more to vocab words and mental labels than superpowers, compounds over the year that you spend seeing all the books you read and all the opportunities you have in that framework. I'm glad I went a year ago instead of this May. I'm even more glad I had the opportunity to see the new classes and meet the new participants a year later.Â </p>
<p><strong><br></strong></p></div>
<a href=\"http://lesswrong.com/lw/hiv/the_centre_for_applied_rationality_a_year_later/#comments\">102 comments</a>
" nil nil "fa715dbeb8077ecd4bafa9593bbd3981") (36 (20949 25625 750807) "http://lesswrong.com/lw/hif/robustness_of_costeffectiveness_estimates_and/" "Robustness of Cost-Effectiveness Estimates and Philanthropy" nil "Sat, 25 May 2013 06:28:43 +1000" "Submitted by <a href=\"http://lesswrong.com/user/JonahSinick\">JonahSinick</a>
#
36 votes
#
<a href=\"http://lesswrong.com/lw/hif/robustness_of_costeffectiveness_estimates_and/#comments\">37 comments</a>
<div><p> </p>
<p class=\"MsoNormal\"><strong>Note:<em>Â </em></strong>I formerly worked as a research analyst at <a href=\"http://www.givewell.org/\">GiveWell</a>. This post describes the evolution of my thinking about robustness of cost-effectiveness estimates in philanthropy. All views expressed here are my own.</p>
<p class=\"MsoNormal\">Up until 2012, I believed that detailed explicit cost-effectiveness estimates are very important in the context of philanthropy. My position was reflected in a <a href=\"http://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/comment-page-1/#comment-230938\">comment that I made</a> in 2011:Â </p>
<p style=\"margin-left:.5in\" class=\"MsoNormal\">The problem with using unquantified heuristics and intuitions is that the âtrueâ expected values of philanthropic efforts plausibly differ by many orders of magnitude, and unquantified heuristics and intuitions are frequently insensitive to this. The last order of magnitude is the only one that matters; all others are negligible by comparison. So if at all possible, one should do oneâs best to pin down the philanthropic efforts with the âtrueâ expected value per dollar of the highest (positive) order of magnitude. It seems to me as though any feasible strategy for attacking this problem involves explicit computation.</p>
<p class=\"MsoNormal\">During my time at GiveWell, my position on this matter shifted. I still believe that there are instances in which <em>rough</em> cost-effectiveness estimates can be useful for determining good philanthropic foci. But Iâve shifted toward the position that <strong>effective altruists should spend much more time on qualitative analysis than on quantitative analysis in determining how they can maximize their positive social impact</strong>.</p>
<p class=\"MsoNormal\">In this post Iâll focus on one reason for my shift: <strong>explicit cost-effectiveness estimates are generally much less robust than I had previously thought</strong>.</p>
<p class=\"MsoNormal\"><a id=\"more\"></a></p>
<p class=\"MsoNormal\"><strong><span style=\"font-size:14.0pt;mso-bidi-font-size:12.0pt\">The history of GiveWellâs estimates for lives saved per dollar</span></strong></p>
<p class=\"MsoNormal\">Historically, GiveWell used âcost per life savedâ as a measure of the cost-effectiveness of its global health recommendations. Examination of the trajectory of GiveWellâs cost-effectiveness estimates shows that <strong>GiveWell has consistently updated in the direction of its ranked charities having higher âcost per life savedâ than GiveWell had previously thought. </strong>I give the details below.</p>
<p class=\"MsoNormal\">The discussion should be read with the understanding that <strong>donating to GiveWellâs top charities has benefits that extend beyond saving lives</strong>, so that ânumber of lives savedâ understates cost-effectiveness..</p>
<p class=\"MsoNormal\">At the end of each of 2009 and 2010, GiveWell named <a href=\"http://www.givewell.org/international/charities/villagereach\">VillageReach</a> its #1 ranked charity. VillageReach <a href=\"http://www.givewell.org/international/top-charities/villagereach/December-2009-review#Pastcosteffectivenesspilotprogram\">estimated</a> the cost-per-life-saved of its pilot project as being < $200, and at the end of 2009, GiveWell gave a âconservativeâ estimate of $545/life saved. In 2011, GiveWell <a href=\"http://blog.givewell.org/2012/07/26/rethinking-villagereachs-pilot-project/\">reassessed VillageReachâs pilot project</a>, commending VillageReach for being transparent enough for reassessment to be possible, and concluding that</p>
<p style=\"margin-left:.5in\" class=\"MsoNormal\">We feel that within the framework of âdelivering proven, cost-effective interventions to improve health,â AMF and SCI are solidly better giving opportunities than VillageReach (both now and at the time when we recommended it). Given the information we have, we see less room for doubt in the cases for AMFâs and SCIâs impact than in the case for VillageReachâs.</p>
<p class=\"MsoNormal\">Here âAMFâ refers to <a href=\"http://www.givewell.org/international/top-charities/AMF\">Against Malaria Foundation</a>, which is GiveWellâs current #1 ranked charity. If AMF is currently more cost-effective than VillageReach was at the time when GiveWell recommended VillageReach, then the best cost-per-life-saved figure for GiveWellâs recommended charities is (and was) the cost-effectiveness of donating to AMF.Â </p>
<p class=\"MsoNormal\">AMF delivers long-lasting insecticide treated nets (LLINs) to the developing world to protect people against mosquitoes that spread malaria. This contrasts with VillageReach, which works to increase vaccination rates. Vaccines are thought to be more cost-effective than LLINs, and <a href=\"http://blog.givewell.org/2013/03/21/trying-and-failing-to-find-more-funding-gaps-for-delivering-proven-cost-effective-interventions/\">GiveWell has not been able to find strong giving opportunities in vaccination</a>, so the cost per life saved of the best opportunity that GiveWell has found for individual donors is correspondingly higher.Â </p>
<p class=\"MsoNormal\">At the end of 2011, GiveWell estimated that the marginal cost per life associated with donating to AMF at <a href=\"http://www.givewell.org/international/top-charities/AMF/2011-review#Costperlifesaved\">$1600/life saved</a>. During 2012, I vetted GiveWellâs page on LLINs and <a href=\"http://blog.givewell.org/2012/10/18/revisiting-the-case-for-insecticide-treated-nets-itns/\">uncovered an issue</a>, which led GiveWell to revise its estimate for AMFâs marginal cost per life saved to <a href=\"http://www.givewell.org/international/top-charities/AMF#Costperlifesaved\">$2300/life saved</a> at the end of 2012. This does not take into account <a href=\"http://www.givingwhatwecan.org/where-to-give/methodology/regression-to-the-mean\">regression to the mean</a>, which can be expected to raise the cost per life saved.Â </p>
<p class=\"MsoNormal\">The discussion above shows a consistent trend in the direction of the marginal cost per life saved in the developing world being higher than initially meets the eye. Note that the difference between VillageReachâs original estimate and GiveWellâs current estimate is about an order of magnitude.Â </p>
<p class=\"MsoNormal\"><strong><span style=\"font-size:14.0pt;mso-bidi-font-size:12.0pt\">Concrete factors that further reduce the expected value of donating to AMF</span></strong></p>
<p class=\"MsoNormal\">A key point that I had missed when I thought about these things earlier in my life is that <strong>there are many small probability failure modes which are not significant individually, but which collectively substantially reduce cost-effectiveness</strong>. When I encountered such a potential failure mode, my reaction was to think âthis is very unlikely to be an issueâ and then to forget about it. I didnât notice that I was doing this many times in a row.</p>
<p class=\"MsoNormal\">I list many relevant factors that reduce AMFâs expected cost-effectiveness below. Some of these are from GiveWellâs discussion of <a href=\"http://www.givewell.org/international/top-charities/AMF#Possiblenegativeoroffsettingimpact\">possible negative or offsetting impacts</a> in GiveWellâs review of AMF. Others are implicitly present in GiveWellâs review of AMF and<a href=\"http://www.givewell.org/international/technical/programs/insecticide-treated-nets\"> GiveWellâs review of LLINs</a>, and others are issues that have emerged in the interim. I would emphasize that <strong>I donât think that any of the points listed is a big issue</strong> and that <strong>GiveWell and AMF take precautionary efforts to guard against them</strong>. But I think that they <em>collectively</em> reduce cost-effectiveness by a substantial amount.</p>
<ul>
<li>If GiveWellâs customers werenât funding AMF, another funder might, and that funder might instead be funding much less effective activities.</li>
<li>If AMF werenât working in a given region, there might be other organizations that would deliver LLINs to that region, and these other organizations may instead be funding much less effective activities.</li>
<li>It could be that the workers who distribute the LLINs would otherwise be providing more cost-effective health care interventions.</li>
<li>The five RCTs that found that LLIN distribution reduces mortality could be systematically flawed in a non-obvious way.</li>
<li>While the Cochrane Review that contains a meta-analysis of the RCTs referred to unpublished studies so as to counteractÂ <a href=\"http://blog.givewell.org/2009/01/25/publication-bias-over-reporting-good-news/\">publication bias</a>, there may be unpublished studies that were missed, and which were not published, because they found no effect.</li>
<li>The field workers who are assigned to distribute LLINs mayÂ <a href=\"http://www.givewell.org/international/top-charities/amf/updates/March-2012#DistributioninNtcheudistrictofMalawi\">steal the nets to sell them for a profit</a>.</li>
<li>FathersÂ <a href=\"http://ugandaradionetwork.com/a/story.php?s=18197\">may steal nets from pregnant mothers</a>Â and sell them for a profit.</li>
<li>LLIN recipients mayÂ <a href=\"http://www.malariajournal.com/content/7/1/165\">use the nets for fishing</a>.</li>
<li>LLIN users may not fasten LLINs properly.</li>
<li>Mosquitoes may developÂ <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/21856232\">biological resistance to the insecticide used on LLINs</a>.</li>
<li>MosquitoesÂ <a href=\"http://blog.givewell.org/2012/11/09/insecticide-resistance-and-malaria-control/\">may develop âbehavioral resistanceâ</a>Â to the insecticides used on LLINs by evolving to bite during the day (when LLINs are not used) rather than during the night.</li>
</ul>
<p class=\"MsoNormal\">Most of the relevant factors will vary by region where AMF ships nets, and some may be present in certain locations and not others.</p>
<p><strong><span style=\"font-size:14.0pt;mso-bidi-font-size:12.0pt\">Do these considerations argue against donating to AMF?</span></strong><strong><span style=\"font-size:14.0pt;mso-bidi-font-size:12.0pt\">Â </span></strong></p>
<p class=\"MsoNormal\">In view of the issues above, one might wonder whether itâs better to donate to a charity in a different cause, or better not to donate at all. Some relevant points follow:</p>
<p class=\"MsoNormal\"><strong style=\"text-indent: -0.25in;\">Donating to AMF has benefits beyond saving lives. </strong><span style=\"text-indent: -0.25in;\">The above discussion of cost-effectiveness figures concerns âcost per life savedâ specifically. But there are benefits to donating to AMF that go beyond saving lives.</span></p>
<ul>
<li><span style=\"text-indent: -0.25in;\">Malaria control reduces the morbidity of malaria. A Cochrane Review of theÂ </span><a style=\"text-indent: -0.25in;\" href=\"http://www.givewell.org/international/technical/programs/insecticide-treated-nets#Evidencefromsmallscalehighqualitystudies\">health benefits of LLINs</a><span style=\"text-indent: -0.25in;\">Â reports on reductions in anemia, enlarged spleen, and other health outcomes.<span style=\"text-indent: -0.25in;\"><br></span></span></li>
<li><span style=\"text-indent: -0.25in;\"><span style=\"text-indent: -0.25in;\">People are more productive when theyâre healthy than they are when theyâre ill.</span></span></li>
<li><span style=\"text-indent: -0.25in;\">There is some evidence that malaria controlÂ </span><a style=\"text-indent: -0.25in;\" href=\"http://www.givewell.org/international/technical/programs/insecticide-treated-nets#Possibledevelopmentaleffects\">increases childrenâs income later on in life</a><span style=\"text-indent: -0.25in;\">.</span></li>
<li><span style=\"text-indent: -0.25in;\">The above benefits could be massively leveraged viaÂ </span><a style=\"text-indent: -0.25in;\" href=\"http://blog.givewell.org/2013/05/15/flow-through-effects/\">flow-through effects</a><span style=\"text-indent: -0.25in;\">.</span></li>
</ul>
<p class=\"MsoNormal\"><strong style=\"text-indent: -0.25in;\">Updates in the direction of reduced cost-effectiveness arenât specific to global health.</strong><span style=\"text-indent: -0.25in;\"> Based on my experience at GiveWell, Iâve found that </span><em style=\"text-indent: -0.25in;\">regardless</em><span style=\"text-indent: -0.25in;\"> of the cause within which one investigates giving opportunities, thereâs a strong tendency for giving opportunities to appear progressively less promising as one learns more. AMF and LLIN distribution have stood up to scrutiny </span><em style=\"text-indent: -0.25in;\">unusually well</em><span style=\"text-indent: -0.25in;\">. It remains the case that </span><a style=\"text-indent: -0.25in;\" href=\"http://blog.givewell.org/2011/12/22/my-favorite-cause-for-individual-donors-global-health-and-nutrition/\">Global health and nutrition</a><span style=\"text-indent: -0.25in;\"> may be an unusually good cause for individual donors.</span></p>
<p class=\"MsoNormal\"><strong style=\"text-indent: -0.25in;\">Updates in the direction of reduced LLIN cost-effectiveness push in favor of cash transfers over LLINs. </strong><span style=\"text-indent: -0.25in;\">Transferring cash to people in the developing world is an unusually straightforward intervention. While there are </span><a style=\"text-indent: -0.25in;\" href=\"http://www.givewell.org/international/technical/programs/cash-transfers#Whatarethepotentialdownsidesoftheintervention\">potential downsides to transferring cash</a><span style=\"text-indent: -0.25in;\">, there seem to be fewer potential failure modes associated with it than there are potential failure modes associated with LLIN distribution. There are </span><a style=\"text-indent: -0.25in;\" href=\"http://blog.givewell.org/2012/05/30/giving-cash-versus-giving-bednets/\">strong arguments that favor LLINs over cash transfers</a><span style=\"text-indent: -0.25in;\">, but difference in straightforwardness of the interventions in juxtaposition with the phenomenon of surprisingly large updates in the direction of reduced cost-effectiveness is a countervailing consideration.</span></p>
<p class=\"MsoNormal\"><strong style=\"text-indent: -0.25in;\"><span style=\"font-size:14.0pt;mso-bidi-font-size:12.0pt\">Why do cost-effectiveness updates skew so negatively?</span></strong></p>
<p class=\"MsoNormal\">When I first started thinking seriously about philanthropy in 2009, I thought that if one has impressions of a philanthropic opportunity, one will be equally likely to update in the direction of it being better than meets the eye as one will be to update the direction of the opportunity being worse than meets the eye. So I was surprised to discover how strong the tendency is for philanthropic opportunities to look worse over time rather than better over time.</p>
<p class=\"MsoNormal\">Aside from the empirical data, something that shifted my view is Holdenâs observation that outlier cost-effectiveness estimates <a href=\"http://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/\">Â need to be regressed to oneâs Bayesian prior</a> over the values of all possible philanthropic opportunities. Another reason for my shift is GiveWell finding that <a href=\"http://blog.givewell.org/2013/05/02/broad-market-efficiency/\">philanthropic markets are more efficient than it had previously thought</a>. Â I think that <a href=\"http://en.wikipedia.org/wiki/Optimism_bias\">optimism bias</a> also plays a role.Â </p>
<p class=\"MsoNormal\">This is all consistent with GiveWellâs view that <a href=\"http://blog.givewell.org/2011/06/11/why-we-should-expect-good-giving-to-be-hard/\">one should expect good giving to be hard</a>.</p>
<p class=\"MsoNormal\"><strong><span style=\"font-size:14.0pt;mso-bidi-font-size:12.0pt\">Implications for maximizing cost-effectiveness</span></strong></p>
<p class=\"MsoNormal\">The remarks and observations above imply that <strong>Bayesian regression in the context of philanthropy is substantially larger than expected</strong>. Â This favors:</p>
<ul>
<li><span style=\"text-indent: -24px;\">Examining a philanthropic opportunityÂ </span><a style=\"text-indent: -24px;\" href=\"http://blog.givewell.org/2011/11/10/maximizing-cost-effectiveness-via-critical-inquiry/\">from many angles</a><span style=\"text-indent: -24px;\">Â rather than relying too heavily on a single perspective.<br></span></li>
<li><span style=\"text-indent: -24px;\"><span style=\"text-indent: -0.25in;\">Giving more weight to robust inputs into oneâs assessment of a philanthropic opportunity</span><span style=\"text-indent: -0.25in;\">. Estimating the cost-effectiveness of health interventions in the developing world has proved to be exceedingly difficult, and this pushes in favor of giving more weight to inputs for which itâs possible to make relatively well-grounded assessments. Some of these areÂ </span><a style=\"text-indent: -0.25in;\" href=\"http://www.givewell.org/international/technical/criteria/scalability\">room for more funding</a><span style=\"text-indent: -0.25in;\">,Â </span><a style=\"text-indent: -0.25in;\" href=\"http://blog.givewell.org/2012/10/25/evaluating-people/\">the quality of the people behind a project</a><span style=\"text-indent: -0.25in;\">Â andÂ </span><a style=\"text-indent: -0.25in;\" href=\"http://blog.givewell.org/2013/04/09/givewells-history-of-philanthropyphilanthropy-journalism-project/\">historical precedent</a><span style=\"text-indent: -0.25in;\">.</span></span><span style=\"text-indent: -0.25in;\"><br></span></li>
<li><span style=\"text-indent: -0.25in;\">Choosing giving opportunities that </span><a style=\"text-indent: -0.25in;\" href=\"http://blog.givewell.org/2012/12/20/more-on-the-ranking-of-our-top-charities/\">it will be possible to learn from</a><span style=\"text-indent: -0.25in;\" class=\"MsoHyperlink\">, </span><span style=\"text-indent: -0.25in;\">and </span><span style=\"text-indent: -0.25in;\" class=\"MsoHyperlink\">Â </span><a style=\"text-indent: -0.25in;\" href=\"http://blog.givewell.org/2011/12/20/give-now-or-give-later/\">giving now instead of giving later</a><span style=\"text-indent: -0.25in;\"> when one encounters such an opportunity.</span></li>
<li><span style=\"text-indent: -0.25in;\"><span style=\"text-indent: -0.25in;\">Choosing giving opportunities </span><a style=\"text-indent: -0.25in;\" href=\"http://blog.givewell.org/2011/05/27/in-defense-of-the-streetlight-effect/\">about which one has a lot of information</a><span style=\"text-indent: -0.25in;\">. GiveWell has been </span><a style=\"text-indent: -0.25in;\" href=\"http://blog.givewell.org/2013/03/14/update-on-givewells-plans-for-2013/\">moving away from</a><span style=\"text-indent: -0.25in;\"> the old criterion of recommending proven interventions, and giving more weight to </span><a style=\"text-indent: -0.25in;\" href=\"http://blog.givewell.org/2012/01/19/trading-off-upside-vs-track-record/\">upside relative to track record</a><span style=\"text-indent: -0.25in;\"> than GiveWell used to. However, this partially reflects the discovery that the expected effectiveness of ostensibly âprovenâ interventions is lower than previously thought.Â </span></span></li>
</ul></div>
<a href=\"http://lesswrong.com/lw/hif/robustness_of_costeffectiveness_estimates_and/#comments\">37 comments</a>
" nil nil "6b3369efc0be79e23bd93f76f0691a24") (35 (20949 24895 480702) "http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/" "Rationality Quotes July 2013" nil "Tue, 02 Jul 2013 16:21:59 +0000" "Submitted by <a href=\"http://lesswrong.com/user/Vaniver\">Vaniver</a>
#
3 votes
#
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">136 comments</a>
<div><div id=\"entry_t3_hlk\" class=\"content clear\">
<div class=\"md\">
<div>
<div>
<p>Another month has passed and here is a new rationality quotes thread. The usual rules are:</p>
<ul>
<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>
<li>Do not quote yourself.</li>
<li>Do not quote from Less Wrong itself, HPMoR, Eliezer Yudkowsky, or Robin Hanson.</li>
<li>No more than 5 quotes per person per monthly thread, please.</li>
</ul>
</div>
</div>
</div>
</div></div>
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">136 comments</a>
" nil nil "64d99138c1f617d7600dfa86e79fc8a9") (34 (20949 20197 802270) "http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/" "Rationality Quotes July 2013" nil "Tue, 02 Jul 2013 16:21:59 +0000" "Submitted by <a href=\"http://lesswrong.com/user/Vaniver\">Vaniver</a>
#
3 votes
#
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">135 comments</a>
<div><div id=\"entry_t3_hlk\" class=\"content clear\">
<div class=\"md\">
<div>
<div>
<p>Another month has passed and here is a new rationality quotes thread. The usual rules are:</p>
<ul>
<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>
<li>Do not quote yourself.</li>
<li>Do not quote from Less Wrong itself, HPMoR, Eliezer Yudkowsky, or Robin Hanson.</li>
<li>No more than 5 quotes per person per monthly thread, please.</li>
</ul>
</div>
</div>
</div>
</div></div>
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">135 comments</a>
" nil nil "68acd52bb2b6366afaa4f76cf286d114") (33 (20949 12144 650653) "http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/" "Rationality Quotes July 2013" nil "Tue, 02 Jul 2013 16:21:59 +0000" "Submitted by <a href=\"http://lesswrong.com/user/Vaniver\">Vaniver</a>
#
3 votes
#
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">133 comments</a>
<div><div id=\"entry_t3_hlk\" class=\"content clear\">
<div class=\"md\">
<div>
<div>
<p>Another month has passed and here is a new rationality quotes thread. The usual rules are:</p>
<ul>
<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>
<li>Do not quote yourself.</li>
<li>Do not quote from Less Wrong itself, HPMoR, Eliezer Yudkowsky, or Robin Hanson.</li>
<li>No more than 5 quotes per person per monthly thread, please.</li>
</ul>
</div>
</div>
</div>
</div></div>
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">133 comments</a>
" nil nil "973c27098e14b2a13defd47891afccd2") (32 (20949 12144 597770) "http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/" "Tiling Agents for Self-Modifying AI (OPFAI #2)" nil "Fri, 07 Jun 2013 06:24:25 +1000" "Submitted by <a href=\"http://lesswrong.com/user/Eliezer_Yudkowsky\">Eliezer_Yudkowsky</a>
#
51 votes
#
<a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/#comments\">250 comments</a>
<div><p>An early draft of publication #2 in the Open Problems in Friendly AI series is now available: Â <a href=\"http://intelligence.org/files/TilingAgents.pdf\">Tiling Agents for Self-Modifying AI, and the Lobian Obstacle</a>.Â  ~20,000 words, aimed at mathematicians or the highly mathematically literate.Â  The research reported on was conducted by Yudkowsky and Herreshoff, substantially refined at the November 2012 MIRI Workshop with Mihaly Barasz and Paul Christiano, and refined further at the April 2013 MIRI Workshop.</p>
<p style=\"padding-left: 60px;\"><strong>Abstract:</strong></p>
<p style=\"padding-left: 30px;\">We model self-modication in AI by introducing 'tiling' agents whose decision systems will approve the construction of highly similar agents, creating a repeating pattern (including similarity of the offspring's goals). Â Constructing a formalism in the most straightforward way produces a Godelian difficulty, the Lobian obstacle. Â By technical methods we demonstrate the possibility of avoiding this obstacle, but the underlying puzzles of rational coherence are thus only partially addressed. Â We extend the formalism to partially unknown deterministic environments, and show a very crude extension to probabilistic environments and expected utility; but the problem of finding a fundamental decision criterion for self-modifying probabilistic agents remains open.</p>
<p>Commenting here is the preferred venue for discussion of the paper. Â This is an early draft and has not been reviewed, so it may contain mathematical errors, and reporting of these will be much appreciated.</p>
<p>The overall agenda of the paper is introduce the conceptual notion of a self-reproducing decision pattern which includes reproduction of the goal or utility function, by exposing a particular possible problem with a tiling logical decision pattern and coming up with some partial technical solutions. Â This then makes it conceptually much clearer to point out the even deeper problems with \"We can't yet describe a probabilistic way to do this because of non-monotonicity\" and \"We don't have a good bounded way to do this because maximization is impossible, satisficing is too weak and Schmidhuber's swapping criterion is underspecified.\" Â The paper uses first-order logic (FOL) because FOL has a lot of useful standard machinery for reflection which we can then invoke; in real life, FOL is of course a poor representational fit to most real-world environments outside a human-constructed computer chip with thermodynamically expensive crisp variable states.</p>
<p>As further background, the idea that something-like-proof might be relevant to Friendly AI is not about achieving some chimera of absolute safety-feeling, but rather about the idea that the total probability of catastrophic failure should not have a significant conditionally independent component on each self-modification, and that self-modification will (at least in initial stages) take place within the highly deterministic environment of a computer chip. Â This means that statistical testing methods (e.g. an evolutionary algorithm's evaluation of average fitness on a set of test problems) are not suitable for self-modifications which can potentially induce catastrophic failure (e.g. of parts of code that can affect the representation or interpretation of the goals). Â Mathematical proofs have the property that they are as strong as their axioms and have no significant conditionally independent per-step failure probability if their axioms are semantically true, which suggests that something like mathematical reasoning may be appropriate for certain particular types of self-modification during some developmental stages.</p>
<p>Thus the content of the paper is very far off from how a realistic AI would work, but conversely, if you can't even answer the kinds of simple problems posed within the paper (both those we partially solve and those we only pose) then you must be very far off from being able to build a stable self-modifying AI. Â Being able to say how to build a theoretical device that would play perfect chess given infinite computing power, is very far off from the ability to build Deep Blue. Â However, if you can't even say how to play perfect chess given infinite computing power, you are confused about the rules of the chess or the structure of chess-playing computation in a way that would make it entirelyÂ hopeless for you to figure out how to build a bounded chess-player. Â Thus \"In real life we're always bounded\" is no excuse for not being able to solve the much simpler unbounded form of the problem, and being able to describe the infinite chess-player would be substantial and useful conceptual progress compared to <em>not </em>being able to do that. Â We can't be absolutely certain that an analogous situation holds between solving the challenges posed in the paper, and realistic self-modifying AIs with stable goal systems, but every line of investigation has to start somewhere.</p>
<p>Parts of the paper will be easier to understand if you've read <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\">Highly Advanced Epistemology 101 For Beginners</a> including the parts on correspondence theories of truth (relevant to section 6) and model-theoretic semantics of logic (relevant to 3, 4, and 6), and there are footnotes intended to make the paper somewhat more accessible than usual, but the paper is still essentially aimed at mathematically sophisticated readers.</p></div>
<a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/#comments\">250 comments</a>
" nil nil "64c4dfb1409f31803f5a9505909441a3") (31 (20948 23324 547678) "http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/" "Rationality Quotes July 2013" nil "Tue, 02 Jul 2013 16:21:59 +0000" "Submitted by <a href=\"http://lesswrong.com/user/Vaniver\">Vaniver</a>
#
3 votes
#
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">119 comments</a>
<div><div id=\"entry_t3_hlk\" class=\"content clear\">
<div class=\"md\">
<div>
<div>
<p>Another month has passed and here is a new rationality quotes thread. The usual rules are:</p>
<ul>
<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>
<li>Do not quote yourself.</li>
<li>Do not quote from Less Wrong itself, Overcoming Bias, HPMoR, Eliezer Yudkowsky, or Robin Hanson.</li>
<li>No more than 5 quotes per person per monthly thread, please.</li>
</ul>
</div>
</div>
</div>
</div></div>
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">119 comments</a>
" nil nil "17bcfcc4947c7e46e046c327053ef20a") (30 (20948 23035 383728) "http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/" "Rationality Quotes July 2013" nil "Tue, 02 Jul 2013 16:21:59 +0000" "Submitted by <a href=\"http://lesswrong.com/user/Vaniver\">Vaniver</a>
#
3 votes
#
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">118 comments</a>
<div><div id=\"entry_t3_hlk\" class=\"content clear\">
<div class=\"md\">
<div>
<div>
<p>Another month has passed and here is a new rationality quotes thread. The usual rules are:</p>
<ul>
<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>
<li>Do not quote yourself.</li>
<li>Do not quote from Less Wrong itself, Overcoming Bias, HPMoR, Eliezer Yudkowsky, or Robin Hanson.</li>
<li>No more than 5 quotes per person per monthly thread, please.</li>
</ul>
</div>
</div>
</div>
</div></div>
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">118 comments</a>
" nil nil "40303bdceb8574223db673c117aa6ef3") (29 (20948 23035 380694) "http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/" "Tiling Agents for Self-Modifying AI (OPFAI #2)" nil "Fri, 07 Jun 2013 06:24:25 +1000" "Submitted by <a href=\"http://lesswrong.com/user/Eliezer_Yudkowsky\">Eliezer_Yudkowsky</a>
#
51 votes
#
<a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/#comments\">243 comments</a>
<div><p>An early draft of publication #2 in the Open Problems in Friendly AI series is now available: Â <a href=\"http://intelligence.org/files/TilingAgents.pdf\">Tiling Agents for Self-Modifying AI, and the Lobian Obstacle</a>.Â  ~20,000 words, aimed at mathematicians or the highly mathematically literate.Â  The research reported on was conducted by Yudkowsky and Herreshoff, substantially refined at the November 2012 MIRI Workshop with Mihaly Barasz and Paul Christiano, and refined further at the April 2013 MIRI Workshop.</p>
<p style=\"padding-left: 60px;\"><strong>Abstract:</strong></p>
<p style=\"padding-left: 30px;\">We model self-modication in AI by introducing 'tiling' agents whose decision systems will approve the construction of highly similar agents, creating a repeating pattern (including similarity of the offspring's goals). Â Constructing a formalism in the most straightforward way produces a Godelian difficulty, the Lobian obstacle. Â By technical methods we demonstrate the possibility of avoiding this obstacle, but the underlying puzzles of rational coherence are thus only partially addressed. Â We extend the formalism to partially unknown deterministic environments, and show a very crude extension to probabilistic environments and expected utility; but the problem of finding a fundamental decision criterion for self-modifying probabilistic agents remains open.</p>
<p>Commenting here is the preferred venue for discussion of the paper. Â This is an early draft and has not been reviewed, so it may contain mathematical errors, and reporting of these will be much appreciated.</p>
<p>The overall agenda of the paper is introduce the conceptual notion of a self-reproducing decision pattern which includes reproduction of the goal or utility function, by exposing a particular possible problem with a tiling logical decision pattern and coming up with some partial technical solutions. Â This then makes it conceptually much clearer to point out the even deeper problems with \"We can't yet describe a probabilistic way to do this because of non-monotonicity\" and \"We don't have a good bounded way to do this because maximization is impossible, satisficing is too weak and Schmidhuber's swapping criterion is underspecified.\" Â The paper uses first-order logic (FOL) because FOL has a lot of useful standard machinery for reflection which we can then invoke; in real life, FOL is of course a poor representational fit to most real-world environments outside a human-constructed computer chip with thermodynamically expensive crisp variable states.</p>
<p>As further background, the idea that something-like-proof might be relevant to Friendly AI is not about achieving some chimera of absolute safety-feeling, but rather about the idea that the total probability of catastrophic failure should not have a significant conditionally independent component on each self-modification, and that self-modification will (at least in initial stages) take place within the highly deterministic environment of a computer chip. Â This means that statistical testing methods (e.g. an evolutionary algorithm's evaluation of average fitness on a set of test problems) are not suitable for self-modifications which can potentially induce catastrophic failure (e.g. of parts of code that can affect the representation or interpretation of the goals). Â Mathematical proofs have the property that they are as strong as their axioms and have no significant conditionally independent per-step failure probability if their axioms are semantically true, which suggests that something like mathematical reasoning may be appropriate for certain particular types of self-modification during some developmental stages.</p>
<p>Thus the content of the paper is very far off from how a realistic AI would work, but conversely, if you can't even answer the kinds of simple problems posed within the paper (both those we partially solve and those we only pose) then you must be very far off from being able to build a stable self-modifying AI. Â Being able to say how to build a theoretical device that would play perfect chess given infinite computing power, is very far off from the ability to build Deep Blue. Â However, if you can't even say how to play perfect chess given infinite computing power, you are confused about the rules of the chess or the structure of chess-playing computation in a way that would make it entirelyÂ hopeless for you to figure out how to build a bounded chess-player. Â Thus \"In real life we're always bounded\" is no excuse for not being able to solve the much simpler unbounded form of the problem, and being able to describe the infinite chess-player would be substantial and useful conceptual progress compared to <em>not </em>being able to do that. Â We can't be absolutely certain that an analogous situation holds between solving the challenges posed in the paper, and realistic self-modifying AIs with stable goal systems, but every line of investigation has to start somewhere.</p>
<p>Parts of the paper will be easier to understand if you've read <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\">Highly Advanced Epistemology 101 For Beginners</a> including the parts on correspondence theories of truth (relevant to section 6) and model-theoretic semantics of logic (relevant to 3, 4, and 6), and there are footnotes intended to make the paper somewhat more accessible than usual, but the paper is still essentially aimed at mathematically sophisticated readers.</p></div>
<a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/#comments\">243 comments</a>
" nil nil "12e7eed4e7b213518f61f54a74c6a33d") (28 (20948 18261 480338) "http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/" "Rationality Quotes July 2013" nil "Tue, 02 Jul 2013 16:21:59 +0000" "Submitted by <a href=\"http://lesswrong.com/user/Vaniver\">Vaniver</a>
#
3 votes
#
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">117 comments</a>
<div><div id=\"entry_t3_hlk\" class=\"content clear\">
<div class=\"md\">
<div>
<div>
<p>Another month has passed and here is a new rationality quotes thread. The usual rules are:</p>
<ul>
<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>
<li>Do not quote yourself.</li>
<li>Do not quote from Less Wrong itself, Overcoming Bias, HPMoR, Eliezer Yudkowsky, or Robin Hanson.</li>
<li>No more than 5 quotes per person per monthly thread, please.</li>
</ul>
</div>
</div>
</div>
</div></div>
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">117 comments</a>
" nil nil "0d574fdea187f169614647df52787af6") (27 (20948 18126 363366) "http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/" "Rationality Quotes July 2013" nil "Tue, 02 Jul 2013 16:21:59 +0000" "Submitted by <a href=\"http://lesswrong.com/user/Vaniver\">Vaniver</a>
#
3 votes
#
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">115 comments</a>
<div><div id=\"entry_t3_hlk\" class=\"content clear\">
<div class=\"md\">
<div>
<div>
<p>Another month has passed and here is a new rationality quotes thread. The usual rules are:</p>
<ul>
<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>
<li>Do not quote yourself.</li>
<li>Do not quote from Less Wrong itself, Overcoming Bias, HPMoR, Eliezer Yudkowsky, or Robin Hanson.</li>
<li>No more than 5 quotes per person per monthly thread, please.</li>
</ul>
</div>
</div>
</div>
</div></div>
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">115 comments</a>
" nil nil "20412a47e55d827243df1fde4c2d18ab") (26 (20948 18126 360670) "http://lesswrong.com/lw/hmx/prisoners_dilemma_with_visible_source_code/" "Prisoner's Dilemma (with visible source code) Tournament" nil "Fri, 07 Jun 2013 18:30:20 +1000" "Submitted by <a href=\"http://lesswrong.com/user/AlexMennen\">AlexMennen</a>
#
47 votes
#
<a href=\"http://lesswrong.com/lw/hmx/prisoners_dilemma_with_visible_source_code/#comments\">229 comments</a>
<div><p style=\"margin-bottom: 0in\">After the <a href=\"/lw/7f2/prisoners_dilemma_tournament_results/\">iterated prisoner's dilemma tournament</a>Â organized by prase two years ago, there was discussion of running tournaments for several variants, including one in which two players submit programs, each of which are given the source code of the other player's program, and outputs either âcooperateâ or âdefectâ. However, as far as I know, no such tournament has been run until now.</p>
<p style=\"margin-bottom: 0in\">Here's how it's going to work: Each player will submit a file containing a single Scheme lambda-function. The function should take one input. Your program will play exactly one round against each other program submitted (not including itself). In each round, two programs will be run, each given the source code of the other as input, and will be expected to return either of the symbols âCâ or âDâ (for \"cooperate\" and \"defect\", respectively). The programs will receive points based on the following payoff matrix:</p>
<p style=\"margin-bottom: 0in\"><img src=\"http://www.codecogs.com/png.latex?%5Cbegin%7Barray%7D%7Bcccc%7D%20&%20C%20&%20D%20&%20other%5C%5C%20C%20&%20(2,%5C,2)%20&%20(0,%5C,3)%20&%20(0,%5C,2)%5C%5C%20D%20&%20(3,%5C,0)%20&%20(1,%5C,1)%20&%20(1,%5C,0)%5C%5C%20other%20&%20(2,%5C,0)%20&%20(0,%5C,1)%20&%20(0,%5C,0)%20%5Cend%7Barray%7D\" alt=\"\" height=\"84\" width=\"215\"></p>
<p style=\"margin-bottom: 0in\">âOtherâ includes any result other than returning âCâ or âDâ, including failing to terminate, throwing an exception, and even returning the string âCooperateâ. Notice that âOtherâ results in a worst-of-both-worlds scenario where you get the same payoff as you would have if you cooperated, but the other player gets the same payoff as if you had defected. This is an attempt to ensure that no one ever has incentive for their program to fail to run properly, or to trick another program into doing so.</p>
<p style=\"margin-bottom: 0in\">Your score is the sum of the number of points you earn in each round. The player with the highest score wins the tournament. <strong>Edit: There is a <a href=\"/lw/hmx/prisoners_dilemma_with_visible_source_code/94no\">0.5 bitcoin prize</a> being offered for the winner. Thanks, VincentYu!</strong></p>
<p style=\"margin-bottom: 0in\">Details:<br>All submissions must be emailed to <a href=\"mailto:wardenPD@gmail.com\">wardenPD@gmail.com</a> by July 5, at noon PDT (Edit: that's 19:00 UTC). Your email should also say how you would like to be identified when I announce the tournament results.<br>Each program will be allowed to run for 10 seconds. If it has not returned either âCâ or âDâ by then, it will be stopped, and treated as returning âOtherâ. For consistency, I will have Scheme collect garbage right before each run.<br>One submission per person or team. No person may contribute to more than one entry. <strong>Edit: This also means no copying from each others' source code. Describing the behavior of your program to others is okay.</strong><br>I will be running the submissions in Racket. You may be interested in how Racket handles <a href=\"http://docs.racket-lang.org/reference/time.html\">time</a>Â (especially the (current-milliseconds) function), <a href=\"http://docs.racket-lang.org/reference/threads.html\">threads</a>Â (in particular, âthreadâ, âkill-threadâ, âsleepâ, and âthread-dead?â), and possibly <a href=\"http://docs.racket-lang.org/reference/generic-numbers.html#%28def._%28%28quote._~23~25kernel%29._random%29%29\">randomness</a>.<br>Don't try to open the file you wrote your program in (or any other file, for that matter). I'll add code to the file before running it, so if you want your program to use a copy of your source code, you will need to use a quine. <strong>Edit: No I/O of any sort.</strong><br>Unless you tell me otherwise, I assume I have permission to publish your code after the contest.<br>You are encouraged to discuss strategies for achieving mutual cooperation in the comments thread.<br>I'm hoping to get as many entries as possible. If you know someone who might be interested in this, please tell them.<br>It's possible that I've said something stupid that I'll have to change or clarify, so you might want to come back to this page again occasionally to look for changes to the rules. Any edits will be bolded, and I'll try not to change anything too drastically, or make any edits late in the contest.</p>
<p style=\"margin-bottom: 0in\">Here is an example of a correct entry, which cooperates with you if and only if you would cooperate with a program that always cooperates (actually, if and only if you would cooperate with one particular program that always cooperates):</p>
<blockquote>
<p style=\"margin-bottom: 0in;\">(lambda (x)<br>Â  Â  (if (eq? ((eval x) '(lambda (y) 'C)) 'C)<br>Â  Â  Â  Â  'C<br>Â  Â  Â  Â  'D))</p>
</blockquote></div>
<a href=\"http://lesswrong.com/lw/hmx/prisoners_dilemma_with_visible_source_code/#comments\">229 comments</a>
" nil nil "5db4dcfbeaf7bebfcaae5b9d242bb069") (25 (20948 13902 901832) "http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/" "Rationality Quotes July 2013" nil "Tue, 02 Jul 2013 16:21:59 +0000" "Submitted by <a href=\"http://lesswrong.com/user/Vaniver\">Vaniver</a>
#
3 votes
#
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">110 comments</a>
<div><div id=\"entry_t3_hlk\" class=\"content clear\">
<div class=\"md\">
<div>
<div>
<p>Another month has passed and here is a new rationality quotes thread. The usual rules are:</p>
<ul>
<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>
<li>Do not quote yourself.</li>
<li>Do not quote from Less Wrong itself, Overcoming Bias, HPMoR, Eliezer Yudkowsky, or Robin Hanson.</li>
<li>No more than 5 quotes per person per monthly thread, please.</li>
</ul>
</div>
</div>
</div>
</div></div>
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">110 comments</a>
" nil nil "4d3c4136913b1484d2fb55ecd1c174c6") (24 (20948 13902 898818) "http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/" "Tiling Agents for Self-Modifying AI (OPFAI #2)" nil "Fri, 07 Jun 2013 06:24:25 +1000" "Submitted by <a href=\"http://lesswrong.com/user/Eliezer_Yudkowsky\">Eliezer_Yudkowsky</a>
#
51 votes
#
<a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/#comments\">242 comments</a>
<div><p>An early draft of publication #2 in the Open Problems in Friendly AI series is now available: Â <a href=\"http://intelligence.org/files/TilingAgents.pdf\">Tiling Agents for Self-Modifying AI, and the Lobian Obstacle</a>.Â  ~20,000 words, aimed at mathematicians or the highly mathematically literate.Â  The research reported on was conducted by Yudkowsky and Herreshoff, substantially refined at the November 2012 MIRI Workshop with Mihaly Barasz and Paul Christiano, and refined further at the April 2013 MIRI Workshop.</p>
<p style=\"padding-left: 60px;\"><strong>Abstract:</strong></p>
<p style=\"padding-left: 30px;\">We model self-modication in AI by introducing 'tiling' agents whose decision systems will approve the construction of highly similar agents, creating a repeating pattern (including similarity of the offspring's goals). Â Constructing a formalism in the most straightforward way produces a Godelian difficulty, the Lobian obstacle. Â By technical methods we demonstrate the possibility of avoiding this obstacle, but the underlying puzzles of rational coherence are thus only partially addressed. Â We extend the formalism to partially unknown deterministic environments, and show a very crude extension to probabilistic environments and expected utility; but the problem of finding a fundamental decision criterion for self-modifying probabilistic agents remains open.</p>
<p>Commenting here is the preferred venue for discussion of the paper. Â This is an early draft and has not been reviewed, so it may contain mathematical errors, and reporting of these will be much appreciated.</p>
<p>The overall agenda of the paper is introduce the conceptual notion of a self-reproducing decision pattern which includes reproduction of the goal or utility function, by exposing a particular possible problem with a tiling logical decision pattern and coming up with some partial technical solutions. Â This then makes it conceptually much clearer to point out the even deeper problems with \"We can't yet describe a probabilistic way to do this because of non-monotonicity\" and \"We don't have a good bounded way to do this because maximization is impossible, satisficing is too weak and Schmidhuber's swapping criterion is underspecified.\" Â The paper uses first-order logic (FOL) because FOL has a lot of useful standard machinery for reflection which we can then invoke; in real life, FOL is of course a poor representational fit to most real-world environments outside a human-constructed computer chip with thermodynamically expensive crisp variable states.</p>
<p>As further background, the idea that something-like-proof might be relevant to Friendly AI is not about achieving some chimera of absolute safety-feeling, but rather about the idea that the total probability of catastrophic failure should not have a significant conditionally independent component on each self-modification, and that self-modification will (at least in initial stages) take place within the highly deterministic environment of a computer chip. Â This means that statistical testing methods (e.g. an evolutionary algorithm's evaluation of average fitness on a set of test problems) are not suitable for self-modifications which can potentially induce catastrophic failure (e.g. of parts of code that can affect the representation or interpretation of the goals). Â Mathematical proofs have the property that they are as strong as their axioms and have no significant conditionally independent per-step failure probability if their axioms are semantically true, which suggests that something like mathematical reasoning may be appropriate for certain particular types of self-modification during some developmental stages.</p>
<p>Thus the content of the paper is very far off from how a realistic AI would work, but conversely, if you can't even answer the kinds of simple problems posed within the paper (both those we partially solve and those we only pose) then you must be very far off from being able to build a stable self-modifying AI. Â Being able to say how to build a theoretical device that would play perfect chess given infinite computing power, is very far off from the ability to build Deep Blue. Â However, if you can't even say how to play perfect chess given infinite computing power, you are confused about the rules of the chess or the structure of chess-playing computation in a way that would make it entirelyÂ hopeless for you to figure out how to build a bounded chess-player. Â Thus \"In real life we're always bounded\" is no excuse for not being able to solve the much simpler unbounded form of the problem, and being able to describe the infinite chess-player would be substantial and useful conceptual progress compared to <em>not </em>being able to do that. Â We can't be absolutely certain that an analogous situation holds between solving the challenges posed in the paper, and realistic self-modifying AIs with stable goal systems, but every line of investigation has to start somewhere.</p>
<p>Parts of the paper will be easier to understand if you've read <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\">Highly Advanced Epistemology 101 For Beginners</a> including the parts on correspondence theories of truth (relevant to section 6) and model-theoretic semantics of logic (relevant to 3, 4, and 6), and there are footnotes intended to make the paper somewhat more accessible than usual, but the paper is still essentially aimed at mathematically sophisticated readers.</p></div>
<a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/#comments\">242 comments</a>
" nil nil "775c06e4c70d026965aa3973410b697c") (23 (20948 11037 255050) "http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/" "Rationality Quotes July 2013" nil "Tue, 02 Jul 2013 16:21:59 +0000" "Submitted by <a href=\"http://lesswrong.com/user/Vaniver\">Vaniver</a>
#
3 votes
#
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">109 comments</a>
<div><div id=\"entry_t3_hlk\" class=\"content clear\">
<div class=\"md\">
<div>
<div>
<p>Another month has passed and here is a new rationality quotes thread. The usual rules are:</p>
<ul>
<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>
<li>Do not quote yourself.</li>
<li>Do not quote from Less Wrong itself, Overcoming Bias, HPMoR, Eliezer Yudkowsky, or Robin Hanson.</li>
<li>No more than 5 quotes per person per monthly thread, please.</li>
</ul>
</div>
</div>
</div>
</div></div>
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">109 comments</a>
" nil nil "4699ae5d183463d6b2893ee465990059") (22 (20948 9032 887170) "http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/" "Rationality Quotes July 2013" nil "Tue, 02 Jul 2013 16:21:59 +0000" "Submitted by <a href=\"http://lesswrong.com/user/Vaniver\">Vaniver</a>
#
3 votes
#
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">108 comments</a>
<div><div id=\"entry_t3_hlk\" class=\"content clear\">
<div class=\"md\">
<div>
<div>
<p>Another month has passed and here is a new rationality quotes thread. The usual rules are:</p>
<ul>
<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>
<li>Do not quote yourself.</li>
<li>Do not quote from Less Wrong itself, Overcoming Bias, HPMoR, Eliezer Yudkowsky, or Robin Hanson.</li>
<li>No more than 5 quotes per person per monthly thread, please.</li>
</ul>
</div>
</div>
</div>
</div></div>
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">108 comments</a>
" nil nil "44e2080c0cf567c0b428cdf0d8d70c0b") (21 (20948 5511 725486) "http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/" "Rationality Quotes July 2013" nil "Tue, 02 Jul 2013 16:21:59 +0000" "Submitted by <a href=\"http://lesswrong.com/user/Vaniver\">Vaniver</a>
#
3 votes
#
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">107 comments</a>
<div><div id=\"entry_t3_hlk\" class=\"content clear\">
<div class=\"md\">
<div>
<div>
<p>Another month has passed and here is a new rationality quotes thread. The usual rules are:</p>
<ul>
<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>
<li>Do not quote yourself.</li>
<li>Do not quote from Less Wrong itself, Overcoming Bias, HPMoR, Eliezer Yudkowsky, or Robin Hanson.</li>
<li>No more than 5 quotes per person per monthly thread, please.</li>
</ul>
</div>
</div>
</div>
</div></div>
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">107 comments</a>
" nil nil "8f2dfc7498e865a6770a387d24e240d5") (20 (20948 5511 722433) "http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/" "Tiling Agents for Self-Modifying AI (OPFAI #2)" nil "Fri, 07 Jun 2013 06:24:25 +1000" "Submitted by <a href=\"http://lesswrong.com/user/Eliezer_Yudkowsky\">Eliezer_Yudkowsky</a>
#
51 votes
#
<a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/#comments\">241 comments</a>
<div><p>An early draft of publication #2 in the Open Problems in Friendly AI series is now available: Â <a href=\"http://intelligence.org/files/TilingAgents.pdf\">Tiling Agents for Self-Modifying AI, and the Lobian Obstacle</a>.Â  ~20,000 words, aimed at mathematicians or the highly mathematically literate.Â  The research reported on was conducted by Yudkowsky and Herreshoff, substantially refined at the November 2012 MIRI Workshop with Mihaly Barasz and Paul Christiano, and refined further at the April 2013 MIRI Workshop.</p>
<p style=\"padding-left: 60px;\"><strong>Abstract:</strong></p>
<p style=\"padding-left: 30px;\">We model self-modication in AI by introducing 'tiling' agents whose decision systems will approve the construction of highly similar agents, creating a repeating pattern (including similarity of the offspring's goals). Â Constructing a formalism in the most straightforward way produces a Godelian difficulty, the Lobian obstacle. Â By technical methods we demonstrate the possibility of avoiding this obstacle, but the underlying puzzles of rational coherence are thus only partially addressed. Â We extend the formalism to partially unknown deterministic environments, and show a very crude extension to probabilistic environments and expected utility; but the problem of finding a fundamental decision criterion for self-modifying probabilistic agents remains open.</p>
<p>Commenting here is the preferred venue for discussion of the paper. Â This is an early draft and has not been reviewed, so it may contain mathematical errors, and reporting of these will be much appreciated.</p>
<p>The overall agenda of the paper is introduce the conceptual notion of a self-reproducing decision pattern which includes reproduction of the goal or utility function, by exposing a particular possible problem with a tiling logical decision pattern and coming up with some partial technical solutions. Â This then makes it conceptually much clearer to point out the even deeper problems with \"We can't yet describe a probabilistic way to do this because of non-monotonicity\" and \"We don't have a good bounded way to do this because maximization is impossible, satisficing is too weak and Schmidhuber's swapping criterion is underspecified.\" Â The paper uses first-order logic (FOL) because FOL has a lot of useful standard machinery for reflection which we can then invoke; in real life, FOL is of course a poor representational fit to most real-world environments outside a human-constructed computer chip with thermodynamically expensive crisp variable states.</p>
<p>As further background, the idea that something-like-proof might be relevant to Friendly AI is not about achieving some chimera of absolute safety-feeling, but rather about the idea that the total probability of catastrophic failure should not have a significant conditionally independent component on each self-modification, and that self-modification will (at least in initial stages) take place within the highly deterministic environment of a computer chip. Â This means that statistical testing methods (e.g. an evolutionary algorithm's evaluation of average fitness on a set of test problems) are not suitable for self-modifications which can potentially induce catastrophic failure (e.g. of parts of code that can affect the representation or interpretation of the goals). Â Mathematical proofs have the property that they are as strong as their axioms and have no significant conditionally independent per-step failure probability if their axioms are semantically true, which suggests that something like mathematical reasoning may be appropriate for certain particular types of self-modification during some developmental stages.</p>
<p>Thus the content of the paper is very far off from how a realistic AI would work, but conversely, if you can't even answer the kinds of simple problems posed within the paper (both those we partially solve and those we only pose) then you must be very far off from being able to build a stable self-modifying AI. Â Being able to say how to build a theoretical device that would play perfect chess given infinite computing power, is very far off from the ability to build Deep Blue. Â However, if you can't even say how to play perfect chess given infinite computing power, you are confused about the rules of the chess or the structure of chess-playing computation in a way that would make it entirelyÂ hopeless for you to figure out how to build a bounded chess-player. Â Thus \"In real life we're always bounded\" is no excuse for not being able to solve the much simpler unbounded form of the problem, and being able to describe the infinite chess-player would be substantial and useful conceptual progress compared to <em>not </em>being able to do that. Â We can't be absolutely certain that an analogous situation holds between solving the challenges posed in the paper, and realistic self-modifying AIs with stable goal systems, but every line of investigation has to start somewhere.</p>
<p>Parts of the paper will be easier to understand if you've read <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\">Highly Advanced Epistemology 101 For Beginners</a> including the parts on correspondence theories of truth (relevant to section 6) and model-theoretic semantics of logic (relevant to 3, 4, and 6), and there are footnotes intended to make the paper somewhat more accessible than usual, but the paper is still essentially aimed at mathematically sophisticated readers.</p></div>
<a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/#comments\">241 comments</a>
" nil nil "67b4f8a446d47aabaecea9159b6c7473") (19 (20947 64192 506087) "http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/" "Rationality Quotes July 2013" nil "Tue, 02 Jul 2013 16:21:59 +0000" "Submitted by <a href=\"http://lesswrong.com/user/Vaniver\">Vaniver</a>
#
3 votes
#
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">104 comments</a>
<div><div id=\"entry_t3_hlk\" class=\"content clear\">
<div class=\"md\">
<div>
<div>
<p>Another month has passed and here is a new rationality quotes thread. The usual rules are:</p>
<ul>
<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>
<li>Do not quote yourself.</li>
<li>Do not quote from Less Wrong itself, Overcoming Bias, HPMoR, Eliezer Yudkowsky, or Robin Hanson.</li>
<li>No more than 5 quotes per person per monthly thread, please.</li>
</ul>
</div>
</div>
</div>
</div></div>
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">104 comments</a>
" nil nil "299538d3891a5d789b1718b12b8f53dd") (18 (20947 64192 502888) "http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/" "Tiling Agents for Self-Modifying AI (OPFAI #2)" nil "Fri, 07 Jun 2013 06:24:25 +1000" "Submitted by <a href=\"http://lesswrong.com/user/Eliezer_Yudkowsky\">Eliezer_Yudkowsky</a>
#
51 votes
#
<a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/#comments\">239 comments</a>
<div><p>An early draft of publication #2 in the Open Problems in Friendly AI series is now available: Â <a href=\"http://intelligence.org/files/TilingAgents.pdf\">Tiling Agents for Self-Modifying AI, and the Lobian Obstacle</a>.Â  ~20,000 words, aimed at mathematicians or the highly mathematically literate.Â  The research reported on was conducted by Yudkowsky and Herreshoff, substantially refined at the November 2012 MIRI Workshop with Mihaly Barasz and Paul Christiano, and refined further at the April 2013 MIRI Workshop.</p>
<p style=\"padding-left: 60px;\"><strong>Abstract:</strong></p>
<p style=\"padding-left: 30px;\">We model self-modication in AI by introducing 'tiling' agents whose decision systems will approve the construction of highly similar agents, creating a repeating pattern (including similarity of the offspring's goals). Â Constructing a formalism in the most straightforward way produces a Godelian difficulty, the Lobian obstacle. Â By technical methods we demonstrate the possibility of avoiding this obstacle, but the underlying puzzles of rational coherence are thus only partially addressed. Â We extend the formalism to partially unknown deterministic environments, and show a very crude extension to probabilistic environments and expected utility; but the problem of finding a fundamental decision criterion for self-modifying probabilistic agents remains open.</p>
<p>Commenting here is the preferred venue for discussion of the paper. Â This is an early draft and has not been reviewed, so it may contain mathematical errors, and reporting of these will be much appreciated.</p>
<p>The overall agenda of the paper is introduce the conceptual notion of a self-reproducing decision pattern which includes reproduction of the goal or utility function, by exposing a particular possible problem with a tiling logical decision pattern and coming up with some partial technical solutions. Â This then makes it conceptually much clearer to point out the even deeper problems with \"We can't yet describe a probabilistic way to do this because of non-monotonicity\" and \"We don't have a good bounded way to do this because maximization is impossible, satisficing is too weak and Schmidhuber's swapping criterion is underspecified.\" Â The paper uses first-order logic (FOL) because FOL has a lot of useful standard machinery for reflection which we can then invoke; in real life, FOL is of course a poor representational fit to most real-world environments outside a human-constructed computer chip with thermodynamically expensive crisp variable states.</p>
<p>As further background, the idea that something-like-proof might be relevant to Friendly AI is not about achieving some chimera of absolute safety-feeling, but rather about the idea that the total probability of catastrophic failure should not have a significant conditionally independent component on each self-modification, and that self-modification will (at least in initial stages) take place within the highly deterministic environment of a computer chip. Â This means that statistical testing methods (e.g. an evolutionary algorithm's evaluation of average fitness on a set of test problems) are not suitable for self-modifications which can potentially induce catastrophic failure (e.g. of parts of code that can affect the representation or interpretation of the goals). Â Mathematical proofs have the property that they are as strong as their axioms and have no significant conditionally independent per-step failure probability if their axioms are semantically true, which suggests that something like mathematical reasoning may be appropriate for certain particular types of self-modification during some developmental stages.</p>
<p>Thus the content of the paper is very far off from how a realistic AI would work, but conversely, if you can't even answer the kinds of simple problems posed within the paper (both those we partially solve and those we only pose) then you must be very far off from being able to build a stable self-modifying AI. Â Being able to say how to build a theoretical device that would play perfect chess given infinite computing power, is very far off from the ability to build Deep Blue. Â However, if you can't even say how to play perfect chess given infinite computing power, you are confused about the rules of the chess or the structure of chess-playing computation in a way that would make it entirelyÂ hopeless for you to figure out how to build a bounded chess-player. Â Thus \"In real life we're always bounded\" is no excuse for not being able to solve the much simpler unbounded form of the problem, and being able to describe the infinite chess-player would be substantial and useful conceptual progress compared to <em>not </em>being able to do that. Â We can't be absolutely certain that an analogous situation holds between solving the challenges posed in the paper, and realistic self-modifying AIs with stable goal systems, but every line of investigation has to start somewhere.</p>
<p>Parts of the paper will be easier to understand if you've read <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\">Highly Advanced Epistemology 101 For Beginners</a> including the parts on correspondence theories of truth (relevant to section 6) and model-theoretic semantics of logic (relevant to 3, 4, and 6), and there are footnotes intended to make the paper somewhat more accessible than usual, but the paper is still essentially aimed at mathematically sophisticated readers.</p></div>
<a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/#comments\">239 comments</a>
" nil nil "e119491db3f9fa30c18eb95baba9101b") (17 (20947 56067 926717) "http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/" "Rationality Quotes July 2013" nil "Tue, 02 Jul 2013 16:21:59 +0000" "Submitted by <a href=\"http://lesswrong.com/user/Vaniver\">Vaniver</a>
#
2 votes
#
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">96 comments</a>
<div><div id=\"entry_t3_hlk\" class=\"content clear\">
<div class=\"md\">
<div>
<div>
<p>Another month has passed and here is a new rationality quotes thread. The usual rules are:</p>
<ul>
<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>
<li>Do not quote yourself.</li>
<li>Do not quote from Less Wrong itself, Overcoming Bias, HPMoR, Eliezer Yudkowsky, or Robin Hanson.</li>
<li>No more than 5 quotes per person per monthly thread, please.</li>
</ul>
</div>
</div>
</div>
</div></div>
<a href=\"http://lesswrong.com/lw/hv9/rationality_quotes_july_2013/#comments\">96 comments</a>
" nil nil "01821c3bfecc9f05dd5a8a1622f0ca9d") (16 (20947 56067 926266) "http://lesswrong.com/lw/hui/new_lw_meetup_lyon/" "New LW Meetup: Lyon" nil "Sat, 29 Jun 2013 08:41:08 +1000" "Submitted by <a href=\"http://lesswrong.com/user/FrankAdamek\">FrankAdamek</a>
#
4 votes
#
<a href=\"http://lesswrong.com/lw/hui/new_lw_meetup_lyon/#comments\">4 comments</a>
<div><p>New meetups (or meetups with a hiatus of more than a year) are happening in:</p>
<ul>
<li><a href=\"/meetups/o1\">[Lyon, France] LW Meetup in Lyon:Â <span class=\"date\">03 July 2013 06:00PM</span></a></li>
</ul>
<p>Other irregularly scheduled Less Wrong meetups are taking place in:</p>
<ul>
<li><a href=\"/meetups/ns\">Brussels meetup with HEALES:Â <span class=\"date\">13 July 2013 01:00PM</span></a></li>
<li><a href=\"/meetups/o2\">Frankfurt meetup:Â <span class=\"date\">30 June 2013 04:30PM</span></a></li>
<li><a href=\"/meetups/o5\">Israel LW meetup:Â <span class=\"date\">04 July 2013 07:00PM</span></a></li>
<li><a href=\"/meetups/o3\">[Moscow] The Goals We Set:Â <span class=\"date\">07 July 2013 04:00PM</span></a></li>
<li><a href=\"/meetups/o4\">[Munich] LW Munich Meetup in July:Â <span class=\"date\">06 July 2013 03:00PM</span></a></li>
<li><a href=\"/meetups/ny\">San Francisco: Effective Altruism:Â <span class=\"date\">28 June 2013 07:54PM</span></a></li>
<li><a href=\"/meetups/o8\">[Vienna] LW Vienna Meetup #4:Â <span class=\"date\">13 July 2013 03:00PM</span></a></li>
</ul>
<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>
<ul>
<li><a href=\"/meetups/bx\">Austin, TX:Â <span class=\"date\">29 June 2019 01:30PM</span></a></li>
<li><a href=\"http://lesswrong.com/meetups/o6\">London Practical - Sunday 7th July:Â <span class=\"date\">07 July 2013 02:00PM</span></a></li>
<li><a href=\"/meetups/o7\">Melbourne LW Outing: Astronomy evening in Eltham, Saturday 29th June, 5:30pm:Â <span class=\"date\">29 June 2013 05:30PM</span></a></li>
<li><a href=\"/meetups/nz\">Melbourne LW Outing: Indoor Rock Climbing, Sunday June 30th:Â <span class=\"date\">30 June 2013 02:00PM</span></a></li>
</ul>
<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong> </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>
<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>
<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>
<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>
<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetupÂ <em>before </em>the Friday before your meetup!</p>
<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>,</strong><strong style=\"font-weight: bold;\"></strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,Â <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>
<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>
<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>
<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p></div>
<a href=\"http://lesswrong.com/lw/hui/new_lw_meetup_lyon/#comments\">4 comments</a>
" nil nil "4bc8e00a1b33b5f3296a41561f8d0e8d") (15 (20947 56067 923314) "http://lesswrong.com/lw/hmx/prisoners_dilemma_with_visible_source_code/" "Prisoner's Dilemma (with visible source code) Tournament" nil "Fri, 07 Jun 2013 18:30:20 +1000" "Submitted by <a href=\"http://lesswrong.com/user/AlexMennen\">AlexMennen</a>
#
47 votes
#
<a href=\"http://lesswrong.com/lw/hmx/prisoners_dilemma_with_visible_source_code/#comments\">229 comments</a>
<div><p style=\"margin-bottom: 0in\">After the <a href=\"/lw/7f2/prisoners_dilemma_tournament_results/\">iterated prisoner's dilemma tournament</a>Â organized by prase two years ago, there was discussion of running tournaments for several variants, including one in which two players submit programs, each of which are given the source code of the other player's program, and outputs either âcooperateâ or âdefectâ. However, as far as I know, no such tournament has been run until now.</p>
<p style=\"margin-bottom: 0in\">Here's how it's going to work: Each player will submit a file containing a single Scheme lambda-function. The function should take one input. Your program will play exactly one round against each other program submitted (not including itself). In each round, two programs will be run, each given the source code of the other as input, and will be expected to return either of the symbols âCâ or âDâ (for \"cooperate\" and \"defect\", respectively). The programs will receive points based on the following payoff matrix:</p>
<p style=\"margin-bottom: 0in\"><img src=\"http://www.codecogs.com/png.latex?%5Cbegin%7Barray%7D%7Bcccc%7D%20&%20C%20&%20D%20&%20other%5C%5C%20C%20&%20(2,%5C,2)%20&%20(0,%5C,3)%20&%20(0,%5C,2)%5C%5C%20D%20&%20(3,%5C,0)%20&%20(1,%5C,1)%20&%20(1,%5C,0)%5C%5C%20other%20&%20(2,%5C,0)%20&%20(0,%5C,1)%20&%20(0,%5C,0)%20%5Cend%7Barray%7D\" alt=\"\" height=\"84\" width=\"215\"></p>
<p style=\"margin-bottom: 0in\">âOtherâ includes any result other than returning âCâ or âDâ, including failing to terminate, throwing an exception, and even returning the string âCooperateâ. Notice that âOtherâ results in a worst-of-both-worlds scenario where you get the same payoff as you would have if you cooperated, but the other player gets the same payoff as if you had defected. This is an attempt to ensure that no one ever has incentive for their program to fail to run properly, or to trick another program into doing so.</p>
<p style=\"margin-bottom: 0in\">Your score is the sum of the number of points you earn in each round. The player with the highest score wins the tournament. <strong>Edit: There is a <a href=\"/lw/hmx/prisoners_dilemma_with_visible_source_code/94no\">0.5 bitcoin prize</a> being offered for the winner. Thanks, VincentYu!</strong></p>
<p style=\"margin-bottom: 0in\">Details:<br>All submissions must be emailed to <a href=\"mailto:wardenPD@gmail.com\">wardenPD@gmail.com</a> by July 5, at noon PDT. Your email should also say how you would like to be identified when I announce the tournament results.<br>Each program will be allowed to run for 10 seconds. If it has not returned either âCâ or âDâ by then, it will be stopped, and treated as returning âOtherâ. For consistency, I will have Scheme collect garbage right before each run.<br>One submission per person or team. No person may contribute to more than one entry. <strong>Edit: This also means no copying from each others' source code. Describing the behavior of your program to others is okay.</strong><br>I will be running the submissions in Racket. You may be interested in how Racket handles <a href=\"http://docs.racket-lang.org/reference/time.html\">time</a>Â (especially the (current-milliseconds) function), <a href=\"http://docs.racket-lang.org/reference/threads.html\">threads</a>Â (in particular, âthreadâ, âkill-threadâ, âsleepâ, and âthread-dead?â), and possibly <a href=\"http://docs.racket-lang.org/reference/generic-numbers.html#%28def._%28%28quote._~23~25kernel%29._random%29%29\">randomness</a>.<br>Don't try to open the file you wrote your program in (or any other file, for that matter). I'll add code to the file before running it, so if you want your program to use a copy of your source code, you will need to use a quine. <strong>Edit: No I/O of any sort.</strong><br>Unless you tell me otherwise, I assume I have permission to publish your code after the contest.<br>You are encouraged to discuss strategies for achieving mutual cooperation in the comments thread.<br>I'm hoping to get as many entries as possible. If you know someone who might be interested in this, please tell them.<br>It's possible that I've said something stupid that I'll have to change or clarify, so you might want to come back to this page again occasionally to look for changes to the rules. Any edits will be bolded, and I'll try not to change anything too drastically, or make any edits late in the contest.</p>
<p style=\"margin-bottom: 0in\">Here is an example of a correct entry, which cooperates with you if and only if you would cooperate with a program that always cooperates (actually, if and only if you would cooperate with one particular program that always cooperates):</p>
<blockquote>
<p style=\"margin-bottom: 0in;\">(lambda (x)<br>Â  Â  (if (eq? ((eval x) '(lambda (y) 'C)) 'C)<br>Â  Â  Â  Â  'C<br>Â  Â  Â  Â  'D))</p>
</blockquote></div>
<a href=\"http://lesswrong.com/lw/hmx/prisoners_dilemma_with_visible_source_code/#comments\">229 comments</a>
" nil nil "c470afaa1df8d345814aa118371b57e4") (14 (20947 56067 922223) "http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/" "Tiling Agents for Self-Modifying AI (OPFAI #2)" nil "Fri, 07 Jun 2013 06:24:25 +1000" "Submitted by <a href=\"http://lesswrong.com/user/Eliezer_Yudkowsky\">Eliezer_Yudkowsky</a>
#
51 votes
#
<a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/#comments\">238 comments</a>
<div><p>An early draft of publication #2 in the Open Problems in Friendly AI series is now available: Â <a href=\"http://intelligence.org/files/TilingAgents.pdf\">Tiling Agents for Self-Modifying AI, and the Lobian Obstacle</a>.Â  ~20,000 words, aimed at mathematicians or the highly mathematically literate.Â  The research reported on was conducted by Yudkowsky and Herreshoff, substantially refined at the November 2012 MIRI Workshop with Mihaly Barasz and Paul Christiano, and refined further at the April 2013 MIRI Workshop.</p>
<p style=\"padding-left: 60px;\"><strong>Abstract:</strong></p>
<p style=\"padding-left: 30px;\">We model self-modication in AI by introducing 'tiling' agents whose decision systems will approve the construction of highly similar agents, creating a repeating pattern (including similarity of the offspring's goals). Â Constructing a formalism in the most straightforward way produces a Godelian difficulty, the Lobian obstacle. Â By technical methods we demonstrate the possibility of avoiding this obstacle, but the underlying puzzles of rational coherence are thus only partially addressed. Â We extend the formalism to partially unknown deterministic environments, and show a very crude extension to probabilistic environments and expected utility; but the problem of finding a fundamental decision criterion for self-modifying probabilistic agents remains open.</p>
<p>Commenting here is the preferred venue for discussion of the paper. Â This is an early draft and has not been reviewed, so it may contain mathematical errors, and reporting of these will be much appreciated.</p>
<p>The overall agenda of the paper is introduce the conceptual notion of a self-reproducing decision pattern which includes reproduction of the goal or utility function, by exposing a particular possible problem with a tiling logical decision pattern and coming up with some partial technical solutions. Â This then makes it conceptually much clearer to point out the even deeper problems with \"We can't yet describe a probabilistic way to do this because of non-monotonicity\" and \"We don't have a good bounded way to do this because maximization is impossible, satisficing is too weak and Schmidhuber's swapping criterion is underspecified.\" Â The paper uses first-order logic (FOL) because FOL has a lot of useful standard machinery for reflection which we can then invoke; in real life, FOL is of course a poor representational fit to most real-world environments outside a human-constructed computer chip with thermodynamically expensive crisp variable states.</p>
<p>As further background, the idea that something-like-proof might be relevant to Friendly AI is not about achieving some chimera of absolute safety-feeling, but rather about the idea that the total probability of catastrophic failure should not have a significant conditionally independent component on each self-modification, and that self-modification will (at least in initial stages) take place within the highly deterministic environment of a computer chip. Â This means that statistical testing methods (e.g. an evolutionary algorithm's evaluation of average fitness on a set of test problems) are not suitable for self-modifications which can potentially induce catastrophic failure (e.g. of parts of code that can affect the representation or interpretation of the goals). Â Mathematical proofs have the property that they are as strong as their axioms and have no significant conditionally independent per-step failure probability if their axioms are semantically true, which suggests that something like mathematical reasoning may be appropriate for certain particular types of self-modification during some developmental stages.</p>
<p>Thus the content of the paper is very far off from how a realistic AI would work, but conversely, if you can't even answer the kinds of simple problems posed within the paper (both those we partially solve and those we only pose) then you must be very far off from being able to build a stable self-modifying AI. Â Being able to say how to build a theoretical device that would play perfect chess given infinite computing power, is very far off from the ability to build Deep Blue. Â However, if you can't even say how to play perfect chess given infinite computing power, you are confused about the rules of the chess or the structure of chess-playing computation in a way that would make it entirelyÂ hopeless for you to figure out how to build a bounded chess-player. Â Thus \"In real life we're always bounded\" is no excuse for not being able to solve the much simpler unbounded form of the problem, and being able to describe the infinite chess-player would be substantial and useful conceptual progress compared to <em>not </em>being able to do that. Â We can't be absolutely certain that an analogous situation holds between solving the challenges posed in the paper, and realistic self-modifying AIs with stable goal systems, but every line of investigation has to start somewhere.</p>
<p>Parts of the paper will be easier to understand if you've read <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\">Highly Advanced Epistemology 101 For Beginners</a> including the parts on correspondence theories of truth (relevant to section 6) and model-theoretic semantics of logic (relevant to 3, 4, and 6), and there are footnotes intended to make the paper somewhat more accessible than usual, but the paper is still essentially aimed at mathematically sophisticated readers.</p></div>
<a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/#comments\">238 comments</a>
" nil nil "e4e16c38cf1a5b28b58fe0dbb48f5f90") (13 (20946 63202 305004) "http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/" "Tiling Agents for Self-Modifying AI (OPFAI #2)" nil "Fri, 07 Jun 2013 06:24:25 +1000" "Submitted by <a href=\"http://lesswrong.com/user/Eliezer_Yudkowsky\">Eliezer_Yudkowsky</a>
#
51 votes
#
<a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/#comments\">234 comments</a>
<div><p>An early draft of publication #2 in the Open Problems in Friendly AI series is now available: Â <a href=\"http://intelligence.org/files/TilingAgents.pdf\">Tiling Agents for Self-Modifying AI, and the Lobian Obstacle</a>.Â  ~20,000 words, aimed at mathematicians or the highly mathematically literate.Â  The research reported on was conducted by Yudkowsky and Herreshoff, substantially refined at the November 2012 MIRI Workshop with Mihaly Barasz and Paul Christiano, and refined further at the April 2013 MIRI Workshop.</p>
<p style=\"padding-left: 60px;\"><strong>Abstract:</strong></p>
<p style=\"padding-left: 30px;\">We model self-modication in AI by introducing 'tiling' agents whose decision systems will approve the construction of highly similar agents, creating a repeating pattern (including similarity of the offspring's goals). Â Constructing a formalism in the most straightforward way produces a Godelian difficulty, the Lobian obstacle. Â By technical methods we demonstrate the possibility of avoiding this obstacle, but the underlying puzzles of rational coherence are thus only partially addressed. Â We extend the formalism to partially unknown deterministic environments, and show a very crude extension to probabilistic environments and expected utility; but the problem of finding a fundamental decision criterion for self-modifying probabilistic agents remains open.</p>
<p>Commenting here is the preferred venue for discussion of the paper. Â This is an early draft and has not been reviewed, so it may contain mathematical errors, and reporting of these will be much appreciated.</p>
<p>The overall agenda of the paper is introduce the conceptual notion of a self-reproducing decision pattern which includes reproduction of the goal or utility function, by exposing a particular possible problem with a tiling logical decision pattern and coming up with some partial technical solutions. Â This then makes it conceptually much clearer to point out the even deeper problems with \"We can't yet describe a probabilistic way to do this because of non-monotonicity\" and \"We don't have a good bounded way to do this because maximization is impossible, satisficing is too weak and Schmidhuber's swapping criterion is underspecified.\" Â The paper uses first-order logic (FOL) because FOL has a lot of useful standard machinery for reflection which we can then invoke; in real life, FOL is of course a poor representational fit to most real-world environments outside a human-constructed computer chip with thermodynamically expensive crisp variable states.</p>
<p>As further background, the idea that something-like-proof might be relevant to Friendly AI is not about achieving some chimera of absolute safety-feeling, but rather about the idea that the total probability of catastrophic failure should not have a significant conditionally independent component on each self-modification, and that self-modification will (at least in initial stages) take place within the highly deterministic environment of a computer chip. Â This means that statistical testing methods (e.g. an evolutionary algorithm's evaluation of average fitness on a set of test problems) are not suitable for self-modifications which can potentially induce catastrophic failure (e.g. of parts of code that can affect the representation or interpretation of the goals). Â Mathematical proofs have the property that they are as strong as their axioms and have no significant conditionally independent per-step failure probability if their axioms are semantically true, which suggests that something like mathematical reasoning may be appropriate for certain particular types of self-modification during some developmental stages.</p>
<p>Thus the content of the paper is very far off from how a realistic AI would work, but conversely, if you can't even answer the kinds of simple problems posed within the paper (both those we partially solve and those we only pose) then you must be very far off from being able to build a stable self-modifying AI. Â Being able to say how to build a theoretical device that would play perfect chess given infinite computing power, is very far off from the ability to build Deep Blue. Â However, if you can't even say how to play perfect chess given infinite computing power, you are confused about the rules of the chess or the structure of chess-playing computation in a way that would make it entirelyÂ hopeless for you to figure out how to build a bounded chess-player. Â Thus \"In real life we're always bounded\" is no excuse for not being able to solve the much simpler unbounded form of the problem, and being able to describe the infinite chess-player would be substantial and useful conceptual progress compared to <em>not </em>being able to do that. Â We can't be absolutely certain that an analogous situation holds between solving the challenges posed in the paper, and realistic self-modifying AIs with stable goal systems, but every line of investigation has to start somewhere.</p>
<p>Parts of the paper will be easier to understand if you've read <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\">Highly Advanced Epistemology 101 For Beginners</a> including the parts on correspondence theories of truth (relevant to section 6) and model-theoretic semantics of logic (relevant to 3, 4, and 6), and there are footnotes intended to make the paper somewhat more accessible than usual, but the paper is still essentially aimed at mathematically sophisticated readers.</p></div>
<a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/#comments\">234 comments</a>
" nil nil "e43fc9415515b153f1ddb10d6ce4e0b2") (12 (20946 62725 591996) "http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/" "Tiling Agents for Self-Modifying AI (OPFAI #2)" nil "Fri, 07 Jun 2013 06:24:25 +1000" "Submitted by <a href=\"http://lesswrong.com/user/Eliezer_Yudkowsky\">Eliezer_Yudkowsky</a>
#
51 votes
#
<a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/#comments\">233 comments</a>
<div><p>An early draft of publication #2 in the Open Problems in Friendly AI series is now available: Â <a href=\"http://intelligence.org/files/TilingAgents.pdf\">Tiling Agents for Self-Modifying AI, and the Lobian Obstacle</a>.Â  ~20,000 words, aimed at mathematicians or the highly mathematically literate.Â  The research reported on was conducted by Yudkowsky and Herreshoff, substantially refined at the November 2012 MIRI Workshop with Mihaly Barasz and Paul Christiano, and refined further at the April 2013 MIRI Workshop.</p>
<p style=\"padding-left: 60px;\"><strong>Abstract:</strong></p>
<p style=\"padding-left: 30px;\">We model self-modication in AI by introducing 'tiling' agents whose decision systems will approve the construction of highly similar agents, creating a repeating pattern (including similarity of the offspring's goals). Â Constructing a formalism in the most straightforward way produces a Godelian difficulty, the Lobian obstacle. Â By technical methods we demonstrate the possibility of avoiding this obstacle, but the underlying puzzles of rational coherence are thus only partially addressed. Â We extend the formalism to partially unknown deterministic environments, and show a very crude extension to probabilistic environments and expected utility; but the problem of finding a fundamental decision criterion for self-modifying probabilistic agents remains open.</p>
<p>Commenting here is the preferred venue for discussion of the paper. Â This is an early draft and has not been reviewed, so it may contain mathematical errors, and reporting of these will be much appreciated.</p>
<p>The overall agenda of the paper is introduce the conceptual notion of a self-reproducing decision pattern which includes reproduction of the goal or utility function, by exposing a particular possible problem with a tiling logical decision pattern and coming up with some partial technical solutions. Â This then makes it conceptually much clearer to point out the even deeper problems with \"We can't yet describe a probabilistic way to do this because of non-monotonicity\" and \"We don't have a good bounded way to do this because maximization is impossible, satisficing is too weak and Schmidhuber's swapping criterion is underspecified.\" Â The paper uses first-order logic (FOL) because FOL has a lot of useful standard machinery for reflection which we can then invoke; in real life, FOL is of course a poor representational fit to most real-world environments outside a human-constructed computer chip with thermodynamically expensive crisp variable states.</p>
<p>As further background, the idea that something-like-proof might be relevant to Friendly AI is not about achieving some chimera of absolute safety-feeling, but rather about the idea that the total probability of catastrophic failure should not have a significant conditionally independent component on each self-modification, and that self-modification will (at least in initial stages) take place within the highly deterministic environment of a computer chip. Â This means that statistical testing methods (e.g. an evolutionary algorithm's evaluation of average fitness on a set of test problems) are not suitable for self-modifications which can potentially induce catastrophic failure (e.g. of parts of code that can affect the representation or interpretation of the goals). Â Mathematical proofs have the property that they are as strong as their axioms and have no significant conditionally independent per-step failure probability if their axioms are semantically true, which suggests that something like mathematical reasoning may be appropriate for certain particular types of self-modification during some developmental stages.</p>
<p>Thus the content of the paper is very far off from how a realistic AI would work, but conversely, if you can't even answer the kinds of simple problems posed within the paper (both those we partially solve and those we only pose) then you must be very far off from being able to build a stable self-modifying AI. Â Being able to say how to build a theoretical device that would play perfect chess given infinite computing power, is very far off from the ability to build Deep Blue. Â However, if you can't even say how to play perfect chess given infinite computing power, you are confused about the rules of the chess or the structure of chess-playing computation in a way that would make it entirelyÂ hopeless for you to figure out how to build a bounded chess-player. Â Thus \"In real life we're always bounded\" is no excuse for not being able to solve the much simpler unbounded form of the problem, and being able to describe the infinite chess-player would be substantial and useful conceptual progress compared to <em>not </em>being able to do that. Â We can't be absolutely certain that an analogous situation holds between solving the challenges posed in the paper, and realistic self-modifying AIs with stable goal systems, but every line of investigation has to start somewhere.</p>
<p>Parts of the paper will be easier to understand if you've read <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\">Highly Advanced Epistemology 101 For Beginners</a> including the parts on correspondence theories of truth (relevant to section 6) and model-theoretic semantics of logic (relevant to 3, 4, and 6), and there are footnotes intended to make the paper somewhat more accessible than usual, but the paper is still essentially aimed at mathematically sophisticated readers.</p></div>
<a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/#comments\">233 comments</a>
" nil nil "a423c65c9baeab71dfbe114b569b91ea") (11 (20946 55653 512135) "http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/" "Tiling Agents for Self-Modifying AI (OPFAI #2)" nil "Fri, 07 Jun 2013 06:24:25 +1000" "Submitted by <a href=\"http://lesswrong.com/user/Eliezer_Yudkowsky\">Eliezer_Yudkowsky</a>
#
51 votes
#
<a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/#comments\">232 comments</a>
<div><p>An early draft of publication #2 in the Open Problems in Friendly AI series is now available: Â <a href=\"http://intelligence.org/files/TilingAgents.pdf\">Tiling Agents for Self-Modifying AI, and the Lobian Obstacle</a>.Â  ~20,000 words, aimed at mathematicians or the highly mathematically literate.Â  The research reported on was conducted by Yudkowsky and Herreshoff, substantially refined at the November 2012 MIRI Workshop with Mihaly Barasz and Paul Christiano, and refined further at the April 2013 MIRI Workshop.</p>
<p style=\"padding-left: 60px;\"><strong>Abstract:</strong></p>
<p style=\"padding-left: 30px;\">We model self-modication in AI by introducing 'tiling' agents whose decision systems will approve the construction of highly similar agents, creating a repeating pattern (including similarity of the offspring's goals). Â Constructing a formalism in the most straightforward way produces a Godelian difficulty, the Lobian obstacle. Â By technical methods we demonstrate the possibility of avoiding this obstacle, but the underlying puzzles of rational coherence are thus only partially addressed. Â We extend the formalism to partially unknown deterministic environments, and show a very crude extension to probabilistic environments and expected utility; but the problem of finding a fundamental decision criterion for self-modifying probabilistic agents remains open.</p>
<p>Commenting here is the preferred venue for discussion of the paper. Â This is an early draft and has not been reviewed, so it may contain mathematical errors, and reporting of these will be much appreciated.</p>
<p>The overall agenda of the paper is introduce the conceptual notion of a self-reproducing decision pattern which includes reproduction of the goal or utility function, by exposing a particular possible problem with a tiling logical decision pattern and coming up with some partial technical solutions. Â This then makes it conceptually much clearer to point out the even deeper problems with \"We can't yet describe a probabilistic way to do this because of non-monotonicity\" and \"We don't have a good bounded way to do this because maximization is impossible, satisficing is too weak and Schmidhuber's swapping criterion is underspecified.\" Â The paper uses first-order logic (FOL) because FOL has a lot of useful standard machinery for reflection which we can then invoke; in real life, FOL is of course a poor representational fit to most real-world environments outside a human-constructed computer chip with thermodynamically expensive crisp variable states.</p>
<p>As further background, the idea that something-like-proof might be relevant to Friendly AI is not about achieving some chimera of absolute safety-feeling, but rather about the idea that the total probability of catastrophic failure should not have a significant conditionally independent component on each self-modification, and that self-modification will (at least in initial stages) take place within the highly deterministic environment of a computer chip. Â This means that statistical testing methods (e.g. an evolutionary algorithm's evaluation of average fitness on a set of test problems) are not suitable for self-modifications which can potentially induce catastrophic failure (e.g. of parts of code that can affect the representation or interpretation of the goals). Â Mathematical proofs have the property that they are as strong as their axioms and have no significant conditionally independent per-step failure probability if their axioms are semantically true, which suggests that something like mathematical reasoning may be appropriate for certain particular types of self-modification during some developmental stages.</p>
<p>Thus the content of the paper is very far off from how a realistic AI would work, but conversely, if you can't even answer the kinds of simple problems posed within the paper (both those we partially solve and those we only pose) then you must be very far off from being able to build a stable self-modifying AI. Â Being able to say how to build a theoretical device that would play perfect chess given infinite computing power, is very far off from the ability to build Deep Blue. Â However, if you can't even say how to play perfect chess given infinite computing power, you are confused about the rules of the chess or the structure of chess-playing computation in a way that would make it entirelyÂ hopeless for you to figure out how to build a bounded chess-player. Â Thus \"In real life we're always bounded\" is no excuse for not being able to solve the much simpler unbounded form of the problem, and being able to describe the infinite chess-player would be substantial and useful conceptual progress compared to <em>not </em>being able to do that. Â We can't be absolutely certain that an analogous situation holds between solving the challenges posed in the paper, and realistic self-modifying AIs with stable goal systems, but every line of investigation has to start somewhere.</p>
<p>Parts of the paper will be easier to understand if you've read <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\">Highly Advanced Epistemology 101 For Beginners</a> including the parts on correspondence theories of truth (relevant to section 6) and model-theoretic semantics of logic (relevant to 3, 4, and 6), and there are footnotes intended to make the paper somewhat more accessible than usual, but the paper is still essentially aimed at mathematically sophisticated readers.</p></div>
<a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/#comments\">232 comments</a>
" nil nil "d999aa02663ec2019ba9df6dfc89d2e0") (10 (20946 49832 961818) "http://lesswrong.com/lw/hui/new_lw_meetup_lyon/" "New LW Meetup: Lyon" nil "Sat, 29 Jun 2013 08:41:08 +1000" "Submitted by <a href=\"http://lesswrong.com/user/FrankAdamek\">FrankAdamek</a>
#
4 votes
#
<a href=\"http://lesswrong.com/lw/hui/new_lw_meetup_lyon/#comments\">3 comments</a>
<div><p>New meetups (or meetups with a hiatus of more than a year) are happening in:</p>
<ul>
<li><a href=\"/meetups/o1\">[Lyon, France] LW Meetup in Lyon:Â <span class=\"date\">03 July 2013 06:00PM</span></a></li>
</ul>
<p>Other irregularly scheduled Less Wrong meetups are taking place in:</p>
<ul>
<li><a href=\"/meetups/ns\">Brussels meetup with HEALES:Â <span class=\"date\">13 July 2013 01:00PM</span></a></li>
<li><a href=\"/meetups/o2\">Frankfurt meetup:Â <span class=\"date\">30 June 2013 04:30PM</span></a></li>
<li><a href=\"/meetups/o5\">Israel LW meetup:Â <span class=\"date\">04 July 2013 07:00PM</span></a></li>
<li><a href=\"/meetups/o6\">London Practical - Sunday 7th July:Â <span class=\"date\">07 July 2013 02:00PM</span></a></li>
<li><a href=\"/meetups/o3\">[Moscow] The Goals We Set:Â <span class=\"date\">07 July 2013 04:00PM</span></a></li>
<li><a href=\"/meetups/o4\">[Munich] LW Munich Meetup in July:Â <span class=\"date\">06 July 2013 03:00PM</span></a></li>
<li><a href=\"/meetups/ny\">San Francisco: Effective Altruism:Â <span class=\"date\">28 June 2013 07:54PM</span></a></li>
<li><a href=\"/meetups/o8\">[Vienna] LW Vienna Meetup #4:Â <span class=\"date\">13 July 2013 03:00PM</span></a></li>
</ul>
<p>The remaining meetups take place in cities with regular scheduling, but involve a change in time or location, special meeting content, or simply a helpful reminder about the meetup:</p>
<ul>
<li><a href=\"/meetups/bx\">Austin, TX:Â <span class=\"date\">29 June 2019 01:30PM</span></a></li>
<li><a href=\"/meetups/o7\">Melbourne LW Outing: Astronomy evening in Eltham, Saturday 29th June, 5:30pm:Â <span class=\"date\">29 June 2013 05:30PM</span></a></li>
<li><a href=\"/meetups/nz\">Melbourne LW Outing: Indoor Rock Climbing, Sunday June 30th:Â <span class=\"date\">30 June 2013 02:00PM</span></a></li>
</ul>
<p>Locations with regularly scheduled meetups:<strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Austin.2C_TX\">Austin</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berkeley\">Berkeley</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_MA\">Cambridge, MA</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Cambridge.2C_UK\">Cambridge UK</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Madison.2C_WI\">Madison WI</a></strong>,<strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Melbourne\">Melbourne</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Tortuga_.28in_Mountain_View.29\">Mountain View</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#New_York_City.2C_NY\">New York</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ohio\">Ohio</a>,</strong><strong></strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Salt_Lake_City.2C_UT\">Salt Lake City</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Seattle.2C_WA\">Seattle</a>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Toronto\">Toronto</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vienna.2C_Austria\">Vienna</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Waterloo\"><strong>Waterloo</strong></a>, and <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">West Los Angeles</a></strong>. There's also a <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Online_Study_Hall\">24/7 online study hall</a> for coworking LWers.<a id=\"more\"></a></p>
<p>If you'd like to talk with other LW-ers face to face, and there is no meetup in your area, consider starting your own meetup; <a href=\"/lw/43s/starting_a_lw_meetup_is_easy\">it's easy</a> (more resources <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_group_resources\">here</a>). Check one out, stretch your rationality skills, <a href=\"/lw/dm4/berkely_visit_report/\">build community</a>, and have fun!</p>
<p>If you missed the deadline and wish to have your meetup featured, you can reach me on gmail at frank dot c dot adamek.</p>
<p>In addition to the handy sidebar of upcoming meetups, a meetup overview will continue to be posted on the front page every Friday. These will be an attempt to collect information on all the meetups happening in the next weeks. The best way to get your meetup featured is still to use the Add New Meetup feature, but you'll now also have the benefit of having your meetup mentioned in a weekly overview. These overview posts will be moved to the discussion section when the new post goes up.</p>
<p>Please note that for your meetup to appear in the weekly meetups feature, you need to post your meetupÂ <em>before </em>the Friday before your meetup!</p>
<p>If you check Less Wrong irregularly, consider subscribing to one or more city-specific mailing list in order to be notified when an irregular meetup is happening: <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Atlanta.2C_GA\">Atlanta</a>,</strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Berlin.2C_Germany\"><strong>Berlin</strong></a>,<strong style=\"font-weight: bold;\"> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Chicago.2C_IL\">Chicago</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Frankfurt.2C_Germany\">Frankfurt</a>,</strong> <strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Helsinki.2C_Finland\">Helsinki</a></strong><strong>, </strong><strong style=\"font-weight: bold;\"><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#London.2C_UK\">London</a>,Â </strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Bay_Area.2C_CA\">Marin CA</a></strong><strong>,</strong><strong> </strong><strong><a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Ottawa\">Ottawa</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Pittsburgh.2C_PA\">Pittsburgh</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Portland.2C_OR\">Portland</a>,</strong><strong> <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Southern_California.2C_CA\">Southern California (Los Angeles/Orange County area)</a>,Â <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#St_Louis.2C_MO\">St. Louis</a>, <a href=\"http://wiki.lesswrong.com/wiki/Less_Wrong_meetup_groups#Vancouver\">Vancouver</a>, </strong><a href=\"http://wiki.lesswrong.com/wiki/Meetup#Washington.2C_DC\"><strong>Washington, DC</strong></a>.</p>
<p>Whether or not there's currently a meetup in your area, you can <a href=\"/lw/f9p/sign_up_to_be_notified_about_new_lw_meetups_in/\"><strong>sign up</strong></a> to be notified automatically of any future meetups. And if you're not interested in notifications you can still enter your approximate location, which will let meetup-starting heroes know that there's an interested LW population in their city!</p>
<p>If your meetup has a mailing list that you'd like mentioned here, or has become regular and isn't listed as such, let me know!</p>
<p>Want to help out the common good? If one of the meetups listed as regular has become inactive, let me know so we can present more accurate information to newcomers.</p></div>
<a href=\"http://lesswrong.com/lw/hui/new_lw_meetup_lyon/#comments\">3 comments</a>
" nil nil "bb8d80d2a883c1e7c6637976c2468b73") (9 (20946 49832 960453) "http://lesswrong.com/lw/hmw/robust_cooperation_in_the_prisoners_dilemma/" "Robust Cooperation in the Prisoner's Dilemma" nil "Fri, 07 Jun 2013 18:30:25 +1000" "Submitted by <a href=\"http://lesswrong.com/user/orthonormal\">orthonormal</a>
#
64 votes
#
<a href=\"http://lesswrong.com/lw/hmw/robust_cooperation_in_the_prisoners_dilemma/#comments\">113 comments</a>
<div><p>I'm proud to announce the preprint of <a href=\"http://intelligence.org/files/RobustCooperation.pdf\" target=\"_blank\">Robust Cooperation in the Prisoner's Dilemma: Program Equilibrium via Provability Logic</a>, a joint paper with Patrick LaVictoire (me), Mihaly Barasz, Paul Christiano, Benja Fallenstein, Marcello Herreshoff, and Eliezer Yudkowsky.</p>
<p>This paper was one of three projects to come out of the <a href=\"http://intelligence.org/2013/03/07/upcoming-miri-research-workshops/\">2nd MIRI Workshop on Probability and Reflection</a> in April 2013, and had its genesis in ideas about formalizations of decision theory that have appeared on LessWrong. (At the end of this post, I'll include links for further reading.)</p>
<p>Below, I'll briefly outline the problem we considered, the results we proved, and the (many) open questions that remain. Thanks in advance for your thoughts and suggestions!</p>
<h2>Background: Writing programs to play the PD with source code swap</h2>
<p>(If you're not familiar with the Prisoner's Dilemma, <a href=\"http://wiki.lesswrong.com/wiki/Prisoner's_dilemma\">see here.</a>)</p>
<p>The paper concerns the following setup, <a href=\"/r/all/lw/duv/ai_cooperation_is_already_studied_in_academia_as/\">which has come up in academic research on game theory</a>: say that you have the chance to write a computer program <strong>X</strong>, which takes in one input and returns either <em>Cooperate</em> or <em>Defect</em>. This program will face off against some other computer program <strong>Y</strong>, but with a twist: <strong>X</strong> will receive the source code of <strong>Y</strong> as input, and <strong>Y</strong> will receive the source code of <strong>X</strong> as input. And you will be given your program's winnings, so you should think carefully about what sort of program you'd write!</p>
<p>Of course, you could simply write a program that defects regardless of its input; we call this program <strong>DefectBot</strong>, and call the program that cooperates on all inputs <strong>CooperateBot</strong>. But with the wealth of information afforded by the setup, you might wonder if there's some program that might be able to achieve mutual cooperation in situations where <strong>DefectBot</strong> achieves mutual defection, without thereby risking a sucker's payoff. (Douglas Hofstadter would call this a perfect opportunity for <a href=\"http://www.gwern.net/docs/1985-hofstadter\">superrationality</a>...)</p>
<h2>Previously known: CliqueBot and FairBot</h2>
<p>And indeed, there's a way to do this that's been known since at least the 1980s. You can write <a href=\"http://en.wikipedia.org/wiki/Quine_(computing)\">a computer program that knows its own source code</a>, compares it to the input, and returns <em>C</em> if and only if the two are identical (and <em>D</em> otherwise). Thus it achieves mutual cooperation in one important case where it intuitively ought to: when playing against itself! We call this program <strong>CliqueBot</strong>, since it cooperates only with the \"clique\" of agents identical to itself.</p>
<p>There's one particularly irksome issue with <strong>CliqueBot</strong>, and that's the fragility of its cooperation. If two people write functionally analogous but syntactically different versions of it, those programs will defect against one another! This problem can be patched somewhat, but not fully fixed. Moreover, mutual cooperation might be the best strategy against some agents that are not even functionally identical, and extending this approach requires you to explicitly delineate the list of programs that you're willing to cooperate with. Is there a more flexible and robust kind of program you could write instead?</p>
<p>As it turns out, there is: <a href=\"/lw/2ip/ai_cooperation_in_practice/\">in a 2010 post on LessWrong</a>, cousin_it introduced an algorithm that we now call <strong>FairBot</strong>. Given the source code of <strong>Y</strong>, <strong>FairBot</strong> searches for a proof (of less than some large fixed length) that <strong>Y</strong> returns <em>C</em> when given the source code of <strong>FairBot</strong>, and then returns <em>C</em> if and only if it discovers such a proof (otherwise it returns <em>D</em>). Clearly, if our proof system is consistent, <strong>FairBot</strong> only cooperates when that cooperation will be mutual. But the really fascinating thing is what happens when you play two versions of <strong>FairBot</strong> against each other. Intuitively, it seems that <em>either</em> mutual cooperation or mutual defection would be stable outcomes, but it turns out that if their limits on proof lengths are sufficiently high, they will achieve mutual cooperation!</p>
<p>The proof that they mutually cooperate follows from a bounded version of <a href=\"http://en.wikipedia.org/wiki/L%C3%B6b's_theorem\">LÃ¶b's Theorem</a>Â from mathematical logic. (If you're not familiar with this result, you might enjoy <a href=\"/lw/t6/the_cartoon_guide_to_l%C3%B6bs_theorem/\">Eliezer's Cartoon Guide to LÃ¶b's Theorem</a>, which is a correct formal proof written in much more intuitive notation.) Essentially, the asymmetry comes from the fact that both programs are searching for the same outcome, so that a short proof that one of them cooperates leads to a short proof that the other cooperates, and vice versa. (The opposite is not true, because <a href=\"/lw/t8/you_provably_cant_trust_yourself/\">the formal system can't know it won't find a contradiction</a>. This is a subtle but essential feature of mathematical logic!)</p>
<h2>Generalization: Modal Agents</h2>
<p>Unfortunately, <strong>FairBot</strong> isn't what I'd consider an ideal program to write: it happily cooperates with <strong>CooperateBot</strong>, when it could do better by defecting. ThisÂ is problematic because in real life, the world isn't separated into agents and non-agents, and any natural phenomenon that doesn't predict your actions can be thought of as aÂ <strong>CooperateBot</strong>Â (or aÂ <strong>DefectBot</strong>). You don't want your agent to be making concessions to rocks that happened not to fall on them. (There's an important caveat: some things have utility functions that you care about, but don't have sufficient ability to predicate their actions on yours. In that case, though, itÂ <a href=\"/lw/tn/the_true_prisoners_dilemma/\">wouldn't be a true Prisoner's Dilemma</a>Â if your values actually prefer the outcome (<em>C</em>,<em>C</em>) to (<em>D</em>,<em>C</em>).)</p>
<p>However, <strong>FairBot</strong> belongs to a promising class of algorithms: those that decide on their action by looking for short proofs of logical statements that concern their opponent's actions. In fact, there's a really convenient mathematical structure that's analogous to the class of such algorithms: the <a href=\"http://plato.stanford.edu/entries/logic-provability/\">modal logic of provability</a> (known as GL, for GÃ¶del-LÃ¶b).</p>
<p>So that's the subject of this preprint: <strong>what can we achieve in decision theory by considering agents defined by formulas of provability logic?</strong><a id=\"more\"></a></p>
<p>More formally <em>(skip the next two paragraphs if you're willing to trust me)</em>, we inductively define the class of \"modal agents\" as formulas using propositional variables and <a href=\"http://en.wikipedia.org/wiki/Logical_connective\">logical connectives</a> and the modal operatorÂ <img src=\"http://www.codecogs.com/png.latex?%5CBox\" alt=\"\">Â <span style=\"font-family: sans-serif; font-size: 13px; line-height: 19.1875px;\">(which represents provability in some base-level formal system like Peano Arithmetic), of the formÂ <img src=\"http://www.codecogs.com/png.latex?P%5Cleftrightarrow%20%5Cvarphi(P,Q,R_1,%5Cdots,R_N)\" alt=\"\">, whereÂ <img src=\"http://www.codecogs.com/png.latex?%5Cvarphi\" alt=\"\" height=\"15\" width=\"12\">Â is fully modalized (i.e. all instances of variables are contained in an expressionÂ <img src=\"http://www.codecogs.com/png.latex?%5CBox%5Cpsi\" alt=\"\">), and with eachÂ <img src=\"http://www.codecogs.com/png.latex?R_i\" alt=\"\" height=\"16\" width=\"18\">Â corresponding to a fixed modal agent of lower rank. For example, <strong>FairBot</strong> is represented by the modal formulaÂ <img src=\"http://www.codecogs.com/png.latex?P%5Cleftrightarrow%20%5CBox%20Q\" alt=\"\">.</span></p>
<p><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19.1875px;\">When two modal agents play against each other, the outcome is given by the unique fixed point of the system of modal statements, where the variables are identified with each other so thatÂ <img src=\"http://www.codecogs.com/png.latex?P\" alt=\"\" height=\"13\" width=\"14\">Â represents the expressionÂ <img src=\"http://www.codecogs.com/png.latex?X(Y)=C\" alt=\"\" height=\"19\" width=\"83\">,Â <img src=\"http://www.codecogs.com/png.latex?Q\" alt=\"\" height=\"18\" width=\"15\">Â representsÂ <img src=\"http://www.codecogs.com/png.latex?Y(X)=C\" alt=\"\" height=\"19\" width=\"83\">, and theÂ <img src=\"http://www.codecogs.com/png.latex?R_i\" alt=\"\" height=\"16\" width=\"18\">Â represent the actions of lower-rank modal agents againstÂ <img src=\"http://www.codecogs.com/png.latex?Y\" alt=\"\" height=\"13\" width=\"14\">Â and vice-versa. (Modal rank is defined as a natural number, so this always bottoms out in a finite number of modal statements; also, we interpret outcomes as statements of provability in Peano Arithmetic, evaluated in the model where PA is consistent, PA+Con(PA) is consistent, and so on. See the paper for the actual details.)</span></p>
<p><span style=\"font-family: sans-serif; font-size: 13px; line-height: 19.1875px;\">The nice part about modal agents is that there are <a href=\"http://en.wikipedia.org/wiki/Kripke_semantics\">simple tools</a> for finding the fixed points without having to search through proofs; in fact, Mihaly and Marcello wrote up a computer program to deduce the outcome of the source-code-swap Prisoner's Dilemma between any two (reasonably simple) modal agents. These tools also made it much easier to prove general theorems about such agents.</span></p>
<h2>PrudentBot: The best of both worlds?</h2>
<p>Can we find a modal agent that seems to improve on <strong>FairBot</strong>? In particular, we should want at least the following properties:</p>
<ul>
<li>It should be un-exploitable: if our axioms are consistent in the first place, then it had better only end up cooperating when it's mutual.</li>
<li>It should cooperate with itself, and also mutually cooperate with <strong>FairBot</strong> (both are, common-sensically, the best actions in those cases).</li>
<li>It should defect, however, against <strong>CooperateBot</strong> and lots of similarly exploitable modal agents.</li>
</ul>
<p>It's nontrivial that such an agent exists: you may remember the post I wrote aboutÂ <a href=\"/lw/ebx/decision_theories_part_375_hang_on_i_think_this/\">the Masquerade agent</a>, which is a modal agent that does <em>almost</em> all of those things (it doesn't cooperate with the original <strong>FairBot</strong>, though it does cooperate with some more complicated variants), and indeed we didn't find anything better until after we had Mihaly and Marcello's modal-agent-evaluator to help us.</p>
<p>But as it turns out, there is such an agent, and it's pretty elegant: we call it <strong>PrudentBot</strong>, and its modal version cooperates with another agent <strong>Y</strong> if and only if (there's a proof in Peano Arithmetic that <strong>Y</strong> cooperates with <strong>PrudentBot</strong> and there's a proof in PA+Con(PA) that <strong>Y</strong> defects against <strong>DefectBot</strong>). This agent can be seen to satisfy all of our criteria. But is it <em>optimal</em> among modal agents, by any reasonable criterion?</p>
<h2>Results: Obstacles to Optimality</h2>
<p>It turns out that, even within the class of modal agents, it's hard to formulate a definition of optimality that's actually true of something, and which meaningfully corresponds to our intuitions about the \"right\" decisions on decision-theoretic problems. (This intuition is not formally defined, so I'm using scare quotes.)</p>
<p>There are agents that give preferential treatment to <strong>DefectBot</strong>, <strong>FairBot</strong>, or even <strong>CooperateBot</strong>, compared to <strong>PrudentBot</strong>, though these agents are not ones you'd program in an attempt to win at the Prisoner's Dilemma. (For instance, one agent that rewards <strong>CooperateBotÂ </strong>over <strong>PrudentBot</strong> is the agent that cooperates with <strong>Y</strong> iff PA proves that <strong>Y</strong> cooperates against <strong>DefectBot</strong>; we've taken to jokingly calling that agent <strong>TrollBot</strong>.) One might well suppose that a modal agent could still be optimal in the sense of making the \"right\" decision in every case, regardless of whether it's being punished for some other decision. However, this is not the only obstacle to a useful concept of optimality.</p>
<p>The second obstacle is that any modal agent only checks proofs at some finite number of levels on the hierarchy of formal systems, and agents that appear indistinguishable at all those levels may have obviously different \"right\" decisions. And thirdly, an agent might mimic another agent in such a way that the \"right\" decision is to treat the mimic differently from the agent it imitates, but in some cases one can prove that no modal agent can treat the two differently.</p>
<p>These three strikes appear to indicate that if we're looking to formalize <a href=\"http://wiki.lesswrong.com/wiki/Decision_theory\">more advanced decision theories</a>, modal agents are too restrictive of a class to work with. We might instead allow things like quantifiers over agents, which would invalidate these specific obstacles, but may well introduce new ones (and certainly would make for more complicated proofs). But for a \"good enough\" algorithm on the original problem (assuming that the computer will have lots of computational resources), one could definitely do worse than submit a finite version of <strong>PrudentBot</strong>.</p>
<h2>Why is this awesome, and what's next?</h2>
<p>In my opinion, the result of LÃ¶bian cooperation deserves to be published for its illustration of Hofstadterian superrationality in action, apart from anything else! It's <em>really cool</em> that two agents reasoning about each other can in theory come to mutual cooperation for genuine reasons that don't have to involve being clones of each other (or other anthropic dodges). It's a far cry from a practical approach, of course, but it's a start: mathematicians always begin with a simplified and artificial model to see what happens, then add complications one at a time.</p>
<p>As for what's next: First, we don't <em>actually</em> know that there's no meaningful non-vacuous concept of optimality for modal agents; it would be nice to know that one way or another. Secondly, we'd like to see if some other class of agents contains a simple example with really nice properties (the way that classical game theory doesn't always have a pure Nash equilibrium, but always has a mixed one). Thirdly, we might hope that there's an actual implementation of a decision theory (<a href=\"http://wiki.lesswrong.com/wiki/Timeless_decision_theory\">TDT</a>, <a href=\"http://wiki.lesswrong.com/wiki/Updateless_decision_theory\">UDT</a>, etc) in the context of program equilibrium.</p>
<p>If we succeed in the positive direction on any of those, we'd next want to extend them in several important ways: using probabilistic information rather than certainty, considering more general games than the Prisoner's Dilemma (bargaining games have many further challenges, and games of more than two players could be more convoluted still), etc. I personally hope to work on such topics in future MIRI workshops.</p>
<h2>Further Reading on LessWrong</h2>
<p>Here are some LessWrong posts that have tackled similar material to the preprint:</p>
<ul>
<li><a href=\"/lw/2ip/ai_cooperation_in_practice/\">AI cooperation in practice</a>, cousin_it, 2010</li>
<li><a href=\"/lw/2tq/notion_of_preference_in_ambient_control/\">Notion of Preference in Ambient Control</a>, Vladimir_Nesov, 2010</li>
<li><a href=\"/lw/8wc/a_model_of_udt_with_a_halting_oracle/\">A model of UDT with a halting oracle</a>, cousin_it, 2011</li>
<li><a href=\"/lw/9o7/formulas_of_arithmetic_that_behave_like_decision/\">Formulas of arithmetic that behave like decision agents</a>, Nisan, 2012</li>
<li><a href=\"/lw/b0e/a_model_of_udt_without_proof_limits/\">A model of UDT without proof limits</a>, <a href=\"/lw/b5t/an_example_of_selffulfilling_spurious_proofs_in/\">An example of self-fulfilling spurious proofs in UDT</a>, <a href=\"/lw/crx/loebian_cooperation_version_2/\">LÃ¶bian cooperation, version 2</a>, <a href=\"/lw/dba/bounded_versions_of_g%C3%B6dels_and_l%C3%B6bs_theorems/\">Bounded versions of GÃ¶del's and LÃ¶b's theorems</a>, cousin_it, 2012</li>
<li><a href=\"/lw/ap3/predictability_of_decisions_and_the_diagonal/\">Predictability of decisions and the diagonal method</a>, <a href=\"/lw/ca5/consequentialist_formal_systems/\">Consequentialist formal systems</a>, Vladimir_Nesov, 2012</li>
<li>Decision Theories: A Semi-Formal Analysis: <a href=\"/lw/aq9/decision_theories_a_less_wrong_primer/\">Part 0 (A LessWrong Primer)</a>, <a href=\"/lw/axl/decision_theories_a_semiformal_analysis_part_i\">Part 1 (The Problem with Naive Decision Theory)</a>, <a href=\"/lw/az6/decision_theories_a_semiformal_analysis_part_ii/\">Part 2 (Causal Decision Theory and Substitution)</a>, <a href=\"/lw/b7w/decision_theories_a_semiformal_analysis_part_iii/\">Part 3 (Formalizing Timeless Decision Theory)</a>, <a href=\"/lw/e94/decision_theories_part_35_halt_melt_and_catch_fire/\">Part 3.5 (Halt, Melt, and Catch Fire)</a>, <a href=\"/lw/ebx/decision_theories_part_375_hang_on_i_think_this/\">Part 3.75 (Hang On, I Think This Works After All)</a>, orthonormal, 2012</li>
</ul></div>
<a href=\"http://lesswrong.com/lw/hmw/robust_cooperation_in_the_prisoners_dilemma/#comments\">113 comments</a>
" nil nil "09b1da5ec7b95f8e24975cf5068a296f") (8 (20946 49832 957907) "http://lesswrong.com/lw/hmx/prisoners_dilemma_with_visible_source_code/" "Prisoner's Dilemma (with visible source code) Tournament" nil "Fri, 07 Jun 2013 18:30:20 +1000" "Submitted by <a href=\"http://lesswrong.com/user/AlexMennen\">AlexMennen</a>
#
47 votes
#
<a href=\"http://lesswrong.com/lw/hmx/prisoners_dilemma_with_visible_source_code/#comments\">228 comments</a>
<div><p style=\"margin-bottom: 0in\">After the <a href=\"/lw/7f2/prisoners_dilemma_tournament_results/\">iterated prisoner's dilemma tournament</a>Â organized by prase two years ago, there was discussion of running tournaments for several variants, including one in which two players submit programs, each of which are given the source code of the other player's program, and outputs either âcooperateâ or âdefectâ. However, as far as I know, no such tournament has been run until now.</p>
<p style=\"margin-bottom: 0in\">Here's how it's going to work: Each player will submit a file containing a single Scheme lambda-function. The function should take one input. Your program will play exactly one round against each other program submitted (not including itself). In each round, two programs will be run, each given the source code of the other as input, and will be expected to return either of the symbols âCâ or âDâ (for \"cooperate\" and \"defect\", respectively). The programs will receive points based on the following payoff matrix:</p>
<p style=\"margin-bottom: 0in\"><img src=\"http://www.codecogs.com/png.latex?%5Cbegin%7Barray%7D%7Bcccc%7D%20&%20C%20&%20D%20&%20other%5C%5C%20C%20&%20(2,%5C,2)%20&%20(0,%5C,3)%20&%20(0,%5C,2)%5C%5C%20D%20&%20(3,%5C,0)%20&%20(1,%5C,1)%20&%20(1,%5C,0)%5C%5C%20other%20&%20(2,%5C,0)%20&%20(0,%5C,1)%20&%20(0,%5C,0)%20%5Cend%7Barray%7D\" alt=\"\" height=\"84\" width=\"215\"></p>
<p style=\"margin-bottom: 0in\">âOtherâ includes any result other than returning âCâ or âDâ, including failing to terminate, throwing an exception, and even returning the string âCooperateâ. Notice that âOtherâ results in a worst-of-both-worlds scenario where you get the same payoff as you would have if you cooperated, but the other player gets the same payoff as if you had defected. This is an attempt to ensure that no one ever has incentive for their program to fail to run properly, or to trick another program into doing so.</p>
<p style=\"margin-bottom: 0in\">Your score is the sum of the number of points you earn in each round. The player with the highest score wins the tournament. <strong>Edit: There is a <a href=\"/lw/hmx/prisoners_dilemma_with_visible_source_code/94no\">0.5 bitcoin prize</a> being offered for the winner. Thanks, VincentYu!</strong></p>
<p style=\"margin-bottom: 0in\">Details:<br>All submissions must be emailed to <a href=\"mailto:wardenPD@gmail.com\">wardenPD@gmail.com</a> by July 5, at noon PDT. Your email should also say how you would like to be identified when I announce the tournament results.<br>Each program will be allowed to run for 10 seconds. If it has not returned either âCâ or âDâ by then, it will be stopped, and treated as returning âOtherâ. For consistency, I will have Scheme collect garbage right before each run.<br>One submission per person or team. No person may contribute to more than one entry. <strong>Edit: This also means no copying from each others' source code. Describing the behavior of your program to others is okay.</strong><br>I will be running the submissions in Racket. You may be interested in how Racket handles <a href=\"http://docs.racket-lang.org/reference/time.html\">time</a>Â (especially the (current-milliseconds) function), <a href=\"http://docs.racket-lang.org/reference/threads.html\">threads</a>Â (in particular, âthreadâ, âkill-threadâ, âsleepâ, and âthread-dead?â), and possibly <a href=\"http://docs.racket-lang.org/reference/generic-numbers.html#%28def._%28%28quote._~23~25kernel%29._random%29%29\">randomness</a>.<br>Don't try to open the file you wrote your program in (or any other file, for that matter). I'll add code to the file before running it, so if you want your program to use a copy of your source code, you will need to use a quine. <strong>Edit: No I/O of any sort.</strong><br>Unless you tell me otherwise, I assume I have permission to publish your code after the contest.<br>You are encouraged to discuss strategies for achieving mutual cooperation in the comments thread.<br>I'm hoping to get as many entries as possible. If you know someone who might be interested in this, please tell them.<br>It's possible that I've said something stupid that I'll have to change or clarify, so you might want to come back to this page again occasionally to look for changes to the rules. Any edits will be bolded, and I'll try not to change anything too drastically, or make any edits late in the contest.</p>
<p style=\"margin-bottom: 0in\">Here is an example of a correct entry, which cooperates with you if and only if you would cooperate with a program that always cooperates (actually, if and only if you would cooperate with one particular program that always cooperates):</p>
<blockquote>
<p style=\"margin-bottom: 0in;\">(lambda (x)<br>Â  Â  (if (eq? ((eval x) '(lambda (y) 'C)) 'C)<br>Â  Â  Â  Â  'C<br>Â  Â  Â  Â  'D))</p>
</blockquote></div>
<a href=\"http://lesswrong.com/lw/hmx/prisoners_dilemma_with_visible_source_code/#comments\">228 comments</a>
" nil nil "f95ddeb944e26373b9c96333f2f58001") (7 (20946 49832 956971) "http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/" "Tiling Agents for Self-Modifying AI (OPFAI #2)" nil "Fri, 07 Jun 2013 06:24:25 +1000" "Submitted by <a href=\"http://lesswrong.com/user/Eliezer_Yudkowsky\">Eliezer_Yudkowsky</a>
#
51 votes
#
<a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/#comments\">231 comments</a>
<div><p>An early draft of publication #2 in the Open Problems in Friendly AI series is now available: Â <a href=\"http://intelligence.org/files/TilingAgents.pdf\">Tiling Agents for Self-Modifying AI, and the Lobian Obstacle</a>.Â  ~20,000 words, aimed at mathematicians or the highly mathematically literate.Â  The research reported on was conducted by Yudkowsky and Herreshoff, substantially refined at the November 2012 MIRI Workshop with Mihaly Barasz and Paul Christiano, and refined further at the April 2013 MIRI Workshop.</p>
<p style=\"padding-left: 60px;\"><strong>Abstract:</strong></p>
<p style=\"padding-left: 30px;\">We model self-modication in AI by introducing 'tiling' agents whose decision systems will approve the construction of highly similar agents, creating a repeating pattern (including similarity of the offspring's goals). Â Constructing a formalism in the most straightforward way produces a Godelian difficulty, the Lobian obstacle. Â By technical methods we demonstrate the possibility of avoiding this obstacle, but the underlying puzzles of rational coherence are thus only partially addressed. Â We extend the formalism to partially unknown deterministic environments, and show a very crude extension to probabilistic environments and expected utility; but the problem of finding a fundamental decision criterion for self-modifying probabilistic agents remains open.</p>
<p>Commenting here is the preferred venue for discussion of the paper. Â This is an early draft and has not been reviewed, so it may contain mathematical errors, and reporting of these will be much appreciated.</p>
<p>The overall agenda of the paper is introduce the conceptual notion of a self-reproducing decision pattern which includes reproduction of the goal or utility function, by exposing a particular possible problem with a tiling logical decision pattern and coming up with some partial technical solutions. Â This then makes it conceptually much clearer to point out the even deeper problems with \"We can't yet describe a probabilistic way to do this because of non-monotonicity\" and \"We don't have a good bounded way to do this because maximization is impossible, satisficing is too weak and Schmidhuber's swapping criterion is underspecified.\" Â The paper uses first-order logic (FOL) because FOL has a lot of useful standard machinery for reflection which we can then invoke; in real life, FOL is of course a poor representational fit to most real-world environments outside a human-constructed computer chip with thermodynamically expensive crisp variable states.</p>
<p>As further background, the idea that something-like-proof might be relevant to Friendly AI is not about achieving some chimera of absolute safety-feeling, but rather about the idea that the total probability of catastrophic failure should not have a significant conditionally independent component on each self-modification, and that self-modification will (at least in initial stages) take place within the highly deterministic environment of a computer chip. Â This means that statistical testing methods (e.g. an evolutionary algorithm's evaluation of average fitness on a set of test problems) are not suitable for self-modifications which can potentially induce catastrophic failure (e.g. of parts of code that can affect the representation or interpretation of the goals). Â Mathematical proofs have the property that they are as strong as their axioms and have no significant conditionally independent per-step failure probability if their axioms are semantically true, which suggests that something like mathematical reasoning may be appropriate for certain particular types of self-modification during some developmental stages.</p>
<p>Thus the content of the paper is very far off from how a realistic AI would work, but conversely, if you can't even answer the kinds of simple problems posed within the paper (both those we partially solve and those we only pose) then you must be very far off from being able to build a stable self-modifying AI. Â Being able to say how to build a theoretical device that would play perfect chess given infinite computing power, is very far off from the ability to build Deep Blue. Â However, if you can't even say how to play perfect chess given infinite computing power, you are confused about the rules of the chess or the structure of chess-playing computation in a way that would make it entirelyÂ hopeless for you to figure out how to build a bounded chess-player. Â Thus \"In real life we're always bounded\" is no excuse for not being able to solve the much simpler unbounded form of the problem, and being able to describe the infinite chess-player would be substantial and useful conceptual progress compared to <em>not </em>being able to do that. Â We can't be absolutely certain that an analogous situation holds between solving the challenges posed in the paper, and realistic self-modifying AIs with stable goal systems, but every line of investigation has to start somewhere.</p>
<p>Parts of the paper will be easier to understand if you've read <a href=\"http://wiki.lesswrong.com/wiki/Highly_Advanced_Epistemology_101_for_Beginners\">Highly Advanced Epistemology 101 For Beginners</a> including the parts on correspondence theories of truth (relevant to section 6) and model-theoretic semantics of logic (relevant to 3, 4, and 6), and there are footnotes intended to make the paper somewhat more accessible than usual, but the paper is still essentially aimed at mathematically sophisticated readers.</p></div>
<a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/#comments\">231 comments</a>
" nil nil "40f90c3c54cb58944c05b6e8ddaacb44") (6 (20946 49832 956088) "http://lesswrong.com/lw/hlk/rationality_quotes_june_2013/" "Rationality Quotes June 2013" nil "Mon, 03 Jun 2013 13:08:50 +1000" "Submitted by <a href=\"http://lesswrong.com/user/Thomas\">Thomas</a>
#
3 votes
#
<a href=\"http://lesswrong.com/lw/hlk/rationality_quotes_june_2013/#comments\">751 comments</a>
<div><p>Another month has passed and here is a new rationality quotes thread. The usual rules are:</p>
<ul>
<li>Please post all quotes separately, so that they can be upvoted or downvoted separately. (If they are strongly related, reply to your own comments. If strongly ordered, then go ahead and post them together.)</li>
<li>Do not quote yourself.</li>
<li>Do not quote from Less Wrong itself, Overcoming Bias, or HPMoR.</li>
<li>No more than 5 quotes per person per monthly thread, please.</li>
</ul></div>
<a href=\"http://lesswrong.com/lw/hlk/rationality_quotes_june_2013/#comments\">751 comments</a>
" nil nil "e5c1c30d2e5b23006b9f0cceca2045f1") (5 (20946 49832 955038) "http://lesswrong.com/lw/hjn/earning_to_give_vs_altruistic_career_choice/" "Earning to Give vs. Altruistic Career Choice Revisited" nil "Sun, 02 Jun 2013 12:55:23 +1000" "Submitted by <a href=\"http://lesswrong.com/user/JonahSinick\">JonahSinick</a>
#
31 votes
#
<a href=\"http://lesswrong.com/lw/hjn/earning_to_give_vs_altruistic_career_choice/#comments\">134 comments</a>
<div><p class=\"MsoNormal\">A commonly voiced sentiment in the effective altruist community is that the best way to do the most good is generally to make as much money as possible, with a view toward donating to the most cost-effective charities. This is often referred to as âearning to give.â In the article <a href=\"http://qz.com/57254/to-save-the-world-dont-get-a-job-at-a-charity-go-work-on-wall-street/\">To save the world, donât get a job at a charity; go work on Wall Street</a> William MacAskill wrote:</p>
<p style=\"padding-left: 30px;\" class=\"MsoNormal\"><em style=\"mso-bidi-font-style:normal\">Top undergraduates who want to âmake a differenceâ are encouraged to forgo the allure of Wall Street and work in the charity sector ...Â </em><em style=\"mso-bidi-font-style:normal\">while researching ethical career choice, I concluded that itâs in fact better to earn a lot of money and donate a good chunk of it to the most cost-effective charities, a path that I call âearning to give.â </em><span style=\"mso-bidi-font-style:normal\">...Â </span><em style=\"mso-bidi-font-style:normal\">In general, the charitable sector is people-rich but money-poor. Adding another person to the labor pool just isnât as valuable as providing more money, so that more workers can be hired.</em></p>
<p class=\"MsoNormal\">In private correspondence, MacAskill clarified that he wasnât arguing that âearning to giveâ is the <em style=\"mso-bidi-font-style: normal\">best</em> way to do good, only that itâs often better than working at a given nonprofit. <span style=\"mso-spacerun:yes\">Â </span>In <a href=\"https://www.facebook.com/jefftk/posts/613456690752?comment_id=713258\">a recent comment</a> MacAskill wrote</p>
<p style=\"padding-left: 30px;\" class=\"MsoNormal\"><em style=\"mso-bidi-font-style:normal\">I think there's too much emphasis on âearning to giveâ as the *best* option rather than as the *baseline* option</em>Â </p>
<p class=\"MsoNormal\">and raises a number of counter-considerations againstÂ âearning to give.<em style=\"mso-bidi-font-style:normal\">â</em>Â Despite this, the idea that âearning to giveâ is optimal has caught on in the effective altruist community, and so itâs important to discuss it.</p>
<p class=\"MsoNormal\">Over the past three years, I myself have shifted from the position thatÂ <em style=\"mso-bidi-font-style:normal\">â</em>earning to give<em style=\"mso-bidi-font-style:normal\">â</em>Â is philanthropically optimal, to the position that <strong style=\"mso-bidi-font-weight:normal\">itâs generally the case that one can do more good by choosing a career with high direct social value than by choosing a lucrative career with a view toward donating as much as possible</strong>.Â </p>
<p class=\"MsoNormal\">In this post Iâll outline some arguments in favor of this view.<a id=\"more\"></a></p>
<p class=\"MsoNormal\"><strong style=\"mso-bidi-font-weight:normal\"><span style=\"font-size:14.0pt;mso-bidi-font-size:12.0pt\">Responses to MacAskillâs Considerations</span></strong></p>
<p class=\"MsoNormal\">In the article <a href=\"http://qz.com/57254/to-save-the-world-dont-get-a-job-at-a-charity-go-work-on-wall-street/\">To save the world, donât get a job at a charity; go work on Wall Street</a>, MacAskill gives three considerations in favor of âearning to give.â I respond to these considerations below. What I write should be read as a response to the article, rather than to MacAskillâs views.Â </p>
<p class=\"MsoNormal\"><strong style=\"mso-bidi-font-weight:normal\">Variance in cost-effectiveness of charities</strong></p>
<p class=\"MsoNormal\">MacAskill wrote</p>
<p style=\"padding-left: 30px;\" class=\"MsoNormal\"><em style=\"mso-bidi-font-style:normal\">â¦ charities vary tremendously in the amount of good they do with the money they receive. For example, it costs about $40,000 to train and provide a guide dog for one person, but it costs less than $25 to cure one person of sight-destroying trachoma. For the cost of improving the life of one person with blindness, you can cure 1,000 people of itâ¦itâs unlikely that you can work for only the very best charities. In contrast, if you earn to give, you can donate anywhere, preferably to the most cost-effective charities, and change your donations as often as you like.</em></p>
<p class=\"MsoNormal\">GiveWell has spent about five years looking for the best giving opportunities in global health, and its current #1 ranked charity is <a href=\"http://www.givewell.org/international/top-charities/AMF\">Against Malaria Foundation</a> (AMF). GiveWell estimates that AMF <a href=\"http://www.givewell.org/international/top-charities/AMF#Costperlifesaved\">saves an infantâs life for ~ $2,300</a>, not counting other benefits. These other benefits not withstanding, AMFâs cost per <a href=\"http://www.givewell.org/international/technical/additional/DALY\">DALY saved</a> is much higher than the implied cost per DALY saved associated with the figure cited for curing sight-destroying trachoma.</p>
<p class=\"MsoNormal\">GiveWell may have missed giving opportunities in global health that are much more cost-effective than AMF is, but given the amount of time, energy and attention that GiveWell spent on its search, one should have a strong prior against the possibility that one can easily find a better giving opportunity in global health. So a plausible estimate of the cost-effectiveness of donating to the best charity that delivers direct global health interventions is much lower than the above quotation suggests.</p>
<p class=\"MsoNormal\">Furthermore, the phenomenon of the <a href=\"/lw/hif/robustness_of_costeffectiveness_estimates_and/91ia\">optimizerâs curse</a> suggests that all charities with robust case for fairly high cost-effectiveness are closer in cost-effectiveness to AMF than explicit cost-effectiveness calculations indicate. This narrows the variance in cost-effectiveness amongst charities.Â </p>
<p class=\"MsoNormal\">So the advantage of being able to choose a charity to support and change at any time is smaller than the above quotation suggests.</p>
<p class=\"MsoNormal\"><strong style=\"mso-bidi-font-weight:normal\">Discrepancy in earnings</strong></p>
<p class=\"MsoNormal\">MacAskill wrote:</p>
<p style=\"padding-left: 30px;\" class=\"MsoNormal\"><em style=\"mso-bidi-font-style:normal\">Annual salaries in banking or investment start at $80,000 and grow to over $500,000 if you do well. A lifetime salary of over $10 million is typical. Careers in nonprofits start at about $40,000, and donât typically exceed $100,000, even for executive directors </em><span style=\"mso-bidi-font-style:normal\"><em>... </em></span><em style=\"mso-bidi-font-style:normal\">By entering finance and donating 50% of your lifetime earnings, you could pay for two nonprofit workers in your placeâwhile still living on double what you would have if youâd chosen that route.</em>Â </p>
<p class=\"MsoNormal\">The assumption âif you do wellâ is a very strong one. Only about 1% of Americans make ~$500k/year. There are some people who have a strong comparative advantage in finance, for whom âearning to giveâ to give may be especially compelling. But people who are able to make ~$500k/year in finance who <strong style=\"mso-bidi-font-weight:normal\">donât</strong> have a large comparative advantage in finance have <strong style=\"mso-bidi-font-weight:normal\">very strong transferable skills</strong>. Such people are significantly more capable than the average non-profit worker, and can plausibly have a bigger impact than 2 or 3 such workers by working directly on something with high social value.Â </p>
<p class=\"MsoNormal\"><strong style=\"mso-bidi-font-weight: normal\">Replaceability</strong></p>
<p class=\"MsoNormal\">MacAskill wrote:</p>
<p style=\"padding-left: 30px;\" class=\"MsoNormal\"><em><span style=\"color: #404040; background-position: initial initial; background-repeat: initial initial;\">â¦âmaking a differenceâ requires<span>Â </span></span></em><em><a style=\"-webkit-tap-highlight-color: rgba(0, 0, 0, 0);\" href=\"http://80000hours.org/blog/18-just-what-is-making-a-difference-counterfactuals-and-career-choice\"><span style=\"color: #168dd9; background-position: initial initial; background-repeat: initial initial;\">doing something that wouldnât have happened</span></a><span><span style=\"color: #404040; background-position: initial initial; background-repeat: initial initial;\">Â </span><span style=\"color: #404040; background-position: initial initial; background-repeat: initial initial;\">anywayâ¦</span></span></em><em><span style=\"color: #404040; background-position: initial initial; background-repeat: initial initial;\">The competition for not-for-profit jobs is fierce, and if someone else takes the job instead of you, he or she likely wonât be much worse at it than you would have been. So the difference you make by taking the job is only the difference between the good you would do, and the good that the other person would have done.</span></em>Â </p>
<p class=\"MsoNormal\">I would guess that there are some highly cost-effective humanitarian interventions that are sufficiently easy to implement that the implementers are easily replaceable. I could easily imagine that this is the case for vaccination efforts.Â </p>
<p class=\"MsoNormal\">But funding opportunities for these interventions can be thought of as âlow hanging fruit.â <a href=\"http://blog.givewell.org/2013/05/02/broad-market-efficiency/\">Broad market efficiency</a> suggests that such interventions will be funded. And indeed, GiveWell has found that straightforward immunization efforts <a href=\"http://blog.givewell.org/2013/03/21/trying-and-failing-to-find-more-funding-gaps-for-delivering-proven-cost-effective-interventions/\">are already largely funded</a>, to the point that GiveWell has been unable to find giving opportunities for individual donors in this area.Â </p>
<p class=\"MsoNormal\">This suggests that at the margin, <strong style=\"mso-bidi-font-weight: normal\">very high value humanitarian efforts require highly skilled and highly motivated laborers</strong>.</p>
<p class=\"MsoNormal\">High skilled laborers are a relatively small subset of laborers, so there are fewer people available to do these sorts of jobs than other jobs. Doing a hard, non-routine job well requires high motivation. <span style=\"mso-spacerun:yes\">Â </span>The collection of people who are sufficiently highly motivated to do a hard job with high social value that doesnât pay well, and who could otherwise be making much more money, largely consists of people who are trying to have a significant positive social impact.Â </p>
<p class=\"MsoNormal\">So suppose that youâre a highly skilled laborer deciding whether to âearn to giveâ or take a job with high social value that requires high skills and motivation. If you donât take the job with high social value, your counterfactual replacement is likely be one of the following:<br style=\"mso-special-character:line-break\"> </p>
<p style=\"margin-left:38.7pt;mso-add-space: auto;text-indent:-.25in;mso-list:l0 level1 lfo1\" class=\"MsoListParagraphCxSpFirst\"><span style=\"mso-fareast-font-family:Cambria;mso-fareast-theme-font:minor-latin; mso-bidi-font-family:Cambria;mso-bidi-theme-font:minor-latin\"><span style=\"mso-list:Ignore\">1.<span style='font:7.0pt \"Times New Roman\"'>Â Â Â Â  </span></span></span>Substantially less capable than you on account of having low skills, or low altruistic motivation.</p>
<p style=\"margin-left:38.7pt;mso-add-space: auto;text-indent:-.25in;mso-list:l0 level1 lfo1\" class=\"MsoListParagraphCxSpMiddle\"><span style=\"mso-fareast-font-family:Cambria;mso-fareast-theme-font:minor-latin; mso-bidi-font-family:Cambria;mso-bidi-theme-font:minor-latin\"><span style=\"mso-list:Ignore\">2.<span style='font:7.0pt \"Times New Roman\"'>Â Â Â Â  </span></span></span>A highly skilled person with high motivation, <em style=\"mso-bidi-font-style:normal\">who would be doing something else with high social value if you had taken the job, and who canât do this because they have to do the job that you would have done</em>.</p>
<p style=\"margin-left:38.7pt;mso-add-space:auto; text-indent:-.25in;mso-list:l0 level1 lfo1\" class=\"MsoListParagraphCxSpLast\"><span style=\"mso-fareast-font-family:Cambria;mso-fareast-theme-font:minor-latin; mso-bidi-font-family:Cambria;mso-bidi-theme-font:minor-latin\"><span style=\"mso-list:Ignore\">3.<span style='font:7.0pt \"Times New Roman\"'>Â Â Â Â  </span></span></span>Nonexistent.</p>
<p class=\"MsoNormal\">So the replaceability consideration carries less weight than it might seem.</p>
<p class=\"MsoNormal\">Admittedly thereâs a counterconsideration âÂ broad market efficiency cuts both ways, and one could imagine that the low hanging fruit in <em style=\"mso-bidi-font-style:normal\">working directly on projects with high social valu</em>e is also plucked, and this counter-consideration pushes in favor of âearning to give.â I have a fairly strong intuition that âif you donât fund it, somebody else willâ is more true than âif you donât do it, somebody else willâ so that this counter-consideration is outweighed. Itâs important to note that many projects of high social value are the first of their kind, and that finding somebody else to execute such a project is highly nontrivial. I think that itâs also relevant that <a href=\"http://givingpledge.org/\">114 billionaires</a> have signed the Giving Pledge, committing to giving 50+% of their wealth away in their lifetimes.<span style=\"mso-spacerun:yes\">Â </span></p>
<p class=\"MsoNormal\">In any case, there isnât a clear-cut, unconditional argument that favors âearning to giveâ: whether âearning to giveâ is the best option very much depends on nuanced empirical considerations rather than a general abstract argument.</p>
<p class=\"MsoNormal\"><strong style=\"mso-bidi-font-weight:normal\"><span style=\"font-size:14.0pt;mso-bidi-font-size:12.0pt\">Other important considerations that favor an altruistic career</span></strong></p>
<p class=\"MsoNormal\">There are additional important considerations that favor pursuing a career with high social value over âearning to giveâ:</p>
<p class=\"MsoNormal\"><strong style=\"mso-bidi-font-weight:normal\">Asymmetric implications of the existence of small probability failure modes</strong>Â </p>
<p class=\"MsoNormal\">In <a href=\"/lw/hif/robustness_of_costeffectiveness_estimates_and/\">Robustness of Cost-Effectiveness Estimates and Philanthropy</a>, I described how a large collection of small probability failure modes conspires to substantially reduce the expected value of a funding opportunity. The same issue applies to choosing a narrow career goal with a view toward directly having a high positive social impact. But <strong style=\"mso-bidi-font-weight:normal\">a worker has more capacity than a donor does to learn whether small probability failure modes prevail in practice, and can switch to a different job if he or she finds that such a failure mode prevails.</strong>Â </p>
<p class=\"MsoNormal\">Hereâs an example. Suppose that you go to medical school with a view toward the possibility of performing cleft palate surgeries in the developing world. Itâs probably the case that the opportunity isnât as promising as it seems. But if you try it, then youâll be able to see how effective the intervention is firsthand. If itâs highly effective, then you can keep doing it. If itâs not highly effective, then you can explore other possibilities, such asÂ </p>
<ul>
<li>Starting your own surgery organization.</li>
<li>Switching to doing a different kind of surgery in the developing world, such as cataract removal.</li>
<li>Working in a poor community in the developed world (which could have a bigger impact than working in the developing world owing to <a href=\"http://blog.givewell.org/2013/05/15/flow-through-effects/\">flow-through effects</a>).</li>
<li>Working for a biotech company.</li>
<li>Getting involved in clinical medical research.</li>
<li>Other things that haven't occurred to me.</li>
</ul>
<p class=\"MsoNormal\">By experimenting, one can hope to hone in on a job that has both high ostensible cost-effectiveness, and and a relatively small mass of small probability failure modes.</p>
<p class=\"MsoNormal\"><strong style=\"mso-bidi-font-weight:normal\">Altruistic careers extend beyond the nonprofit world</strong></p>
<p class=\"MsoNormal\">Even on the assumption that âearning to giveâ is better than working at a nonprofit, it doesnât follow that âearning to giveâ optimizes social impact. There are ways to have a positive social impact in the for-profit world, in scientific research, and in the government.</p>
<p class=\"MsoNormal\"><strong style=\"mso-bidi-font-weight:normal\">Historical Precedent</strong></p>
<p class=\"MsoNormal\">For the most part, the people who have had the biggest positive impact on the world havenât had their impact by âearning to give.â</p>
<p class=\"MsoNormal\">There are a few possible exceptions, such as Bill Gates and Warren Buffett, whose philanthropic activities could be having a huge impact (though itâs hard to tell from the outside) and could well outstrip the value that they contributed through their labor. But they appear to have an unusually high ratio of wealth to direct positive impact of their work, and so appear to be unrepresentative.</p>
<p class=\"MsoNormal\">Steve Jobsâ highest net worth was on the order of $10 billion, whereas Bill Gatesâ highest net worth was on the order of $100 billion. I donât think that Bill Gates contributed 10x as much as Steve Jobs to technology, and I donât think that Jobs could have had a bigger social impact by donating than through his work (which had massive positive <a href=\"http://blog.givewell.org/2013/05/15/flow-through-effects/\">flow-through effects</a>). I acknowledge that Jobs is a cherry picked example, but I think that the general principle still holds.Â </p>
<p class=\"MsoNormal\"><strong style=\"mso-bidi-font-weight:normal\">Mainstream consensus</strong></p>
<p class=\"MsoNormal\">Few people think that âearning to giveâ is the best way to make the world a better place. This could be attributable to irrationality or to low altruism, but my experience is that there are many people who care about global welfare, or just welfare within a specific cause, and many people who are highly intelligent. In light of the existence ofÂ <a href=\"https://en.wikipedia.org/wiki/Illusory_superiority\">illusory superiority</a>, one should be wary of holding an implicit view that one knows more about how to make the world a better place than the vast majority of the population.</p>
<p class=\"MsoNormal\"><strong style=\"mso-bidi-font-weight:normal\"><span style=\"font-size:14.0pt;mso-bidi-font-size:12.0pt\">Steelmanning wealth maximization</span></strong></p>
<p class=\"MsoNormal\">Itâs worth highlighting some factors that <em style=\"mso-bidi-font-style:normal\">favor</em> choosing a career with a view toward maximizing wealth in some situations:</p>
<ul>
<li><strong style=\"mso-bidi-font-weight:normal\">Comparative advantage</strong> âÂ Some people are unusually good at making money relative to doing other things. Such people may do better to âearn to giveâ than to try to choose a job that has a direct positive impact (which theyâre relatively bad at).<br></li>
<li><strong style=\"mso-bidi-font-weight:normal\">The market mechanism</strong>Â âÂ In the for-profit world, maximizing wealth is often correlated with maximizing positive social impact, and so can be used as a proxy goal for maximizing positive social impact.<br><strong style=\"mso-bidi-font-weight:normal\"><br></strong></li>
<li><span style=\"mso-bidi-font-weight:normal\"><strong>Connections and personal growth</strong></span>Â â People with high earnings are generally more capable and more knowledgeable than people in other contexts, and tend to be well connected, so positioning oneself among such people can increase oneâs prospects of soaring to greater heights. Jeff Bezos <a href=\"http://en.wikipedia.org/wiki/Jeff_Bezos#Business_career\">started his career</a> in finance, and later created Amazon, which has had massive positive social impact (both direct, and via <a href=\"http://blog.givewell.org/2013/05/15/flow-through-effects/\">flow-through effects</a>).<br><strong style=\"mso-bidi-font-weight:normal\"><br></strong></li>
<li><span style=\"mso-bidi-font-weight:normal\"><strong>Unusual values</strong></span> âÂ If one cares about causes that very few people care about, then it could be difficult to find funding for work on them, so âearning to giveâ could be necessary. I donât believe this to be the case, but itâs a consideration that's been raised by others, and so is worth mentioning.</li>
</ul>
<p class=\"MsoNormal\"><strong style=\"mso-bidi-font-weight:normal\"><span style=\"font-size:14.0pt;mso-bidi-font-size:12.0pt\">Closing summary</span></strong><strong style=\"mso-bidi-font-weight:normal\"><span style=\"font-size:14.0pt;mso-bidi-font-size:12.0pt\">Â </span></strong></p>
<p class=\"MsoNormal\"> </p>
<p class=\"MsoNormal\"><span style=\"font-size:14.0pt;mso-bidi-font-size:12.0pt\">Â­</span>There are many arguments against the claim that âearning to giveâ is generally the best way to maximize oneâs positive social impact, and I believe that choosing a job where one can do as much good as possible through oneâs work is generally the best way to maximize oneâs positive social impact. However, for some people in unusual situations, âearning to giveâ may be the best way to have a positive social impact.</p>
<p class=\"MsoNormal\"><strong>Note:<em>Â </em></strong>I formerly worked as a research analyst atÂ <a href=\"http://www.givewell.org/\">GiveWell</a>. All views expressed here are my own.</p>
<p class=\"MsoNormal\"><strong>Acknowledgements:</strong><em style=\"font-weight: bold;\">Â </em>I thank Nick Beckstead, ModusPonies and Will Crouch for helpful feedback on an earlier version of this article.</p>
<div style=\"mso-element:comment-list\">
<div style=\"mso-element:comment\">
<div id=\"_com_4\" class=\"msocomtxt\"></div>
</div>
</div></div>
<a href=\"http://lesswrong.com/lw/hjn/earning_to_give_vs_altruistic_career_choice/#comments\">134 comments</a>
" nil nil "806da8d4370c32fc11c4e714a19deb40") (4 (20946 49832 952227) "http://lesswrong.com/lw/hhy/reductionism_sequence_now_available_in_audio/" "Reductionism sequence now available in audio format" nil "Sun, 02 Jun 2013 12:55:00 +1000" "Submitted by <a href=\"http://lesswrong.com/user/Rick_from_Castify\">Rick_from_Castify</a>
#
18 votes
#
<a href=\"http://lesswrong.com/lw/hhy/reductionism_sequence_now_available_in_audio/#comments\">6 comments</a>
<div><p>The sequence \"<a href=\"http://wiki.lesswrong.com/wiki/Reductionism_(sequence)\">Reductionism</a>\", which includes the subsequences \"<a href=\"http://wiki.lesswrong.com/wiki/Joy_in_the_Merely_Real\">Joy in the Merely Real</a>\" and \"<a href=\"http://wiki.lesswrong.com/wiki/Zombies_(sequence)\">Zombies</a>\", is now available as a <a href=\"http://castify.co/channels/43-reductionism\">professionally read podcast</a>.</p>
<p>Thanks to those who've been listening, let us know how your experience has been thus far and what you think of the service by dropping an email to support@castify.co. Â </p></div>
<a href=\"http://lesswrong.com/lw/hhy/reductionism_sequence_now_available_in_audio/#comments\">6 comments</a>
" nil nil "63816beaa496a95b92549c3f6e0c9cec") (3 (20946 49832 951704) "http://lesswrong.com/lw/hiv/the_centre_for_applied_rationality_a_year_later/" "The Centre for Applied Rationality: a year later from a (somewhat) outside perspective" nil "Tue, 28 May 2013 04:31:41 +1000" "Submitted by <a href=\"http://lesswrong.com/user/Swimmer963\">Swimmer963</a>
#
39 votes
#
<a href=\"http://lesswrong.com/lw/hiv/the_centre_for_applied_rationality_a_year_later/#comments\">102 comments</a>
<div><p>I recently had the privilege of being a CFAR alumni volunteering at a later workshop, which is a fascinating thing to do, and put me in a position both to evaluate how much of a difference the first workshop actually made in my life, and to see how the workshops themselves have evolved.Â </p>
<p>Exactly a year ago, I attended one of the first <a href=\"/lw/b98/minicamps_on_rationality_and_awesomeness_may_1113/\">workshops</a>, back when they were still inexplicably called âminicampsâ. I wasn't sure what to expect, and I especially wasn't sure why I had been accepted. But I bravely bullied the nursing faculty staff until they reluctantly let me switch a day of clinical around, and laterÂ stumbled off my plane into the San Francisco airport in a haze of exhaustion. The workshop spat me out three days later, twice as exhausted, with teetering piles of ideas and very little time or energy to apply them. I left with a list of annual goals, which I had never bothered to have before, and a feeling that more was possibleâthis included the feeling that more would have been possible if the workshop had been longer and less chaotic, if I had slept more the week before, if I hadn't had to rush out on Sunday evening to catch a plane and miss the social.Â </p>
<p>Like I frequently do on Less Wrong the website, I left the minicamp feeling a bit like an outsider, but also a bit like I had come home. As well as my written goals, I made an unwrittenÂ pre-commitmentÂ to come back to San Francisco later, for longer, and see whether I could make the \"more is possible\" in my head more specific. Of my thirteen written goals on my list, I fully accomplished only four and partially accomplished five, but I did make it back to San Francisco, at the opportunity cost of four weeks of sacrificed hospital shifts.Â </p>
<p>A week or so into my stay, while I shifted around between different rationalist shared houses and attempted to max out interesting-conversations-for-day, I found out that CFAR was holding another May workshop. I offered to volunteer, proved my sincerity by spending 6 hours printing and sticking nametags, and lived on site for another 4-day weekend of delightful information overload and limited sleep.Â </p>
<p>Before the May 2012 workshop, I had a low prior that any four-day workshop could be life-changing in a major way. A four-year nursing degree, okayâI've successfully retrained my social skills and my <a href=\"/lw/4fo/ability_to_react/\">ability to react under pressure</a> by putting myself in particular situations over and over and over and over again. Four days? Nah. Brains don't work that way.Â </p>
<p>In my experience, it's exceedingly hard for the human brain to doÂ <em>anythingÂ </em>deliberately. In Kahneman-speak, habits are System 1, effortless and automatic. Doing things on purpose involves System 2, effortful and a bit aversive. I could have had aÂ <em>muchÂ </em>better experience in my <a href=\"/lw/gnv/learning_critical_thinking_a_personal_example/\">final intensive care clinical</a> if I'd though to open up my workshop notes and tried to address the causes of aversions, or use offline time to train habits, or, y'know, doÂ <em>anythingÂ </em>on purpose instead of floundering around trying things at random until they worked.Â </p>
<p>(The again, I didn't apply concepts like System 1 and System 2 to myself a year ago. I read 'Thinking Fast and Slow' by Kahneman and 'Rationality and the Reflective Mind' by Stanovich as part of my minicamp goal 'read 12 hard nonfiction books this year', most of which came from the <a href=\"http://rationality.org/recommended-reading-on-rationality/\">CFAR recommended reading list</a>. If my preceptor had had any idea what I was saying when I explained to her that she was running particular nursing skills on System 1, because they were engrained on the level of habit, and I was running the same tasks on System 2 in working memory because they were new and confusing to me, and that was why I appeared to have poor time management, because System 2 takes forever to do anything, this terminology might have helped. Oh, for the world where everyone knows all jargon!)</p>
<p>...And here I am, setting aside a month of my life to think only about rationality. I can't imagine that my counterfactual self-who-didn't-attend-in-May-2012 would be here. I can't imagine that being here now will have <em>zeroÂ </em>effect on what I'm doing in a year, or ten years. Bingo. I did one thing deliberately!</p>
<p><strong>So what was the May 2013 workshop actually like?</strong></p>
<p>The curriculum has shifted around a lot in the past year, and I think with 95% probability that it's now more concretely useful. (Speaking of probabilities, the prediction markets during the workshop seemed to flow better and be more fun and interesting this time, although this may just show that I was more averse to games in general and betting in particular. In that case, yay for partly-cured aversions!)</p>
<p>The classes are grouped in an order that allows them to build on each other usefully, and they've been honed by practice into forms that successfully teach skills, instead of just putting words in the air and on flipcharts. For example, having a personal productivity system like GTD came across as a culturally prestigious thing at the last workshop, but there wasn't a lot of useful curriculum on it. Of course, I left on this trip wanting to spend my offline month creating with a GTD system better than paper to-do lists taped to walls, so I have both motivation and a low threshold for improvement.Â </p>
<p>There are also some completely new classes, including \"Againstness training\" by <a href=\"/user/Valentine/overview/\">Valentine</a>, which seem to relate to some of the 'reacting under pressure' stuff in interesting ways, and gave me vocabulary and techniques for something I've been doing inefficiently by trial and error for a good part of my life.</p>
<p>In general, there are more classes about emotions, both how to deal with them when they're in the way and how to use them when they're the best tool available. Given that none of us are Spock, I think this is useful.Â </p>
<p>Rejection therapy has morphed into a less terrifying and more helpful form with the awesome name of CoZE (Comfort Zone Expansion). I didn't personally find the original rejection therapy all that awful, but some people did, and that problem is largely solved.Â </p>
<p>The workshops are vastly more orderly and organized. (I like to think I contributed to this slightly with my volunteer skills of keeping the fridge stocked with water bottles and calling restaurants to confirm orders and make sure food arrived on time.) Classes began and ended on time. The venue stayed tidy. The food was excellent. It was easier to get enough sleep. Etc. The May 2012 venue had a pool, and this one didn't, which made exercise harder for addicts like me. CFAR staff are talking about solving this.Â </p>
<p>The workshops still aren't an easy environment for introverts. The negative parts of my experience in May 2012 were mostly because of this. It was easier this time, because as a volunteer I could skip classes if I started to feel socially overloaded, but periods of quiet alone time had to be effortfully carved out of the day, and at an opportunity cost of missing interesting conversations. I'm not sure if this problem is solvable without either making the workshops longer, in order to space the material out, and thus less accessible for people with jobs, or by cutting out curriculum. Either would impose a cost on the extroverts who don't want an hour at lunch to meditate or go running alone or read a sci-fi book, etc.Â </p>
<p>In general, I found the May 2012 workshop too short and intenseâwe had material thrown at us at a rate far exceeding the usual human idea-digestion rate. Keeping in touch via Skype chats with other participants helped. CFAR now does official followups with participants for six weeks following the workshop.Â </p>
<p>Meeting the other participants was, as usual, the best part of the weekend. The group was quite diverse, although I was still the only health care professional there. (Whyyy???? The health care system needs more rationality <em>so </em>badly!)Â The conversations were engaging. Many of the participants seem eager to stay in touch. The May 2012 workshop has a total of six people still on the Skype chats list, which is a 75% attrition rate. CFAR is now working on strategies to help people who want to stay in touch do it successfully.Â </p>
<p><strong>Conclusions?</strong></p>
<p>I thought the May 2012 workshop was awesome. I thought the May 2013 workshop was about an order of magnitude more awesome. I would say that now is a great time to attend a CFAR workshop...except that the organization is financially stable and likely to still be around in a year and producing even <em>better </em>workshops. So I'm not sure. Then again, rationality skills have compound interestâthe value of learning some new skills now, even if they amount more to vocab words and mental labels than superpowers, compounds over the year that you spend seeing all the books you read and all the opportunities you have in that framework. I'm glad I went a year ago instead of this May. I'm even more glad I had the opportunity to see the new classes and meet the new participants a year later.Â </p>
<p><strong><br></strong></p></div>
<a href=\"http://lesswrong.com/lw/hiv/the_centre_for_applied_rationality_a_year_later/#comments\">102 comments</a>
" nil nil "52238fe1d947214901a90664d26ced68") (2 (20946 49832 949755) "http://lesswrong.com/lw/hif/robustness_of_costeffectiveness_estimates_and/" "Robustness of Cost-Effectiveness Estimates and Philanthropy" nil "Sat, 25 May 2013 06:28:43 +1000" "Submitted by <a href=\"http://lesswrong.com/user/JonahSinick\">JonahSinick</a>
#
36 votes
#
<a href=\"http://lesswrong.com/lw/hif/robustness_of_costeffectiveness_estimates_and/#comments\">37 comments</a>
<div><p> </p>
<p class=\"MsoNormal\"><strong>Note:<em>Â </em></strong>I formerly worked as a research analyst at <a href=\"http://www.givewell.org/\">GiveWell</a>. This post describes the evolution of my thinking about robustness of cost-effectiveness estimates in philanthropy. All views expressed here are my own.</p>
<p class=\"MsoNormal\">Up until 2012, I believed that detailed explicit cost-effectiveness estimates are very important in the context of philanthropy. My position was reflected in a <a href=\"http://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/comment-page-1/#comment-230938\">comment that I made</a> in 2011:Â </p>
<p style=\"margin-left:.5in\" class=\"MsoNormal\">The problem with using unquantified heuristics and intuitions is that the âtrueâ expected values of philanthropic efforts plausibly differ by many orders of magnitude, and unquantified heuristics and intuitions are frequently insensitive to this. The last order of magnitude is the only one that matters; all others are negligible by comparison. So if at all possible, one should do oneâs best to pin down the philanthropic efforts with the âtrueâ expected value per dollar of the highest (positive) order of magnitude. It seems to me as though any feasible strategy for attacking this problem involves explicit computation.</p>
<p class=\"MsoNormal\">During my time at GiveWell, my position on this matter shifted. I still believe that there are instances in which <em>rough</em> cost-effectiveness estimates can be useful for determining good philanthropic foci. But Iâve shifted toward the position that <strong>effective altruists should spend much more time on qualitative analysis than on quantitative analysis in determining how they can maximize their positive social impact</strong>.</p>
<p class=\"MsoNormal\">In this post Iâll focus on one reason for my shift: <strong>explicit cost-effectiveness estimates are generally much less robust than I had previously thought</strong>.</p>
<p class=\"MsoNormal\"><a id=\"more\"></a></p>
<p class=\"MsoNormal\"><strong><span style=\"font-size:14.0pt;mso-bidi-font-size:12.0pt\">The history of GiveWellâs estimates for lives saved per dollar</span></strong></p>
<p class=\"MsoNormal\">Historically, GiveWell used âcost per life savedâ as a measure of the cost-effectiveness of its global health recommendations. Examination of the trajectory of GiveWellâs cost-effectiveness estimates shows that <strong>GiveWell has consistently updated in the direction of its ranked charities having higher âcost per life savedâ than GiveWell had previously thought. </strong>I give the details below.</p>
<p class=\"MsoNormal\">The discussion should be read with the understanding that <strong>donating to GiveWellâs top charities has benefits that extend beyond saving lives</strong>, so that ânumber of lives savedâ understates cost-effectiveness..</p>
<p class=\"MsoNormal\">At the end of each of 2009 and 2010, GiveWell named <a href=\"http://www.givewell.org/international/charities/villagereach\">VillageReach</a> its #1 ranked charity. VillageReach <a href=\"http://www.givewell.org/international/top-charities/villagereach/December-2009-review#Pastcosteffectivenesspilotprogram\">estimated</a> the cost-per-life-saved of its pilot project as being < $200, and at the end of 2009, GiveWell gave a âconservativeâ estimate of $545/life saved. In 2011, GiveWell <a href=\"http://blog.givewell.org/2012/07/26/rethinking-villagereachs-pilot-project/\">reassessed VillageReachâs pilot project</a>, commending VillageReach for being transparent enough for reassessment to be possible, and concluding that</p>
<p style=\"margin-left:.5in\" class=\"MsoNormal\">We feel that within the framework of âdelivering proven, cost-effective interventions to improve health,â AMF and SCI are solidly better giving opportunities than VillageReach (both now and at the time when we recommended it). Given the information we have, we see less room for doubt in the cases for AMFâs and SCIâs impact than in the case for VillageReachâs.</p>
<p class=\"MsoNormal\">Here âAMFâ refers to <a href=\"http://www.givewell.org/international/top-charities/AMF\">Against Malaria Foundation</a>, which is GiveWellâs current #1 ranked charity. If AMF is currently more cost-effective than VillageReach was at the time when GiveWell recommended VillageReach, then the best cost-per-life-saved figure for GiveWellâs recommended charities is (and was) the cost-effectiveness of donating to AMF.Â </p>
<p class=\"MsoNormal\">AMF delivers long-lasting insecticide treated nets (LLINs) to the developing world to protect people against mosquitoes that spread malaria. This contrasts with VillageReach, which works to increase vaccination rates. Vaccines are thought to be more cost-effective than LLINs, and <a href=\"http://blog.givewell.org/2013/03/21/trying-and-failing-to-find-more-funding-gaps-for-delivering-proven-cost-effective-interventions/\">GiveWell has not been able to find strong giving opportunities in vaccination</a>, so the cost per life saved of the best opportunity that GiveWell has found for individual donors is correspondingly higher.Â </p>
<p class=\"MsoNormal\">At the end of 2011, GiveWell estimated that the marginal cost per life associated with donating to AMF at <a href=\"http://www.givewell.org/international/top-charities/AMF/2011-review#Costperlifesaved\">$1600/life saved</a>. During 2012, I vetted GiveWellâs page on LLINs and <a href=\"http://blog.givewell.org/2012/10/18/revisiting-the-case-for-insecticide-treated-nets-itns/\">uncovered an issue</a>, which led GiveWell to revise its estimate for AMFâs marginal cost per life saved to <a href=\"http://www.givewell.org/international/top-charities/AMF#Costperlifesaved\">$2300/life saved</a> at the end of 2012. This does not take into account <a href=\"http://www.givingwhatwecan.org/where-to-give/methodology/regression-to-the-mean\">regression to the mean</a>, which can be expected to raise the cost per life saved.Â </p>
<p class=\"MsoNormal\">The discussion above shows a consistent trend in the direction of the marginal cost per life saved in the developing world being higher than initially meets the eye. Note that the difference between VillageReachâs original estimate and GiveWellâs current estimate is about an order of magnitude.Â </p>
<p class=\"MsoNormal\"><strong><span style=\"font-size:14.0pt;mso-bidi-font-size:12.0pt\">Concrete factors that further reduce the expected value of donating to AMF</span></strong></p>
<p class=\"MsoNormal\">A key point that I had missed when I thought about these things earlier in my life is that <strong>there are many small probability failure modes which are not significant individually, but which collectively substantially reduce cost-effectiveness</strong>. When I encountered such a potential failure mode, my reaction was to think âthis is very unlikely to be an issueâ and then to forget about it. I didnât notice that I was doing this many times in a row.</p>
<p class=\"MsoNormal\">I list many relevant factors that reduce AMFâs expected cost-effectiveness below. Some of these are from GiveWellâs discussion of <a href=\"http://www.givewell.org/international/top-charities/AMF#Possiblenegativeoroffsettingimpact\">possible negative or offsetting impacts</a> in GiveWellâs review of AMF. Others are implicitly present in GiveWellâs review of AMF and<a href=\"http://www.givewell.org/international/technical/programs/insecticide-treated-nets\"> GiveWellâs review of LLINs</a>, and others are issues that have emerged in the interim. I would emphasize that <strong>I donât think that any of the points listed is a big issue</strong> and that <strong>GiveWell and AMF take precautionary efforts to guard against them</strong>. But I think that they <em>collectively</em> reduce cost-effectiveness by a substantial amount.</p>
<ul>
<li>If GiveWellâs customers werenât funding AMF, another funder might, and that funder might instead be funding much less effective activities.</li>
<li>If AMF werenât working in a given region, there might be other organizations that would deliver LLINs to that region, and these other organizations may instead be funding much less effective activities.</li>
<li>It could be that the workers who distribute the LLINs would otherwise be providing more cost-effective health care interventions.</li>
<li>The five RCTs that found that LLIN distribution reduces mortality could be systematically flawed in a non-obvious way.</li>
<li>While the Cochrane Review that contains a meta-analysis of the RCTs referred to unpublished studies so as to counteractÂ <a href=\"http://blog.givewell.org/2009/01/25/publication-bias-over-reporting-good-news/\">publication bias</a>, there may be unpublished studies that were missed, and which were not published, because they found no effect.</li>
<li>The field workers who are assigned to distribute LLINs mayÂ <a href=\"http://www.givewell.org/international/top-charities/amf/updates/March-2012#DistributioninNtcheudistrictofMalawi\">steal the nets to sell them for a profit</a>.</li>
<li>FathersÂ <a href=\"http://ugandaradionetwork.com/a/story.php?s=18197\">may steal nets from pregnant mothers</a>Â and sell them for a profit.</li>
<li>LLIN recipients mayÂ <a href=\"http://www.malariajournal.com/content/7/1/165\">use the nets for fishing</a>.</li>
<li>LLIN users may not fasten LLINs properly.</li>
<li>Mosquitoes may developÂ <a href=\"http://www.ncbi.nlm.nih.gov/pubmed/21856232\">biological resistance to the insecticide used on LLINs</a>.</li>
<li>MosquitoesÂ <a href=\"http://blog.givewell.org/2012/11/09/insecticide-resistance-and-malaria-control/\">may develop âbehavioral resistanceâ</a>Â to the insecticides used on LLINs by evolving to bite during the day (when LLINs are not used) rather than during the night.</li>
</ul>
<p class=\"MsoNormal\">Most of the relevant factors will vary by region where AMF ships nets, and some may be present in certain locations and not others.</p>
<p><strong><span style=\"font-size:14.0pt;mso-bidi-font-size:12.0pt\">Do these considerations argue against donating to AMF?</span></strong><strong><span style=\"font-size:14.0pt;mso-bidi-font-size:12.0pt\">Â </span></strong></p>
<p class=\"MsoNormal\">In view of the issues above, one might wonder whether itâs better to donate to a charity in a different cause, or better not to donate at all. Some relevant points follow:</p>
<p class=\"MsoNormal\"><strong style=\"text-indent: -0.25in;\">Donating to AMF has benefits beyond saving lives. </strong><span style=\"text-indent: -0.25in;\">The above discussion of cost-effectiveness figures concerns âcost per life savedâ specifically. But there are benefits to donating to AMF that go beyond saving lives.</span></p>
<ul>
<li><span style=\"text-indent: -0.25in;\">Malaria control reduces the morbidity of malaria. A Cochrane Review of theÂ </span><a style=\"text-indent: -0.25in;\" href=\"http://www.givewell.org/international/technical/programs/insecticide-treated-nets#Evidencefromsmallscalehighqualitystudies\">health benefits of LLINs</a><span style=\"text-indent: -0.25in;\">Â reports on reductions in anemia, enlarged spleen, and other health outcomes.<span style=\"text-indent: -0.25in;\"><br></span></span></li>
<li><span style=\"text-indent: -0.25in;\"><span style=\"text-indent: -0.25in;\">People are more productive when theyâre healthy than they are when theyâre ill.</span></span></li>
<li><span style=\"text-indent: -0.25in;\">There is some evidence that malaria controlÂ </span><a style=\"text-indent: -0.25in;\" href=\"http://www.givewell.org/international/technical/programs/insecticide-treated-nets#Possibledevelopmentaleffects\">increases childrenâs income later on in life</a><span style=\"text-indent: -0.25in;\">.</span></li>
<li><span style=\"text-indent: -0.25in;\">The above benefits could be massively leveraged viaÂ </span><a style=\"text-indent: -0.25in;\" href=\"http://blog.givewell.org/2013/05/15/flow-through-effects/\">flow-through effects</a><span style=\"text-indent: -0.25in;\">.</span></li>
</ul>
<p class=\"MsoNormal\"><strong style=\"text-indent: -0.25in;\">Updates in the direction of reduced cost-effectiveness arenât specific to global health.</strong><span style=\"text-indent: -0.25in;\"> Based on my experience at GiveWell, Iâve found that </span><em style=\"text-indent: -0.25in;\">regardless</em><span style=\"text-indent: -0.25in;\"> of the cause within which one investigates giving opportunities, thereâs a strong tendency for giving opportunities to appear progressively less promising as one learns more. AMF and LLIN distribution have stood up to scrutiny </span><em style=\"text-indent: -0.25in;\">unusually well</em><span style=\"text-indent: -0.25in;\">. It remains the case that </span><a style=\"text-indent: -0.25in;\" href=\"http://blog.givewell.org/2011/12/22/my-favorite-cause-for-individual-donors-global-health-and-nutrition/\">Global health and nutrition</a><span style=\"text-indent: -0.25in;\"> may be an unusually good cause for individual donors.</span></p>
<p class=\"MsoNormal\"><strong style=\"text-indent: -0.25in;\">Updates in the direction of reduced LLIN cost-effectiveness push in favor of cash transfers over LLINs. </strong><span style=\"text-indent: -0.25in;\">Transferring cash to people in the developing world is an unusually straightforward intervention. While there are </span><a style=\"text-indent: -0.25in;\" href=\"http://www.givewell.org/international/technical/programs/cash-transfers#Whatarethepotentialdownsidesoftheintervention\">potential downsides to transferring cash</a><span style=\"text-indent: -0.25in;\">, there seem to be fewer potential failure modes associated with it than there are potential failure modes associated with LLIN distribution. There are </span><a style=\"text-indent: -0.25in;\" href=\"http://blog.givewell.org/2012/05/30/giving-cash-versus-giving-bednets/\">strong arguments that favor LLINs over cash transfers</a><span style=\"text-indent: -0.25in;\">, but difference in straightforwardness of the interventions in juxtaposition with the phenomenon of surprisingly large updates in the direction of reduced cost-effectiveness is a countervailing consideration.</span></p>
<p class=\"MsoNormal\"><strong style=\"text-indent: -0.25in;\"><span style=\"font-size:14.0pt;mso-bidi-font-size:12.0pt\">Why do cost-effectiveness updates skew so negatively?</span></strong></p>
<p class=\"MsoNormal\">When I first started thinking seriously about philanthropy in 2009, I thought that if one has impressions of a philanthropic opportunity, one will be equally likely to update in the direction of it being better than meets the eye as one will be to update the direction of the opportunity being worse than meets the eye. So I was surprised to discover how strong the tendency is for philanthropic opportunities to look worse over time rather than better over time.</p>
<p class=\"MsoNormal\">Aside from the empirical data, something that shifted my view is Holdenâs observation that outlier cost-effectiveness estimates <a href=\"http://blog.givewell.org/2011/08/18/why-we-cant-take-expected-value-estimates-literally-even-when-theyre-unbiased/\">Â need to be regressed to oneâs Bayesian prior</a> over the values of all possible philanthropic opportunities. Another reason for my shift is GiveWell finding that <a href=\"http://blog.givewell.org/2013/05/02/broad-market-efficiency/\">philanthropic markets are more efficient than it had previously thought</a>. Â I think that <a href=\"http://en.wikipedia.org/wiki/Optimism_bias\">optimism bias</a> also plays a role.Â </p>
<p class=\"MsoNormal\">This is all consistent with GiveWellâs view that <a href=\"http://blog.givewell.org/2011/06/11/why-we-should-expect-good-giving-to-be-hard/\">one should expect good giving to be hard</a>.</p>
<p class=\"MsoNormal\"><strong><span style=\"font-size:14.0pt;mso-bidi-font-size:12.0pt\">Implications for maximizing cost-effectiveness</span></strong></p>
<p class=\"MsoNormal\">The remarks and observations above imply that <strong>Bayesian regression in the context of philanthropy is substantially larger than expected</strong>. Â This favors:</p>
<ul>
<li><span style=\"text-indent: -24px;\">Examining a philanthropic opportunityÂ </span><a style=\"text-indent: -24px;\" href=\"http://blog.givewell.org/2011/11/10/maximizing-cost-effectiveness-via-critical-inquiry/\">from many angles</a><span style=\"text-indent: -24px;\">Â rather than relying too heavily on a single perspective.<br></span></li>
<li><span style=\"text-indent: -24px;\"><span style=\"text-indent: -0.25in;\">Giving more weight to robust inputs into oneâs assessment of a philanthropic opportunity</span><span style=\"text-indent: -0.25in;\">. Estimating the cost-effectiveness of health interventions in the developing world has proved to be exceedingly difficult, and this pushes in favor of giving more weight to inputs for which itâs possible to make relatively well-grounded assessments. Some of these areÂ </span><a style=\"text-indent: -0.25in;\" href=\"http://www.givewell.org/international/technical/criteria/scalability\">room for more funding</a><span style=\"text-indent: -0.25in;\">,Â </span><a style=\"text-indent: -0.25in;\" href=\"http://blog.givewell.org/2012/10/25/evaluating-people/\">the quality of the people behind a project</a><span style=\"text-indent: -0.25in;\">Â andÂ </span><a style=\"text-indent: -0.25in;\" href=\"http://blog.givewell.org/2013/04/09/givewells-history-of-philanthropyphilanthropy-journalism-project/\">historical precedent</a><span style=\"text-indent: -0.25in;\">.</span></span><span style=\"text-indent: -0.25in;\"><br></span></li>
<li><span style=\"text-indent: -0.25in;\">Choosing giving opportunities that </span><a style=\"text-indent: -0.25in;\" href=\"http://blog.givewell.org/2012/12/20/more-on-the-ranking-of-our-top-charities/\">it will be possible to learn from</a><span style=\"text-indent: -0.25in;\" class=\"MsoHyperlink\">, </span><span style=\"text-indent: -0.25in;\">and </span><span style=\"text-indent: -0.25in;\" class=\"MsoHyperlink\">Â </span><a style=\"text-indent: -0.25in;\" href=\"http://blog.givewell.org/2011/12/20/give-now-or-give-later/\">giving now instead of giving later</a><span style=\"text-indent: -0.25in;\"> when one encounters such an opportunity.</span></li>
<li><span style=\"text-indent: -0.25in;\"><span style=\"text-indent: -0.25in;\">Choosing giving opportunities </span><a style=\"text-indent: -0.25in;\" href=\"http://blog.givewell.org/2011/05/27/in-defense-of-the-streetlight-effect/\">about which one has a lot of information</a><span style=\"text-indent: -0.25in;\">. GiveWell has been </span><a style=\"text-indent: -0.25in;\" href=\"http://blog.givewell.org/2013/03/14/update-on-givewells-plans-for-2013/\">moving away from</a><span style=\"text-indent: -0.25in;\"> the old criterion of recommending proven interventions, and giving more weight to </span><a style=\"text-indent: -0.25in;\" href=\"http://blog.givewell.org/2012/01/19/trading-off-upside-vs-track-record/\">upside relative to track record</a><span style=\"text-indent: -0.25in;\"> than GiveWell used to. However, this partially reflects the discovery that the expected effectiveness of ostensibly âprovenâ interventions is lower than previously thought.Â </span></span></li>
</ul></div>
<a href=\"http://lesswrong.com/lw/hif/robustness_of_costeffectiveness_estimates_and/#comments\">37 comments</a>
" nil nil "150731b7f997803efebfdc41ae1d32e1") (1 (20946 49832 947168) "http://lesswrong.com/lw/h9b/post_ridiculous_munchkin_ideas/" "Post ridiculous munchkin ideas!" nil "Thu, 16 May 2013 08:27:19 +1000" "Submitted by <a href=\"http://lesswrong.com/user/D_Malik\">D_Malik</a>
#
47 votes
#
<a href=\"http://lesswrong.com/lw/h9b/post_ridiculous_munchkin_ideas/#comments\">1115 comments</a>
<div><p><a href=\"/lw/5c0/epistle_to_the_new_york_less_wrongians/\">Thus spake Eliezer</a>:</p>
<blockquote>
<p>A Munchkin is the sort of person who, faced with a role-playing game, reads through the rulebooks over and over until he finds a way to combine three innocuous-seeming magical items into a cycle of infinite <em>wish</em> spells.<span>Â  </span>Or who, in real life, composes a surprisingly effective diet out of drinking a quarter-cup of extra-light olive oil at least one hour before and after tasting anything else.<span>Â  </span>Or combines liquid nitrogen and antifreeze and life-insurance policies into a ridiculously cheap method of defeating the invincible specter of unavoidable Death.<span>Â  </span>Or figures out how to build the real-life version of the cycle of infinite <em>wish</em> spells.</p>
</blockquote>
<p>It seems that many here might have outlandish ideas for ways of improving our lives. For instance, <a href=\"/lw/gdl/my_simple_hack_for_increased_alertness_and/\">a recent post</a> advocated installing really bright lights as a way to boost alertness and productivity. We should not adopt such hacks into our dogma until we're pretty sure they work; however, one way of knowing whether a crazy idea works is to try implementing it, and you may have more ideas than you're planning to implement.</p>
<p>So: please post all such lifehack ideas! Even if you haven't tried them, even if they seem unlikely to work. Post them separately, unless some other way would be more appropriate. If you've tried some idea and it <em>hasn't</em> worked, it would be useful to post that too.</p></div>
<a href=\"http://lesswrong.com/lw/h9b/post_ridiculous_munchkin_ideas/#comments\">1115 comments</a>
" nil nil "9581e3f73980a2c7e5fc8dafb0a75d86")))