;; -*- coding: utf-8-emacs; -*-
(setq nnrss-group-data '((6 (20954 62970 758888) "http://intelligence.org/2013/06/19/what-is-intelligence-2/?utm_source=rss&utm_medium=rss&utm_campaign=what-is-intelligence-2" "What is Intelligence?" "Luke Muehlhauser" "Wed, 19 Jun 2013 19:59:12 +0000" "<p><img class=\"alignright size-full wp-image-10276\" alt=\"brain of gears and circuits\" src=\"http://intelligence.org/wp-content/uploads/2013/06/brain-of-gears-and-circuits.jpg\" width=\"250\" height=\"327\" />When asked their opinions about “human-level artificial intelligence” — <em>aka</em> “artificial general intelligence” (AGI)<sup>1</sup> — many experts understandably reply that these terms haven’t yet been precisely defined, and it’s hard to talk about something that hasn’t been defined.<sup>2</sup> In this post, I want to briefly outline an imprecise but useful “working definition” for <em>intelligence</em> we tend to use at MIRI. In a future post I will write about some useful working definitions for <em>artificial general intelligence</em>.</p>
<p> </p>
<h3>Imprecise definitions can be useful</h3>
<p>Precise definitions are important, but I concur with Bertrand Russell that</p>
<blockquote>[You cannot] start with anything precise. You have to achieve such precision… as you go along.</p></blockquote>
<p>Physicist <a href=\"http://ieet.org/index.php/IEET/bio/cirkovic/\">Milan Ćirković</a> agrees, and <a href=\"http://www.amazon.com/Astrobiological-Landscape-Start-Publishing-ebook/dp/B008CDSB30/\">gives</a> an example:</p>
<blockquote><p>The formalization of knowledge — which includes giving precise definitions — usually comes at the end of the original research in a given field, not at the very beginning. A particularly illuminating example is the concept of <em>number</em>, which was properly defined in the modern sense only after the development of axiomatic set theory in the… twentieth century.<sup>3</sup></p></blockquote>
<p>For a more AI-relevant example, consider the concept of a “self-driving car,” which has been given a variety of vague definitions <a href=\"http://books.google.com/books?id=7OEDAAAAMBAJ&lpg=PA210&dq=automatic%20car&pg=PA210#v=onepage&q&f=false\">since the 1930s</a>. Would a car <a href=\"http://books.google.com/books?id=xiUDAAAAMBAJ&lpg=PA75&dq=car%20drives%20itself&pg=PA75#v=onepage&q&f=false\">guided by a buried cable</a> qualify? What about a <a href=\"http://books.google.com/books?id=jd8DAAAAMBAJ&lpg=PA128&dq=automatic%20car&pg=PA128#v=onepage&q&f=false\">modified 1955 Studebaker</a> that could use sound waves to detect obstacles and automatically engage the brakes if necessary, but could only steer “on its own” if each turn was preprogrammed? Does that count as a “self-driving car”?</p>
<p>What about the “<a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/05/Dickmanns-et-al-The-seeing-passenger-car-VaMoRs-P.pdf\">VaMoRs</a>” of the 1980s that could avoid obstacles and steer around turns using computer vision, but weren’t advanced enough to be ready for public roads? How about the 1995 <a href=\"http://en.wikipedia.org/wiki/Navlab\">Navlab</a> car that drove across the USA and was fully autonomous for 98.2% of the trip, or the robotic cars which finished the 132-mile off-road course of the <a href=\"http://is.gd/CDvT8V\">2005 DARPA Grand Challenge</a>, supplied only with the GPS coordinates of the route? What about the winning cars of the <a href=\"http://is.gd/kOjor6\">2007 DARPA Grand Challenge</a>, which finished an urban race while obeying all traffic laws and avoiding collisions with other cars? Does <a href=\"http://www.forbes.com/sites/joannmuller/2013/03/21/no-hands-no-feet-my-unnerving-ride-in-googles-driverless-car/\">Google’s driverless car</a> qualify, given that it has logged more than 500,000 autonomous miles without a single accident under computer control, but still struggles with difficult merges and snow-covered roads?<sup>4</sup></p>
<p>Our lack of a precise definition for “self-driving car” doesn’t seem to have hindered progress on self-driving cars very much.<sup>5</sup> And I’m glad we didn’t wait to seriously discuss self-driving cars until we had a precise definition for the term.</p>
<p>Similarly, I don’t think we should wait for a precise definition of AGI before discussing the topic seriously. On the other hand, the term is useless if it carries <em>no</em> information. So let’s work our way toward a stipulative, operational definition for AGI. We’ll start by developing an operational definition for <em>intelligence</em>.</p>
<p><span id=\"more-10275\"></span></p>
<h3>A definition for “intelligence”</h3>
<p><a href=\"http://arxiv.org/pdf/0706.3639.pdf\">Legg and Hutter (2007)</a> found that definitions of intelligence converge toward the idea that “Intelligence measures an agent’s ability to achieve goals in a wide range of environments.” Let’s call this the “optimization power” concept of intelligence, because it measures an agent’s power to optimize the world according to its preferences.</p>
<p>I think this is a productive approach to the issue, since it identifies intelligence with externally measurable <em>performance</em> rather than with the details of <em>how</em> that performance might be achieved (e.g. via consciousness, <a href=\"http://www.hutter1.net/publ/uaigentle.pdf\">brute force calculation</a>, “complexity,” or something else). Moreover, it’s usually <em>performance</em> we care about: we tend to care most about whether an AI will perform well enough to replace human workers, or whether it will perform well enough improve its own abilities without human assistance, not whether it has some particular internal feature.<sup>6</sup></p>
<p>Furthermore, the concept of optimization power allows us to compare the intelligence of different kinds of agents. As Albus (<a href=\"ftp://calhau.dca.fee.unicamp.br/pub/docs/ia005/Albus-outline.pdf\">1991</a>) said, “A useful definition of intelligence… should include both biological and machine embodiments, and these should span an intellectual range from that of an insect to that of an Einstein, from that of a thermostat to that of the most sophisticated computer system that could ever be built.”</p>
<p>I’d like to add one more consideration, though. What if two agents have roughly equal ability to optimize the world according to their preferences, but the second agent requires far more resources to do so? These agents have the same optimization power, but the second one seems to be optimizing more intelligently. So perhaps we could use “intelligence” to mean “optimization power divided by resources used” — what Yudkowsky called <a href=\"http://lesswrong.com/lw/vb/efficient_crossdomain_optimization/\">efficient cross-domain optimization</a>.<sup>7</sup></p>
<p>Other definitions<sup>8</sup> have their merits, too. But at MIRI we find the concept of “efficient cross-domain optimization” sufficiently useful that it serves as our (still imprecise!) working definition for intelligence.</p>
<p>In a future post, I’ll discuss some useful working definitions for <em>artificial general intelligence</em>.</p>
<p> </p>
<h4>Notes</h4>
<p><sup>1</sup> <small>I use the HLAI and AGI interchangeably, but lately I’ve been using AGI almost exclusively, because I’ve learned that many people in the AI community react negatively to any mention of “human-level” AI but have no objection to the concept of narrow vs. general intelligence. See also Ben Goertzel’s comments <a href=\"http://wp.goertzel.org/?p=173\">here</a>.</small></p>
<p><sup>2</sup> <small>Asked when he thought HLAI would be created, Pat Hayes (a past president of <a href=\"http://en.wikipedia.org/wiki/Association_for_the_Advancement_of_Artificial_Intelligence\">AAAI</a>) <a href=\"http://lesswrong.com/r/discussion/lw/999/qa_with_experts_on_risks_from_ai_1/\">replied</a>: “I do not consider this question to be answerable, as I do not accept this (common) notion of ‘human-level intelligence’ as meaningful.” Asked the same question, AI scientist <a href=\"http://www.cse.unsw.edu.au/~willu/\">William Uther</a> <a href=\"http://lesswrong.com/r/discussion/lw/9cm/qa_with_experts_on_risks_from_ai_3/\">replied</a>: “You ask a lot about ‘human level AGI’. I do not think this term is well defined,” while AI scientist <a href=\"http://homepages.inf.ed.ac.uk/bundy/\">Alan Bundy</a> <a href=\"http://lesswrong.com/r/discussion/lw/9cm/qa_with_experts_on_risks_from_ai_3/\">replied</a>: “I don’t think the concept of ‘human-level machine intelligence’ is well formed.”</small></p>
<p><sup>3</sup> <small><a href=\"http://www.amazon.com/Mathematicians-Delight-Dover-Science-Books/dp/0486462404/\">Sawyer (1943)</a> gives another example: “Mathematicians first used the sign √-1, without in the least knowing what it could mean, because it shortened work and led to correct results. People naturally tried to find out why this happened and what √-1 really meant. After two hundreds years they succeeded.” <a href=\"http://www.amazon.com/Intuition-Pumps-Other-Tools-Thinking/dp/0393082067/\">Dennett (2013)</a> makes a related comment: “<em>Define your terms, sir!</em> No, I won’t. That would be premature… My [approach] is an instance of <em>nibbling</em> on a tough problem instead of trying to eat (and digest) the whole thing from the outset… In <em>Elbow Room</em>, I compared my method to the sculptor’s method of roughing out the form in a block of marble, approaching the final surfaces cautiously, modestly, working by successive approximation.”</small></p>
<p><sup>4</sup> <small>With self-driving cars, researchers did use many precise external performance measures (e.g. accident rates, speed, portion of the time they could run unassisted, frequency of getting stuck) to evaluate progress, as well as internal performance metrics (speed of search, bounded loss guarantees, etc.). Researchers could see that these bits of progress were in the right direction, even if their relative contribution long-term was unclear. And so it is with AI in general. AI researchers use many precise external and internal performance measures to evaluate progress, but it is difficult to know the relative contribution of these bits of progress toward the final goal of AGI.</small></p>
<p><sup>5</sup> <small>Heck, we’ve had pornography for millennia and <em>still</em> haven’t been able to define it precisely. Encyclopedia entries for “pornography” <a href=\"http://books.google.com/books?id=xOJvVJv2YlAC&pg=PA636&lpg=PA636&source=bl&ots=N3PI1PbmK1&sig=SIhfXbej_Z_HRDAxxnGZWftTZhg&hl=en&sa=X&ei=9o-NUfTEAqafiALa34CoDQ&ved=0CC8Q6AEwADgK#v=onepage&q&f=false\">often</a> <a href=\"http://books.google.com/books?id=TN-qpt7kAK4C&pg=PA336&lpg=PA336&source=bl&ots=CspWxfT67z&sig=_ymQ5x3lvNnMF0DsaSZkJzsSaqM&hl=en&sa=X&ei=9o-NUfTEAqafiALa34CoDQ&ved=0CDMQ6AEwAjgK#v=onepage&q&f=false\">simply</a> quote Justice Potter Stewart: “I shall not today attempt further to define the kinds of material I understand to be [pornography]… but I know it when I see it.”</small></p>
<p><sup>6</sup> <small>We might care about whether machines are conscious in addition to being intelligent, but we already have a convenient term for that: <em>consciousness</em>. In particular, we might care about machine consciousness because the slow, plodding invention of AI may involve the creation and destruction of millions of partially-conscious near-AIs that are switched on, suffer for a while, and are then switched off — all while being unable to signal to us that they are suffering. This is especially likely if we remain unclear about the nature of consciousness for several more decades, and thus have no principled way (e.g. via <a href=\"http://lesswrong.com/lw/x4/nonperson_predicates/\">nonperson predicates</a>) to create intelligent machines that we <em>know</em> are not conscious (and are thus incapable of suffering). One of the first people to make this point clearly was <a href=\"http://www.amazon.com/Being-No-One-Self-Model-Subjectivity/dp/0262633086/\">Metzinger (2003)</a>, p. 621: “What would you say if someone came along and said, ‘Hey, we want to genetically engineer mentally retarded human infants! For reasons of scientific progress we need infants with certain cognitive and emotional deficits in order to study their postnatal psychological development—we urgently need some funding for this important and innovative kind of research!’ You would certainly think this was not only an absurd and appalling but also a dangerous idea. It would hopefully not pass any ethics committee in the democratic world. However, what today’s ethics committees don’t see is how the first machines satisfying a minimally sufficient set of constraints for conscious experience could be just like such mentally retarded infants. They would suffer from all kinds of functional and representational deficits too. But they would now also subjectively experience those deficits. In addition, they would have no political lobby—no representatives in any ethics committee.” Metzinger repeats the point in <a href=\"http://www.amazon.com/Ego-Tunnel-Science-Mind-Myth/dp/0465020690/\">Metzinger (2010)</a>, starting on page 194.</small></p>
<p><sup>7</sup> <small>Admittedly, this is still pretty vague. One step toward precision would be to propose a definition of intelligence as optimization power for some canonical distribution of possible preferences, over some canonical distribution of environments, with a penalty for resource use. The canonical preferences and canonical environments could be weighted toward preferences and environments relevant to our concerns: we care more about whether AIs can do science than whether they can paint abstract art, and we care more about whether they can achieve their goals in our solar system than whether they can achieve their goals inside a black hole. Also see <a href=\"http://agi-conf.org/2010/wp-content/uploads/2009/06/paper_14.pdf\">Goertzel (2010)</a>‘s “efficient pragmatic general intelligence.”</small></p>
<p><sup>8</sup> <small><a href=\"http://www.ssec.wisc.edu/~billh/g/hibbard_agi11a.pdf\">Hibbard (2011)</a>; <a href=\"http://arxiv.org/pdf/1109.5951.pdf\">Legg & Veness (2011)</a>; <a href=\"http://www.cis.temple.edu/~wangp/Publication/AI_Definitions.pdf\">Wang (2008)</a>; <a href=\"http://arxiv.org/pdf/1109.1314.pdf\">Schaul et al. (2011)</a>; <a href=\"http://www.csse.monash.edu.au/~dld/Publications/2012/Dowe%2BHernandez-Orallo_2012_IQ_tests_are_not_for_machines_comma_yet_IN_PRESS.pdf\">Dowe & Hernandez-Orallo (2012)</a>; <a href=\"http://agi-conf.org/2010/wp-content/uploads/2009/06/paper_14.pdf\">Goertzel (2010)</a>; <a href=\"http://www.cse.buffalo.edu/faculty/shapiro/Papers/hlai.pdf\">Adams et al. (2011)</a>.</small></p>
<p>The post <a href=\"http://intelligence.org/2013/06/19/what-is-intelligence-2/\">What is Intelligence?</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/06/19/what-is-intelligence-2/#comments" "68187908540eeacf83aa68ce4cd0b05e") (5 (20954 62970 754960) "http://intelligence.org/2013/06/07/miris-july-2013-workshop/?utm_source=rss&utm_medium=rss&utm_campaign=miris-july-2013-workshop" "=?utf-8?Q?MIRI=E2=80=99s?= July 2013 Workshop" "Luke Muehlhauser" "Fri, 07 Jun 2013 22:15:07 +0000" "<p><a href=\"http://intelligence.org/get-involved/#workshop\"><img class=\"aligncenter size-full wp-image-10250\" alt=\"Mihaly at April workshop\" src=\"http://intelligence.org/wp-content/uploads/2013/06/Mihaly-at-April-workshop.jpg\" width=\"500\" height=\"308\" /></a></p>
<p>From July 8-14, MIRI will host its <strong>3rd Workshop on Logic, Probability, and Reflection</strong>. The focus of this workshop will be the <a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/\">Löbian obstacle to self-modifying systems</a>.</p>
<p>Participants confirmed so far include:</p>
<ul>
<li><a href=\"http://acritch.com/\">Andrew Critch</a> (just finished his math PhD at UC Berkeley, now working at <a href=\"http://rationality.org/\">CFAR</a>)</li>
<li><a href=\"https://plus.google.com/111568410659864255951\">Abram Demski</a> (USC)</li>
<li><a href=\"http://lesswrong.com/user/Benja/submitted/\">Benja Fallenstein</a> (Bristol U)</li>
<li><a href=\"http://www.linkedin.com/pub/marcello-herreshoff/0/8b4/51a\">Marcello Herreshoff </a>(Google)</li>
<li>Jonathan Lee (Cambridge)</li>
<li><a href=\"http://www.ctpost.com/news/article/Wisdom-beyond-his-years-1390299.php\">Will Sawin</a> (Princeton)</li>
<li><a href=\"http://math.berkeley.edu/~qchu/\">Qioachu Yuan</a> (UC Berkeley)</li>
<li><a href=\"http://yudkowsky.net/\">Eliezer Yudkowsky</a> (MIRI)</li>
</ul>
<p>If you have a strong mathematics background and might like to attend this workshop, it’s not too late to <a href=\"http://intelligence.org/get-involved/#workshop\">apply</a>! And even if <em>this</em> workshop doesn’t fit your schedule, please <strong>do apply</strong>, so that we can notify you of other workshops (long before they are announced publicly).</p>
<p>Information on past workshops:</p>
<ul>
<li><span style=\"line-height: 13px;\">Our<strong> 1st Workshop</strong> (Nov. 11-18, 2012; 4 participants) resulted in Christiano’s <a href=\"http://lesswrong.com/lw/h1k/reflection_in_probabilistic_logic/\">probabilistic logic</a>, an attack on the <a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/\">Löbian obstacle for self-modifying systems</a>.<br />
</span></li>
<li>Our <strong>2nd Workshop</strong> (Apr. 3-24, 2013; 12 participants coming in and out) resulted in (1) some as-yet unpublished progress on Christiano’s probabilistic logic, (2) some progress on program equilibrium recorded in <a href=\"http://intelligence.org/files/RobustCooperation.pdf\">LaVictoire et al. (2013)</a>, and some progress on the Löbian obstacle resulting in <a href=\"http://intelligence.org/files/TilingAgents.pdf\">Yudkowsky & Herreschoff (2013)</a>.</li>
</ul>
<p>The post <a href=\"http://intelligence.org/2013/06/07/miris-july-2013-workshop/\">MIRI’s July 2013 Workshop</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/06/07/miris-july-2013-workshop/#comments" "5dd8c96ab7d5dee064f1bc9f95e46e40") (4 (20954 62970 754206) "http://intelligence.org/2013/06/06/new-research-page-and-two-new-articles/?utm_source=rss&utm_medium=rss&utm_campaign=new-research-page-and-two-new-articles" "New Research Page and Two New Articles" "Luke Muehlhauser" "Fri, 07 Jun 2013 06:54:02 +0000" "<p><a href=\"/research/\"><img class=\"aligncenter size-full wp-image-10246\" alt=\"research page\" src=\"http://intelligence.org/wp-content/uploads/2013/06/research-page.png\" width=\"500\" height=\"321\" /></a></p>
<p>Our new <a href=\"http://intelligence.org/research/\">Research</a> page has launched!</p>
<p>Our previous research page was a simple list of articles, but the new page describes the purpose of our research, explains four categories of research to which we contribute, and highlights the papers we think are most important to read.</p>
<p>We’ve also released drafts of two new research articles.</p>
<p><a href=\"http://intelligence.org/files/TilingAgents.pdf\">Tiling Agents for Self-Modifying AI, and the Löbian Obstacle</a> (discuss it <a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/\">here</a>), by Yudkowsky and Herreshoff, explains one of the key open problems in MIRI’s research agenda:</p>
<blockquote><p>We model self-modification in AI by introducing “tiling” agents whose decision systems will approve the construction of highly similar agents, creating a repeating pattern (including similarity of the offspring’s goals). Constructing a formalism in the most straightforward way produces a Gödelian difficulty, the “Löbian obstacle.” By technical methods we demonstrates the possibility of avoiding this obstacle, but the underlying puzzles of rational coherence are thus only partially addressed. We extend the formalism to partially unknown deterministic environments, and show a very crude extension to probabilistic environments and expected utility; but the problem of finding a fundamental decision criterion for self-modifying probabilistic agents remains open.</p></blockquote>
<p><a href=\"http://intelligence.org/files/RobustCooperation.pdf\">Robust Cooperation in the Prisoner’s Dilemma: Program Equilibrium via Provability Logic</a> (discuss it <a href=\"http://lesswrong.com/lw/hmw/robust_cooperation_in_the_prisoners_dilemma/\">here</a>), by LaVictoire et al., explains some progress in program equilibrium made by MIRI research associate Patrick LaVictoire and several others during MIRI’s April 2013 workshop:</p>
<blockquote><p>Rational agents defect on the one-shot prisoner’s dilemma even though mutual cooperation would yield higher utility for both agents. Moshe Tennenholtz showed that if each program is allowed to pass its playing strategy to all other players, some programs can then cooperate on the one-shot prisoner’s dilemma. Program equilibria is Tennenholtz’s term for Nash equilibria in a context where programs can pass their playing strategies to the other players.</p>
<p>One weakness of this approach so far has been that any two programs which make different choices cannot “recognize” each other for mutual cooperation, even if they are functionally identical. In this paper, provability logic is used to enable a more flexible and secure form of mutual cooperation.</p></blockquote>
<p>Participants of MIRI’s April workshop also made progress on <a href=\"http://lesswrong.com/lw/h1k/reflection_in_probabilistic_logic/\">Christiano’s probabilistic logic</a> (an attack on the Löbian obstacle), but that work is not yet ready to be released.</p>
<p>We’ve also revamped the <a href=\"http://intelligence.org/get-involved/\">Get Involved</a> page, which now includes an <a href=\"/get-involved/#workshop\">application form</a> for forthcoming workshops. If you <em>might</em> like to work with MIRI on some of its open research problems sometime in the next 18 months, <a href=\"/get-involved/#workshop\">please apply</a>! Likewise, if you know someone who might enjoy attending such a workshop, please encourage <em>them</em> to apply.</p>
<p>The post <a href=\"http://intelligence.org/2013/06/06/new-research-page-and-two-new-articles/\">New Research Page and Two New Articles</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/06/06/new-research-page-and-two-new-articles/#comments" "3d723ea88e5053947a7e516ffa4dcb75") (3 (20954 62970 752799) "http://intelligence.org/2013/06/05/friendly-ai-research-as-effective-altruism/?utm_source=rss&utm_medium=rss&utm_campaign=friendly-ai-research-as-effective-altruism" "Friendly AI Research as Effective Altruism" "Luke Muehlhauser" "Thu, 06 Jun 2013 00:55:23 +0000" "<p><iframe src=\"http://embed.ted.com/talks/peter_singer_the_why_and_how_of_effective_altruism.html\" width=\"560\" height=\"315\" frameborder=\"0\" scrolling=\"no\" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe></p>
<p>MIRI was founded in 2000 on the premise that creating<sup>1</sup> Friendly AI might be a particularly efficient way to do as much good as possible.</p>
<p>Some developments since then include:</p>
<ul>
<li>The field of “<a href=\"http://www.ted.com/talks/peter_singer_the_why_and_how_of_effective_altruism.html\">effective altruism</a>” — trying not just to do good but to do <em>as much good as possible</em><sup>2</sup> — has seen more publicity and better research than ever before, in particular through the work of <a href=\"http://www.givewell.org/\">GiveWell</a>, the <a href=\"http://centreforeffectivealtruism.org/\">Center for Effective Altruism</a>, the philosopher <a href=\"http://en.wikipedia.org/wiki/Peter_Singer\">Peter Singer</a>, and the community at <a href=\"http://lesswrong.com/\">Less Wrong</a>.<sup>3</sup></li>
<li>In his recent <a href=\"https://sites.google.com/site/nbeckstead/research/Beckstead%2C%20Nick--On%20the%20Overwhelming%20Importance%20of%20Shaping%20the%20Far%20Future.pdf?attredirects=0&d=1\">PhD dissertation</a>, <a href=\"https://sites.google.com/site/nbeckstead/\">Nick Beckstead</a> has clarified the assumptions behind the claim that shaping the far future (e.g. via Friendly AI) is overwhelmingly important.</li>
<li>Due to research performed by MIRI, the <a href=\"http://www.fhi.ox.ac.uk/\">Future of Humanity Institute</a> (FHI), and others, our strategic situation with regard to machine superintelligence is more clearly understood, and FHI’s <a href=\"http://nickbostrom.com/\">Nick Bostrom</a> has organized much of this work in a <a href=\"http://lesswrong.com/lw/bd6/ai_risk_opportunity_a_timeline_of_early_ideas_and/91zl\">forthcoming book</a>.<sup>4</sup></li>
<li>MIRI’s Eliezer Yudkowsky has <a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/\">begun</a> to describe in more detail which open research problems constitute “Friendly AI research,” in his view.</li>
</ul>
<p>Given these developments, we are in a better position than ever before to assess the value of Friendly AI research as effective altruism.</p>
<p>Still, this is a difficult question. It is challenging enough to evaluate the cost-effectiveness of <a href=\"http://blog.givewell.org/2012/10/18/revisiting-the-case-for-insecticide-treated-nets-itns/\">anti-malaria nets</a> or <a href=\"http://www.givewell.org/international/top-charities/give-directly\">direct cash transfers</a>. Evaluating the cost-effectiveness of attempts to shape the far future (e.g. via Friendly AI) is even more difficult than that. Hence, <strong>this short post sketches an argument that can be given in favor of Friendly AI research as effective altruism, to enable future discussion</strong>, and is <strong>not intended as a thorough analysis.</strong></p>
<p><span id=\"more-10240\"></span></p>
<h3>An argument for Friendly AI research as effective altruism</h3>
<p><a href=\"https://sites.google.com/site/nbeckstead/research/Beckstead%2C%20Nick--On%20the%20Overwhelming%20Importance%20of%20Shaping%20the%20Far%20Future.pdf?attredirects=0&d=1\">Beckstead (2013)</a> argues<sup>5</sup> for the following thesis:</p>
<blockquote><p>From a global perspective, what matters most (in expectation) is that we do what is best (in expectation) for the general trajectory along which our descendants develop over the coming millions, billions, and trillions of years.</p></blockquote>
<p>Why think this? Astronomical facts suggest that humanity (including “post-humanity”) could survive for billions or trillions of years (<a href=\"http://books.google.com/books?id=X5jdMyJKNL4C&pg=PT77&lpg=PT77#v=onepage&q&f=false\">Adams 2008</a>), and could thus produce enormous amounts of good.<sup>6</sup> But the value produced by our future depends on our <em>development trajectory</em>. If humanity destroys itself with powerful technologies in the 21st century, then nearly all that future value is lost. And if we survive but develop along a trajectory dominated by conflict and poor decisions, then the future could be much less good than if our trajectory is dominated by altruism and wisdom. Moreover, some of our actions today can have “ripple effects”<sup>7</sup> which determine the trajectory of human development, because many outcomes are <a href=\"http://en.wikipedia.org/wiki/Path_dependence\">path-dependent</a>. Hence, actions which directly or indirectly precipitate particular trajectory changes (e.g. mitigating existential risks) can have vastly more value (in expectation) than actions with merely proximate benefits (e.g. saving the lives of 20 wild animals). Beckstead calls this the “rough future-shaping argument.”</p>
<p>If we accept the normative assumptions lurking behind this argument (e.g. <a href=\"http://en.wikipedia.org/wiki/Risk_neutral\">risk neutrality</a>; see Beckstead’s dissertation), then the far future is enormously valuable (if it goes at least as well on average as the past century), and existential risk reduction is much more important than producing proximate benefits (e.g. global health, poverty reduction) or speeding up development (which could in fact increase existential risks, and even if it doesn’t, has lower expected value than existential risk reduction).</p>
<p>However, Beckstead’s conclusion is not necessarily that existential risk reduction should be our global priority, because</p>
<blockquote><p>there may be other ways to have a large, persistent effect on the far future without reducing existential risk… Some persistent changes in values and social norms could make the future [some fraction] better or worse… Sure, succeeding in preventing an existential catastrophe would be better than making a smaller trajectory change, but creating a small positive trajectory change may be significantly easier.</p></blockquote>
<p>Instead, Beckstead’s arguments suggest that “what matters most for shaping the far future is producing positive trajectory changes and avoiding negative ones.” Existential risk reduction is one important kind of positive trajectory change that could turn out to be the intervention with the highest expected value.</p>
<p>One important clarification is in order. It could turn out to be that working toward proximate benefits or development acceleration does more good than “direct” efforts for trajectory change, if working toward proximate benefits or development acceleration turns out to have major ripple effects which produce important trajectory change. For example, perhaps an “ordinary altruistic effort” like solving India’s <a href=\"http://www.dnaindia.com/india/1593586/report-71-million-hit-by-iodine-deficiency-in-india\">iodine deficiency problem</a> would cause there to be thousands of “extra” world-class elite thinkers two generations from now, which could increase humanity’s chances of intelligently navigating the crucial 21st century and spreading to the stars. (I don’t think this is likely; I suggest it merely for illustration.)</p>
<p>For the sake of argument, suppose you agree with Beckstead’s core thesis that “what matters most (in expectation) is that we do what is best (in expectation) for the general trajectory along which our descendants develop.” Suppose you also think, as I do, that machine superintelligence is probably inevitable.<sup>8</sup></p>
<p>In that case, you might think that Friendly AI research is a uniquely foreseeable and impactful way to shape the far future in an enormously positive way, <a href=\"http://lesswrong.com/lw/hjb/a_proposed_adjustment_to_the_astronomical_waste/91zq\">because</a> “our effects on the far future must almost entirely pass through our effects on the development of machine superintelligence.” All other developing trends might be overridden by the overwhelming effectiveness of machine superintelligence — and specifically, by the values that were (explicitly or implicitly, directly or indirectly) written into the machine superintelligence(s).</p>
<p>If that’s right, our situation is a bit like sending an interstellar probe to <a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/05/Armstrong-Sandberg-Eternity-in-six-hours-intergalactic-spreading-of-intelligent-life-and-sharpening-the-Fermi-paradox.pdf\">colonize distant solar systems</a> before they recede beyond the <a href=\"http://en.wikipedia.org/wiki/Observable_universe#Particle_horizon\">cosmological horizon</a> and can thus never be reached from Earth again due to <a href=\"https://en.wikipedia.org/wiki/Metric_expansion_of_space\">the expansion of the universe</a>. Anything on Earth that doesn’t affect the content of the probe will have no impact on those solar systems. (See also <a href=\"http://lesswrong.com/lw/hjb/a_proposed_adjustment_to_the_astronomical_waste/921r\">this comment</a>.)</p>
<p> </p>
<h3>Potential defeaters</h3>
<p>The rough argument above — in favor of Friendly AI research as an efficient form of effective altruism — deserves to be “fleshed out” in more detail.<sup>9</sup></p>
<p>Potential defeaters should also be examined:</p>
<ul>
<li>Perhaps we ought to reject one or more of the normative assumptions behind Beckstead’s rough future-shaping argument.</li>
<li>Perhaps it’s not true that “our effects on the far future must almost entirely pass through our effects on the development of machine superintelligence.”</li>
<li>Perhaps Friendly AI research is not (today) a particularly efficient way to positively affect the development of machine superintelligence. Competing interventions may include: (1) <a href=\"http://lesswrong.com/lw/bd6/ai_risk_opportunity_a_timeline_of_early_ideas_and/91zl\">AI risk strategy research</a>, (2) improving <a href=\"http://intelligence.org/2013/05/15/when-will-ai-be-created/\">technological forecasting</a>, (3) <a href=\"http://www.vannevargroup.org/\">improving science in general</a>, (4) <a href=\"http://rationalaltruist.com/2013/06/03/my-outlook/\">improving and spreading effective altruism and rationality</a>, and (5) many others.</li>
</ul>
<p>In future blog posts, members of the effective altruist community (including myself) will expand on the original argument and examine potential defeaters.</p>
<h4></h4>
<h4>Notes</h4>
<p><small>My thanks to those who provided feedback on this post: Carl Shulman, Nick Beckstead, Jonah Sinick, and Eliezer Yudkowsky.</small></p>
<p><sup>1</sup> <small>In this post, I talk about the value of <em>humanity in general</em> creating Friendly AI, though MIRI co-founder Eliezer Yudkowsky usually talks about <em>MIRI in particular</em> — or at least, a functional equivalent — creating Friendly AI. This is because I am not as confident as Yudkowsky that it is best for MIRI to attempt to build Friendly AI. When updating MIRI’s bylaws in early 2013, Yudkowsky and I came to a compromise on the language of MIRI’s mission statement, which now reads: “[MIRI] exists to ensure that the creation of smarter-than-human intelligence has a positive impact. Thus, the charitable purpose of [MIRI] is to: (a) perform research relevant to ensuring that smarter-than-human intelligence has a positive impact; (b) raise awareness of this important issue; (c) advise researchers, leaders and laypeople around the world; and (d) <em>as necessary</em>, implement a smarter-than-human intelligence with humane, stable goals” (emphasis added). My own hope is that it will not be necessary for MIRI (or a functional equivalent) to attempt to build Friendly AI itself. But of course I must remain open to the possibility that this will be the wisest course of action as the first creation of AI <a href=\"http://intelligence.org/2013/05/15/when-will-ai-be-created/\">draws nearer</a>. There is also the question of capability: few people think that a non-profit research organization has much chance of being the first to build AI. I worry, however, that the world’s elites will not find it fashionable to take this problem seriously until the creation of AI is only a few decades away, at which time it will be especially difficult to develop the mathematics of Friendly AI in time, and humanity will be forced to take a gamble on its very survival with powerful AIs we have little reason to trust.</small></p>
<p><sup>2</sup> <small>One might think of effective altruism as a straightforward application of <a href=\"http://lesswrong.com/lw/gu1/decision_theory_faq/\">decision theory</a> to the subject of philanthropy. Philanthropic agents of all kinds (individuals, groups, foundations, etc.) ask themselves: “How can we choose philanthropic acts (e.g. donations) which (in expectation) will do as much good as possible, given what we care about?” The consensus recommendation for <em>all</em> kinds of choices under uncertainty, including philanthropic choices, is to maximize expected utility (<a href=\"http://books.google.com/books?id=S1-K4AT3zXYC&lpg=PA11&ots=wjavib87qF&dq=normative%20systems%3A%20logic%2C%20probability%2C%20and%20rational%20choice&lr&pg=PA11#v=onepage&q&f=false\">Chater & Oaksford 2012</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/05/Peterson-From-Outcomes-to-Acts-a-non-standard-axiomatization-of-the-expected-utility-principle.pdf\">Peterson 2004</a>; <a href=\"http://www.amazon.com/Without-Good-Reason-Rationality-Philosophy/dp/0198235747/\">Stein 1996</a>; <a href=\"http://www.amazon.com/Axiomatic-Utility-Theory-under-Risk/dp/3540643192\">Schmidt 1998</a>:19). Different philanthropic agents value different things, but decision theory suggests that each of them can get the most of what they want if they each maximize their expected utility. Choices which maximize expected utility are in this sense “optimal,” and thus another term for effective altruism is “<a href=\"http://lesswrong.com/lw/da4/what_is_optimal_philanthropy/\">optimal philanthropy</a>.” Note that effective altruism in this sense is not too dissimilar from earlier approaches to philanthropy, including <a href=\"http://en.wikipedia.org/wiki/High_impact_philanthropy\">high-impact philanthropy</a> (making “<a href=\"http://www.impact.upenn.edu/faq/\">the biggest difference possible, given the amount of capital invested</a>“), <a href=\"http://www.amphilsoc.org/sites/default/files/490202.pdf\">strategic philanthropy</a>, <a href=\"http://www.hudson.org/files/pdf_upload/Stanley_Katz_APS_Proceedings_Piece_June_2005.pdf\">effective philanthropy</a>, and <a href=\"http://wisephilanthropy.com/\">wise philanthropy</a>. Note also that effective altruism does not say that a philanthropic agent should specify complete utility and probability functions over outcomes and then compute the philanthropic act with the highest expected utility — that is impractical for bounded agents. We must keep in mind the distinction between normative, descriptive, and prescriptive models of decision-making (Baron 2007): “normative models tell us how to evaluate… decisions in terms of their departure from an ideal standard. Descriptive models specify what people in a particular culture actually do and how they deviate from the normative models. Prescriptive models are designs or inventions, whose purpose is to bring the results of actual thinking into closer conformity to the normative model.” The <em>prescriptive</em> question — about what bounded philanthropic agents should do to maximize expected utility with their philanthropic choices — tends to be extremely complicated, and is the subject of most of the research performed by the effective altruism community.</small></p>
<p><sup>3</sup> <small>See, for example: <a href=\"http://lesswrong.com/lw/37f/efficient_charity/\">Efficient Charity</a>, <a href=\"http://lesswrong.com/lw/3gj/efficient_charity_do_unto_others/\">Efficient Charity: Do Unto Others</a>, <a href=\"http://lesswrong.com/lw/2qq/politics_as_charity/\">Politics as Charity</a>, <a href=\"http://lesswrong.com/lw/aid/heuristics_and_biases_in_charity/\">Heuristics and Biases in Charity</a>, <a href=\"http://lesswrong.com/lw/2hv/public_choice_and_the_altruists_burden/\">Public Choice and the Altruist’s Burden</a>, <a href=\"http://lesswrong.com/lw/44c/on_charities_and_linear_utility/\">On Charities and Linear Utility</a>, <a href=\"http://lesswrong.com/lw/6py/optimal_philanthropy_for_human_beings/\">Optimal Philanthropy for Human Beings</a>, <a href=\"http://lesswrong.com/lw/6z/purchase_fuzzies_and_utilons_separately/\">Purchase Fuzzies and Utilons Separately</a>, <a href=\"http://lesswrong.com/lw/65/money_the_unit_of_caring/\">Money: The Unit of Caring</a>, <a href=\"http://lesswrong.com/lw/3kl/optimizing_fuzzies_and_utilons_the_altruism_chip/\">Optimizing Fuzzies and Utilons: The Altruism Chip Jar</a>, <a href=\"http://lesswrong.com/lw/684/efficient_philanthropy_local_vs_global_approaches/\">Efficient Philanthropy: Local vs. Global Approaches</a>, <a href=\"http://lesswrong.com/lw/2pq/the_effectiveness_of_developing_world_aid/\">The Effectiveness of Developing World Aid</a>, <a href=\"http://lesswrong.com/lw/2kh/against_cryonics_for_costeffective_charity/\">Against Cryonics & For Cost-Effective Charity</a>, <a href=\"http://lesswrong.com/lw/gzq/bayesian_adjustment_does_not_defeat_existential/\">Bayesian Adjustment Does Not Defeat Existential Risk Charity</a>, <a href=\"http://lesswrong.com/lw/373/how_to_save_the_world/\">How to Save the World</a>, and <a href=\"http://lesswrong.com/lw/da4/what_is_optimal_philanthropy/\">What is Optimal Philanthropy?</a></small></p>
<p><sup>4</sup> <small>I believe Beckstead and Bostrom have done the research community an enormous service in creating a <em>framework</em>, a <em>shared language</em>, for discussing trajectory changes, existential risks, and machine superintelligence. When discussing these topics with my colleagues, it has often been the case that the first hour of conversation is spent merely trying to understand what the other person is saying — how they are using the terms and concepts they employ. Beckstead’s and Bostrom’s recent work should enable clearer and more efficient communication between researchers, and therefore greater research productivity. Though I am not aware of any controlled, experimental studies on the effect of shared language on research productivity, a shared language is widely considered to be of great benefit for any field of research, and I shall provide a few examples of this claim which appear in print. <a href=\"http://atmos-chem-phys.net/6/2017/2006/acp-6-2017-2006.pdf\">Fuzzi et al. (2006)</a>: “The use of inconsistent terms can easily lead to misunderstandings and confusion in the communication between specialists from different [disciplines] of atmospheric and climate research, and may thus potentially inhibit scientiﬁc progress.” <a href=\"http://www.pik-potsdam.de/research/transdisciplinary-concepts-and-methods/projects/project-archive/favaia/pubs/hinkel-knowledge-integration.pdf\">Hinkel (2008)</a>: “Technical languages enable their users, e.g. members of a scientiﬁc discipline, to communicate eﬃciently about a domain of interest.” <a href=\"http://www.cs.cofc.edu/~bowring/classes/csis%20633/readings/madin-etal-tree-2008.pdf\">Madin et al. (2007)</a>: “terminological ambiguity slows scientiﬁc progress, leads to redundant research efforts, and ultimately impedes advances towards a uniﬁed foundation for ecological science.”</small></p>
<p><sup>5</sup> <small>In addition to Beckstead’s thesis, see also <a href=\"http://lesswrong.com/lw/hjb/a_proposed_adjustment_to_the_astronomical_waste/\">A Proposed Adjustment to the Astronomical Waste Argument</a>.</small></p>
<p><sup>6</sup> <small>Beckstead doesn’t mention this, but I would like to point out that moral realism is not required for Beckstead’s arguments to go through. In fact, I generally accept Beckstead’s arguments even though most philosophers would not consider me a moral realist, though to some degree that is a semantic debate (<a href=\"http://lesswrong.com/lw/5u2/pluralistic_moral_reductionism/\">Muehlhauser 2011</a>; <a href=\"http://www.victoria.ac.nz/staff/richard_joyce/acrobat/joyce_metaethical.pluralism.pdf\">Joyce 2012</a>). If you’re a moral realist and you believe your intuitive moral judgments are data about what is morally true, then Beckstead’s arguments (if successful) have something to say about what is morally true, and about what you should do if you want to act in morally good ways. If you’re a moral anti-realist but you think your intuitive judgments are data about what you value — or about what you would value if you had more time to think about your values and how to resolve the contradictions among them — then Beckstead’s arguments (if successful) have something to say about what you value, and about what you should do if you want to help achieve what you value.</small></p>
<p><sup>7</sup> <small>Karnofsky calls these “<a href=\"http://blog.givewell.org/2013/05/15/flow-through-effects/\">flow-through effects</a>.”</small></p>
<p><sup>8</sup> <small>See <a href=\"http://lesswrong.com/lw/bd6/ai_risk_opportunity_a_timeline_of_early_ideas_and/91zl\">Bostrom (forthcoming)</a> for an extended argument. Perhaps the most likely defeater for machine superintelligence is that global catastrophe may halt scientific progress before human-level AI is created.</small></p>
<p><sup>9</sup> <small>Beckstead, in personal communication, suggested (but didn’t necessarily endorse) the following formalization of the rough argument sketched in the main text of the blog post: “(1) To a first approximation, the future of humanity is all that matters. (2) To a much greater extent than anything else, the future of humanity is highly sensitive to how machine intelligence unfolds. (3) Therefore, there is a very strong presumption in favor of working on any project which makes machine intelligence unfold in a better way. (4) FAI research is the most promising route to making machine intelligence unfold in a better way. (5) Therefore, there is a very strong presumption in favor of doing FAI research.” <a href=\"https://sites.google.com/site/nbeckstead/research/Beckstead%2C%20Nick--On%20the%20Overwhelming%20Importance%20of%20Shaping%20the%20Far%20Future.pdf?attredirects=0&d=1\">Beckstead (2013)</a> examines the case for (1). <a href=\"http://lesswrong.com/lw/bd6/ai_risk_opportunity_a_timeline_of_early_ideas_and/91zl\">Bostrom (forthcoming)</a>, in large part, examines the case for (2). Premise (3) informally follows from (1) and (2), and the conclusion (5) informally follows from (3) and (4). Premise (4) appears to me to be the most dubious part of the argument, and the least explored in the extant literature.</small></p>
<p>The post <a href=\"http://intelligence.org/2013/06/05/friendly-ai-research-as-effective-altruism/\">Friendly AI Research as Effective Altruism</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/06/05/friendly-ai-research-as-effective-altruism/#comments" "f792fa2e4c21633edd56dfd2669717e0") (2 (20954 62970 749118) "http://intelligence.org/2013/05/30/miri-may-newsletter-intelligence-explosion-microeconomics-and-other-publications/?utm_source=rss&utm_medium=rss&utm_campaign=miri-may-newsletter-intelligence-explosion-microeconomics-and-other-publications" "MIRI May Newsletter: Intelligence Explosion Microeconomics and Other Publications" "jake" "Thu, 30 May 2013 18:48:11 +0000" "<p><!-- // End Template Preheader \\\\ --></p>
<table id=\"templateContainer\" style=\"border: 0px solid #e5e5e5; background-color: #ffffff;\" width=\"575\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">
<tbody>
<tr>
<td style=\"border-collapse: collapse;\" align=\"center\" valign=\"top\"><!-- // Begin Template Header \\\\ --></p>
<table id=\"templateHeader\" style=\"background-color: #ffffff; border-bottom: 0px none; padding: 0px;\" width=\"575\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">
<tbody>
<tr>
<td class=\"headerContent\" style=\"border-collapse: collapse; color: #ffffff; font-family: Arial; font-size: 34px; font-weight: bold; line-height: 100%; padding: 0; text-align: center; vertical-align: middle;\"><!-- // Begin Module: Standard Header Image \\\\ --><img id=\"headerImage campaign-icon\" style=\"border: 1px; border-style: none; border-width: px; height: auto; width: 575px; margin: 0; padding: 1px; max-width: 575; line-height: 100%; outline: none; text-decoration: none;\" alt=\"\" src=\"http://gallery.mailchimp.com/353906382677fa789a483ba9e/images/newsletterheader_sm_c.jpg\" width=\"575\" height=\"178\" border=\"0\" /><!-- // End Module: Standard Header Image \\\\ --></td>
</tr>
</tbody>
</table>
<p><!-- // End Template Header \\\\ --></td>
</tr>
<tr>
<td style=\"border-collapse: collapse;\" align=\"center\" valign=\"top\"><!-- // Begin Template Body \\\\ --></p>
<table id=\"templateBody\" width=\"575\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">
<tbody>
<tr>
<td style=\"border-collapse: collapse;\" valign=\"top\" width=\"575\">
<table width=\"575\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">
<tbody>
<tr>
<td style=\"border-collapse: collapse;\" valign=\"top\">
<table width=\"575\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">
<tbody>
<tr>
<td class=\"bodyContent\" style=\"border-collapse: collapse; background-color: #ffffff; border: 1px solid #e5e5e5;\" valign=\"top\"><!-- // Begin Module: Standard Content \\\\ --></p>
<table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"20\">
<tbody>
<tr>
<td style=\"border-collapse: collapse;\" valign=\"top\">
<div mc:repeatable=\"repeat_1\" mc:repeatindex=\"0\" style=\"color: #4a535d; font-family: sans-serif; font-size: 14px; line-height: 1.5em; text-align: left; border-bottom: solid 1px #e5e5e5; padding: 13px 24px 10px 23px;\">
<h2 class=\"h2 title\" style=\"color: #003b65; display: block; font-family: sans-serif; font-size: 22px; font-weight: bold; line-height: 100%; text-align: left; padding-top: 5px; margin-top: 0 !important; margin-right: 0 !important; margin-bottom: 10px !important; margin-left: 0 !important;\"><span class=\"mc-toc-title\">Greetings From the Executive Director</span></h2>
<p><img style=\"max-width: 160px; height: auto; line-height: 100%; outline: none; padding: 1px; text-decoration: none; display: inline; width: 165px; border: 1px; border-style: solid; border-width: 1px; border-color: Black;\" alt=\"\" src=\"http://intelligence.org/files/luke.jpeg\" width=\"165\" height=\"250\" align=\"right\" />Dear friends,</p>
<p>It’s been a busy month!</p>
<p>Mostly, we’ve been busy <em>publishing</em> things. As you’ll see below, <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://www.amazon.com/Singularity-Hypotheses-Scientific-Philosophical-Assessment/dp/3642325599/\" target=\"_self\"><em>Singularity Hypotheses</em></a> has now been published, and it includes four chapters by MIRI researchers or research associates. We’ve also published two new technical reports — one on decision theory and another on intelligence explosion microeconomics — and several new blog posts analyzing various issues relating to the future of AI. Finally, we added <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/2013/05/24/four-articles-added-to-research-page/\" target=\"_self\">four older articles</a> to the research page, including <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/files/IdealAdvisorTheories.pdf\" target=\"_self\">Ideal Advisor Theories and Personal CEV</a> (2012).</p>
<p>In our <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/2013/04/18/miri-april-newsletter-relaunch-celebration-and-a-new-math-result/\" target=\"_self\">April newsletter</a> we spoke about our April 11th party in San Francisco, celebrating our relaunch as the Machine Intelligence Research Institute and our transition to mathematical research. Additional photos from that event are now available as a <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"https://www.facebook.com/media/set/?set=a.520720301298692.1073741826.170446419659417&type=3\" target=\"_self\">Facebook photo album</a>. We’ve also uploaded a video from the event, in which I spend 2 minutes explaining MIRI’s relaunch and some tentative results from the April workshop. After that, visiting researcher <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://qchu.wordpress.com/\" target=\"_self\">Qiaochu Yuan</a> spends 4 minutes explaining one of MIRI’s core research questions: the Löbian obstacle to self-modifying systems.</p>
<p>Some of the research from our April workshop will be published in June, so if you’d like to read about those results right away, you might like to <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/blog/\" target=\"_self\">subscribe</a> to <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/blog\" target=\"_self\">our blog</a>.</p>
<p>Cheers!</p>
<p>Luke Muehlhauser</p>
<p>Executive Director</p>
<p><span id=\"more-10217\"></span></p>
</div>
<div mc:repeatable=\"repeat_1\" mc:repeatindex=\"1\" style=\"color: #4a535d; font-family: sans-serif; font-size: 14px; line-height: 1.5em; text-align: left; border-bottom: solid 1px #e5e5e5; padding: 13px 24px 10px 23px;\">
<h2 class=\"h2 title\" style=\"color: #003b65; display: block; font-family: sans-serif; font-size: 22px; font-weight: bold; line-height: 100%; text-align: left; padding-top: 5px; margin-top: 0 !important; margin-right: 0 !important; margin-bottom: 10px !important; margin-left: 0 !important;\"><span class=\"mc-toc-title\">Intelligence Explosion Microeconomics</span></h2>
<p>Our largest new publication this month is Yudkowsky’s 91-page <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/files/IEM.pdf\" target=\"_self\">Intelligence Explosion Microeconomics</a> (discuss <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://lesswrong.com/lw/hbd/new_report_intelligence_explosion_microeconomics/\" target=\"_self\">here</a>). In this article, Yudkowsky takes some initial steps toward tackling the key quantitative issue in the intelligence explosion, “reinvestable returns on cognitive investments”: what kind of returns can you get from an investment in cognition, can you reinvest it to make yourself even smarter, and does this process die out or blow up? The article can be thought of as a compact and hopefully more coherent successor to the <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate\" target=\"_self\">AI Foom Debate</a> of 2009, featuring Yudkowsky and GMU economist <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://hanson.gmu.edu/\" target=\"_self\">Robin Hanson</a>.</p>
<p>Here is the abstract:</p>
<blockquote><p>I. J. Good’s thesis of the ‘intelligence explosion’ is that a sufficiently advanced machine intelligence could build a smarter version of itself, which could in turn build an even smarter version of itself, and that this process could continue enough to vastly exceed human intelligence. As Sandberg (2010) correctly notes, there are several attempts to lay down return-on-investment formulas intended to represent sharp speedups in economic or technological growth, but very little attempt has been made to deal formally with I. J. Good’s intelligence explosion thesis as such.</p>
<p>I identify the key issue as <em>returns on cognitive reinvestment</em> — the ability to invest more computing power, faster computers, or improved cognitive algorithms to yield cognitive labor which produces larger brains, faster brains, or better mind designs. There are many phenomena in the world which have been argued as evidentially relevant to this question, from the observed course of hominid evolution, to Moore’s Law, to the competence over time of machine chess-playing systems, and many more. I go into some depth on the sort of debates which then arise on how to interpret such evidence. I propose that the next step forward in analyzing positions on the intelligence explosion would be to formalize return-on-investment curves, so that each stance can say formally which possible microfoundations they hold to be <em>falsified</em> by historical observations already made. More generally, I pose multiple open questions of ‘returns on cognitive reinvestment’ or ‘intelligence explosion microeconomics’. Although such questions have received little attention thus far, they seem highly relevant to policy choices affecting the outcomes for Earth-originating intelligent life.</p></blockquote>
<p>The dedicated mailing list will be small and restricted to technical discussants: apply for it <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"https://docs.google.com/forms/d/1KElE2Zt_XQRqj8vWrc_rG89nrO4JtHWxIFldJ3IY_FQ/viewform\" target=\"_self\">here</a>.</p>
</div>
<div mc:repeatable=\"repeat_1\" mc:repeatindex=\"2\" style=\"color: #4a535d; font-family: sans-serif; font-size: 14px; line-height: 1.5em; text-align: left; border-bottom: solid 1px #e5e5e5; padding: 13px 24px 10px 23px;\">
<h2 class=\"h2 title\" style=\"color: #003b65; display: block; font-family: sans-serif; font-size: 22px; font-weight: bold; line-height: 100%; text-align: left; padding-top: 5px; margin-top: 0 !important; margin-right: 0 !important; margin-bottom: 10px !important; margin-left: 0 !important;\">When Will AI Be Created?</h2>
<p>In part, intelligence explosion microeconomics seeks to answer the question “How quickly will human-level AI self-improve to become superintelligent?” Another major question in AI forecasting is, of course: “When will we create human-level AI?”</p>
<p>This is another difficult question, and Luke Muehlhauser surveyed those difficulties in a recent (and quite detailed) blog post: <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/2013/05/15/when-will-ai-be-created/\" target=\"_self\">When Will AI Be Created?</a> He concludes:</p>
<blockquote><p>Given these considerations, I think the most appropriate stance on the question “When will AI be created?” is something like this:</p>
<p>“We can’t be confident AI will come in the next 30 years, and we can’t be confident it’ll take more than 100 years, and anyone who is confident of either claim is pretending to know too much.”</p>
<p>How confident is “confident”? Let’s say 70%. That is, I think it is unreasonable to be 70% confident that AI is fewer than 30 years away, and I also think it’s unreasonable to be 70% confident that AI is more than 100 years away.</p>
<p>This statement admits my inability to predict AI, but it also constrains my probability distribution over “years of AI creation” quite a lot.</p>
<p>I think the considerations above justify these constraints on my probability distribution, but I haven’t spelled out my reasoning in great detail. That would require more analysis than I can present here. But I hope I’ve at least summarized the basic considerations on this topic, and those with different probability distributions than mine can now build on my work here to try to justify them.</p></blockquote>
<p>Muehlhauser also explains four methods for reducing our uncertainty about AI timelines: <em>explicit quantification</em>, <em>leveraging aggregation</em>, <em>signposting the future</em>, and <em>decomposing the phenomena</em>.</p>
<p>As it turns out, <strong>you can participate</strong> in the first two methods for improving our AI forecasts by <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/2013/05/24/sign-up-for-daggre-to-improve-science-technology-forecasting/\" target=\"_self\">signing up for GMU’s DAGGRE program</a>. Muehlhauser himself has signed up.</p>
<p>Muehlhauser also wrote a 400-word piece on the difficulty of AI forecasting for <em>Quartz</em> magazine: <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://qz.com/85825/robots-may-take-our-jobs-but-its-hard-to-say-when/\" target=\"_self\">Robots will take our jobs, but it’s hard to say when</a>. Here’s a choice quote:</p>
<blockquote><p>We’ve had the computing power of a honeybee’s brain for quite a while now, but that doesn’t mean we know how to build tiny robots that fend for themselves outside the lab, find their own sources of energy, and communicate with others to build their homes in the wild.</p></blockquote>
<p> </p>
</div>
<div mc:repeatable=\"repeat_1\" mc:repeatindex=\"3\" style=\"color: #4a535d; font-family: sans-serif; font-size: 14px; line-height: 1.5em; text-align: left; border-bottom: solid 1px #e5e5e5; padding: 13px 24px 10px 23px;\">
<h2 class=\"h2 title\" style=\"color: #003b65; display: block; font-family: sans-serif; font-size: 22px; font-weight: bold; line-height: 100%; text-align: left; padding-top: 5px; margin-top: 0 !important; margin-right: 0 !important; margin-bottom: 10px !important; margin-left: 0 !important;\"><span class=\"mc-toc-title\">Singularity Hypothesis Published</span></h2>
<p><a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://www.amazon.com/Singularity-Hypotheses-Scientific-Philosophical-Assessment/dp/3642325599/\"><img style=\"max-width: 160px; height: auto; line-height: 100%; outline: none; padding: 1px; text-decoration: none; display: inline; width: 131px; border: 1px; border-style: solid; border-width: 1px; border-color: Black;\" alt=\"\" src=\"http://intelligence.org/wp-content/uploads/2013/04/singularity-hypotheses.jpg\" width=\"131\" height=\"200\" align=\"right\" /></a></p>
<p><a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://www.amazon.com/Singularity-Hypotheses-Scientific-Philosophical-Assessment/dp/3642325599/\" target=\"_self\"><em>Singularity Hypotheses: A Scientific and Philosophical Assessment</em></a> has now been published by Springer, in hardcover and ebook forms.</p>
<p>The book contains 20 chapters about the prospect of machine superintelligence, including 4 chapters by MIRI researchers and research associates:</p>
<ul>
<li><a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/files/IE-EI.pdf\" target=\"_self\">Intelligence Explosion: Evidence and Import</a> by Luke Muehlhauser and Anna Salamon</li>
<li><a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/files/IE-ME.pdf\" target=\"_self\">Intelligence Explosion and Machine Ethics</a> by Luke Muehlhauser and Louie Helm</li>
<li>Friendly Artificial Intelligence by Eliezer Yudkowsky, a shortened version of <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/files/AIPosNegFactor.pdf\" target=\"_self\">Yudkowsky (2008)</a></li>
<li><a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/files/AGI-HMM.pdf\" target=\"_self\">Artificial General Intelligence and the Human Mental Model</a> by Roman Yampolskiy and (MIRI research associate) Joshua Fox</li>
</ul>
<p>For more details, see <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/2013/04/25/singularity-hypotheses-published/\" target=\"_self\">the blog post</a>.</p>
</div>
<div mc:repeatable=\"repeat_1\" mc:repeatindex=\"4\" style=\"color: #4a535d; font-family: sans-serif; font-size: 14px; line-height: 1.5em; text-align: left; border-bottom: solid 1px #e5e5e5; padding: 13px 24px 10px 23px;\">
<h2 class=\"h2 title\" style=\"color: #003b65; display: block; font-family: sans-serif; font-size: 22px; font-weight: bold; line-height: 100%; text-align: left; padding-top: 5px; margin-top: 0 !important; margin-right: 0 !important; margin-bottom: 10px !important; margin-left: 0 !important;\"><span class=\"mc-toc-title\">Timeless Decision Theory Paper Published</span></h2>
<p><a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/files/Comparison.pdf\"><img style=\"max-width: 160px; height: auto; line-height: 100%; outline: none; padding: 1px; text-decoration: none; display: inline; width: 203px; border: 1px; border-style: solid; border-width: 1px; border-color: Black;\" alt=\"\" src=\"http://intelligence.org/wp-content/uploads/2013/04/Altair-paper-front.png\" width=\"203\" height=\"200\" align=\"right\" /></a> During his time as a research fellow for MIRI, Alex Altair wrote an article on <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://wiki.lesswrong.com/wiki/Timeless_decision_theory\" target=\"_self\">Timeless Decision Theory</a> (TDT) that has now been published: “<a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/files/Comparison.pdf\" target=\"_self\">A Comparison of Decision Algorithms on Newcomblike Problems</a>.”</p>
<p>Altair’s article is both more succinct and also more precise in its formulation of TDT than Yudkowsky’s earlier paper “<a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/files/TDT.pdf\" target=\"_self\">Timeless Decision Theory</a>.” Thus, Altair’s paper should serve as a handy introduction to TDT for philosophers, computer scientists, and mathematicians, while Yudkowsky’s paper remains required reading for anyone interested to develop TDT further, for it covers more ground than Altair’s paper.</p>
<p>For a gentle introduction to the entire field of normative decision theory (including TDT), see Muehlhauser and Williamson’s <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://lesswrong.com/lw/gu1/decision_theory_faq/\" target=\"_self\">Decision Theory FAQ</a>.</p>
</div>
<div mc:repeatable=\"repeat_1\" mc:repeatindex=\"5\" style=\"color: #4a535d; font-family: sans-serif; font-size: 14px; line-height: 1.5em; text-align: left; border-bottom: solid 1px #e5e5e5; padding: 13px 24px 10px 23px;\">
<h2 class=\"h2 title\" style=\"color: #003b65; display: block; font-family: sans-serif; font-size: 22px; font-weight: bold; line-height: 100%; text-align: left; padding-top: 5px; margin-top: 0 !important; margin-right: 0 !important; margin-bottom: 10px !important; margin-left: 0 !important;\"><span class=\"mc-toc-title\">AGI Impacts Experts and Friendly AI Experts</span></h2>
<p>In <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/2013/05/01/agi-impacts-experts-and-friendly-ai-experts/\" target=\"_self\">AGI Impacts Experts and Friendly AI Experts</a>, Luke Muehlhauser explains the two types of experts MIRI hopes to cultivate.</p>
<p><em>AGI impacts experts</em> develop skills related to predicting technological development (e.g. building <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/files/ChangingTheFrame.pdf\">computational models</a> of AI development or reasoning about <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/files/IEM.pdf\">intelligence explosion microeconomics</a>), predicting AGI’s likely impacts on society, and identifying which interventions are most likely to increase humanity’s chances of safely navigating the creation of AGI. For overviews, see <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/files/EthicsofAI.pdf\">Bostrom & Yudkowsky (2013)</a>; <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/files/IE-EI.pdf\">Muehlhauser & Salamon (2013)</a>.</p>
<p><em>Friendly AI experts</em> develop skills useful for the development of mathematical architectures that can enable AGIs to be <em>trustworthy</em> (or “human-friendly”). This work is carried out at <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/2013/03/07/upcoming-miri-research-workshops/\">MIRI research workshops</a> and in various publications, e.g. <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://lesswrong.com/lw/h1k/reflection_in_probabilistic_set_theory/\">Christiano et al. (2013)</a>; <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://arxiv.org/pdf/1111.3934v2.pdf\">Hibbard (2013)</a>. Note that the term “Friendly AI” was selected (in part) to avoid the suggestion that we understand the subject very well — a phrase like “Ethical AI” might sound like the kind of thing one can learn a lot about by looking it up in an encyclopedia, but our present understanding of trustworthy AI is too impoverished for that.</p>
<p>For more details on which skills these kinds of experts should develop, <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/2013/05/01/agi-impacts-experts-and-friendly-ai-experts/\" target=\"_self\">read the blog post</a>.</p>
</div>
<div mc:repeatable=\"repeat_1\" mc:repeatindex=\"6\" style=\"color: #4a535d; font-family: sans-serif; font-size: 14px; line-height: 1.5em; text-align: left; border-bottom: solid 1px #e5e5e5; padding: 13px 24px 10px 23px;\">
<h2 class=\"h2 title\" style=\"color: #003b65; display: block; font-family: sans-serif; font-size: 22px; font-weight: bold; line-height: 100%; text-align: left; padding-top: 5px; margin-top: 0 !important; margin-right: 0 !important; margin-bottom: 10px !important; margin-left: 0 !important;\"><span class=\"mc-toc-title\">MIRI’s Mission in Five Theses and Two Lemmas</span></h2>
<p>Yudkowsky sums up MIRI’s research mission in the blog post <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/2013/05/05/five-theses-two-lemmas-and-a-couple-of-strategic-implications/\" target=\"_self\">Five theses, two lemmas, and a couple of strategic implications</a>. The five theses are:</p>
<ul>
<li>Intelligence explosion</li>
<li>Orthogonality of intelligence and goals</li>
<li>Convergent instrumental goals</li>
<li>Complexity of value</li>
<li>Fragility of value</li>
</ul>
<p>According to Yudkowsky, these theses imply two important lemmas:</p>
<ul>
<li>Indirect normativity</li>
<li>Large bounded extra difficulty of Friendliness</li>
</ul>
<p>In turn, these two lemmas have two important strategic implications:</p>
<ol>
<li>We have a lot of work to do on things like indirect normativity and stable self-improvement. At this stage a lot of this work looks really foundational — that is, we can’t describe how to do these things using infinite computing power, let alone finite computing power.  We should get started on this work as early as possible, since basic research often takes a lot of time.</li>
<li>There needs to be a Friendly AI project that has some sort of boost over competing projects which don’t live up to a (very) high standard of Friendly AI work — a project which can successfully build a stable-goal-system self-improving AI, before a less-well-funded project hacks together a much sloppier self-improving AI. Giant supercomputers may be less important to this than being able to bring together the smartest researchers… but the required advantage cannot be left up to chance. Leaving things to default means that projects less careful about self-modification would have an advantage greater than casual altruism is likely to overcome.</li>
</ol>
<p>For more details on the theses and lemmas, <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/2013/05/05/five-theses-two-lemmas-and-a-couple-of-strategic-implications/\" target=\"_self\">read the blog post</a> and its linked articles.</p>
</div>
<div mc:repeatable=\"repeat_1\" mc:repeatindex=\"7\" style=\"color: #4a535d; font-family: sans-serif; font-size: 14px; line-height: 1.5em; text-align: left; border-bottom: solid 1px #e5e5e5; padding: 13px 24px 10px 23px;\">
<h2 class=\"h2 title\" style=\"color: #003b65; display: block; font-family: sans-serif; font-size: 22px; font-weight: bold; line-height: 100%; text-align: left; padding-top: 5px; margin-top: 0 !important; margin-right: 0 !important; margin-bottom: 10px !important; margin-left: 0 !important;\"><span class=\"mc-toc-title\">Our Final Invention available for preorder</span></h2>
<p><a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://www.amazon.com/Our-Final-Invention-Artificial-Intelligence/dp/0312622376/\"><img style=\"max-width: 160px; height: auto; line-height: 100%; outline: none; padding: 1px; text-decoration: none; display: inline; width: 131px; border: 1px; border-style: solid; border-width: 1px; border-color: Black;\" alt=\"\" src=\"http://ecx.images-amazon.com/images/I/71t4FkxPNqL._SL1500_.jpg\" width=\"131\" height=\"200\" align=\"right\" /></a><a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://www.jamesbarrat.com/author/\" target=\"_self\">James Barrat</a>, a documentary filmmaker for National Geographic, Discovery, PBS, and other broadcasters, has written a wonderful new book about the intelligence explosion called <em>Our Final Invention: Artificial Intelligence and the End of the Human Era</em>. It will be released October 1st, and is <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://www.amazon.com/Our-Final-Invention-Artificial-Intelligence/dp/0312622376/?tag=r601000000-20\" target=\"_self\">available for pre-order on Amazon</a>.</p>
<p>Here are some blurbs from people who have read an advance copy:</p>
<p>“A hard-hitting book about the most important topic of this century and possibly beyond — the issue of whether our species can survive. I wish it was science fiction but I know it’s not.”</p>
<p>—Jaan Tallinn, co-founder of Skype</p>
<p>“The compelling story of humanity’s most critical challenge. A Silent Spring for the 21st Century.”</p>
<p>—Michael Vassar, former MIRI president</p>
<p>“<em>Our Final Invention</em> is a thrilling detective story, and also the best book yet written on the most important problem of the 21st century.”</p>
<p>—Luke Muehlhauser, MIRI executive director</p>
<p>“An important and disturbing book.”</p>
<p>—Huw Price, co-founder, Cambridge University Center for the Study of Existential Risk</p>
</div>
<div mc:repeatable=\"repeat_1\" mc:repeatindex=\"8\" style=\"color: #4a535d; font-family: sans-serif; font-size: 14px; line-height: 1.5em; text-align: left; border-bottom: solid 1px #e5e5e5; padding: 13px 24px 10px 23px;\">
<h2 class=\"h2 title\" style=\"color: #003b65; display: block; font-family: sans-serif; font-size: 22px; font-weight: bold; line-height: 100%; text-align: left; padding-top: 5px; margin-top: 0 !important; margin-right: 0 !important; margin-bottom: 10px !important; margin-left: 0 !important;\"><span class=\"mc-toc-title\">MIRI Needs Advisors</span></h2>
<p>MIRI currently has a few dozen volunteer advisors on a wide range of subjects, but we need more! We’re especially hoping for additional advisors in mathematical logic, theoretical computer science, artificial intelligence, economics, and game theory.</p>
<p>If you <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/2013/05/15/advise-miri-with-your-domain-specific-expertise/\" target=\"_self\"><strong>sign up</strong></a>, we will occasionally ask you questions, or send you early drafts of upcoming writings for feedback.</p>
<p>We don’t always want technical advice (“Well, you can do that with a relativized arithmetical hierarchy…”); often, we just want to understand how different groups of experts respond to our writing (“The tone of this paragraph rubs me the wrong way because…”).</p>
<p>Even if you don’t have much time to help, <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/2013/05/15/advise-miri-with-your-domain-specific-expertise/\" target=\"_self\"><strong>please sign up</strong></a>! We will of course respect your own limits on availability.</p>
</div>
<div mc:repeatable=\"repeat_1\" mc:repeatindex=\"9\" style=\"color: #4a535d; font-family: sans-serif; font-size: 14px; line-height: 1.5em; text-align: left; border-bottom: solid 1px #e5e5e5; padding: 13px 24px 10px 23px;\">
<h2 class=\"h2 title\" style=\"color: #003b65; display: block; font-family: sans-serif; font-size: 22px; font-weight: bold; line-height: 100%; text-align: left; padding-top: 5px; margin-top: 0 !important; margin-right: 0 !important; margin-bottom: 10px !important; margin-left: 0 !important;\">Featured Volunteer – Florian Blumm</h2>
<p><img style=\"max-width: 160px; height: auto; line-height: 100%; outline: none; padding: 1px; text-decoration: none; display: inline; width: 184px; border: 1px; border-style: solid; border-width: 1px; border-color: Black;\" alt=\"\" src=\"http://gallery.mailchimp.com/353906382677fa789a483ba9e/images/tauchen2.jpg\" width=\"184\" height=\"200\" align=\"right\" />Florian Bumm has been one of our most active translators. Florian translates materials from English into his native tongue, German. Historically a software engineer, he is now on a traveling vacation in Bolivia progressively extended by remote contract labor, which he has found conducive to his volunteering for MIRI. After leaving a position as a Java engineer for a financial services company, he has decided that he would rather contribute directly to a cause of some sort, and has determined that there is nothing more important than mitigating existential risks from artificial intelligence.</p>
<p>Thanks, Florian!</p>
<p>To join Florian and dozens of other volunteers, visit <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://mirivolunteers.org/\" target=\"_self\">MIRIvolunteers.org</a>.</p>
</div>
</td>
</tr>
</tbody>
</table>
<p><!-- // End Module: Standard Content \\\\ --></td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>
<p><!-- // End Template Body \\\\ --></td>
</tr>
<tr>
<td style=\"border-collapse: collapse;\" align=\"center\" valign=\"top\"><!-- // Begin Template Footer \\\\ --></p>
<table id=\"templateFooter\" style=\"background-color: #aeb6c3; border-top: 0;\" width=\"575\" border=\"0\" cellspacing=\"0\" cellpadding=\"10\">
<tbody>
<tr>
<td class=\"footerContent\" style=\"border-collapse: collapse;\" valign=\"top\"><!-- // Begin Module: Standard Footer \\\\ --></p>
<table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"10\">
<tbody>
<tr>
<td id=\"social\" style=\"border-collapse: collapse; background-color: #aeb6c3; border: 0;\" colspan=\"2\" valign=\"middle\"></td>
</tr>
<tr>
<td style=\"border-collapse: collapse;\" valign=\"top\" width=\"350\">
<div style=\"color: #707070; font-family: Arial; font-size: 12px; line-height: 125%; text-align: left;\">
<p>You’re receiving this because you have subscribed to content from the Machine Intelligence Research Institute.</p>
<p><em>Copyright © Machine Intelligence Research Institute, All rights reserved.</em></p>
</div>
</td>
<td id=\"monkeyRewards\" style=\"border-collapse: collapse;\" valign=\"top\" width=\"190\"></td>
</tr>
<tr>
<td id=\"utility\" style=\"border-collapse: collapse; background-color: #aeb6c3; border: 0;\" colspan=\"2\" valign=\"middle\"></td>
</tr>
</tbody>
</table>
<p><!-- // End Module: Standard Footer \\\\ --></td>
</tr>
</tbody>
</table>
<p><!-- // End Template Footer \\\\ --></td>
</tr>
</tbody>
</table>
<p>The post <a href=\"http://intelligence.org/2013/05/30/miri-may-newsletter-intelligence-explosion-microeconomics-and-other-publications/\">MIRI May Newsletter: Intelligence Explosion Microeconomics and Other Publications</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/05/30/miri-may-newsletter-intelligence-explosion-microeconomics-and-other-publications/#comments" "7cc602d3bd8b10e26c4cbe521833f0ed") (1 (20954 62970 744909) "http://intelligence.org/2013/05/29/new-transcript-yudkowsky-and-aaronson/?utm_source=rss&utm_medium=rss&utm_campaign=new-transcript-yudkowsky-and-aaronson" "New Transcript: Yudkowsky and Aaronson" "Luke Muehlhauser" "Thu, 30 May 2013 03:21:29 +0000" "<p><a href=\"https://docs.google.com/document/d/1JIqzTGNvdLukR0Ce5T2eiv_vxtPO53N3KGCbxVvREtU/pub\"><img class=\"aligncenter size-full wp-image-10214\" alt=\"Yudkowsky-Aaronson\" src=\"http://intelligence.org/wp-content/uploads/2013/05/Yudkowsky-Aaronson.png\" width=\"454\" height=\"289\" /></a></p>
<p>In <a href=\"http://intelligence.org/2013/05/15/when-will-ai-be-created/\">When Will AI Be Created?</a>, I referred to a <a href=\"http://bloggingheads.tv/videos/2220\">bloggingheads.tv conversation</a> between Eliezer Yudkowsky and Scott Aaronson. A transcript of that dialogue is <a href=\"https://docs.google.com/document/d/1JIqzTGNvdLukR0Ce5T2eiv_vxtPO53N3KGCbxVvREtU/pub\">now available</a>, thanks to MIRI volunteers Ethan Dickinson, Daniel Kokotajlo, and Rick Schwall.</p>
<p>See also <a href=\"http://intelligence.org/2013/01/09/new-transcript-eliezer-yudkowsky-and-massimo-pigliucci-on-the-singularity/\">the transcript</a> for a <a href=\"http://bloggingheads.tv/videos/2561\">bloggingheads.tv conversation</a> between Eliezer Yudkowsky and Massimo Pigliucci.</p>
<p>To join these volunteers in assisting our cause, visit <a href=\"http://mirivolunteers.org/\">MIRIvolunteers.org</a>!</p>
<p>The post <a href=\"http://intelligence.org/2013/05/29/new-transcript-yudkowsky-and-aaronson/\">New Transcript: Yudkowsky and Aaronson</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/05/29/new-transcript-yudkowsky-and-aaronson/#comments" "9eaa76e114204e134c9c3bbacd5c579e")))