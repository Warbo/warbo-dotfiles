;; -*- coding: utf-8-emacs; -*-
(setq nnrss-group-data '((33 (21009 53374 154540) "http://intelligence.org/2013/08/16/luke-at-quixey-on-tuesday-aug-20th/?utm_source=rss&utm_medium=rss&utm_campaign=luke-at-quixey-on-tuesday-aug-20th" "Luke at Quixey on Tuesday (Aug. 20th)" "staff" "Fri, 16 Aug 2013 18:03:01 +0000" "<img class=\"aligncenter size-full wp-image-10399\" alt=\"EA & EotW\" src=\"http://intelligence.org/wp-content/uploads/2013/08/EA-EotW.png\" />
<p>This coming Tuesday, MIRI’s Executive Director Luke Muehlhauser will give a talk at <a href=\"https://www.quixey.com/\">Quixey</a> titled <strong>Effective Altruism and the End of the World</strong>. If you’re in or near the South Bay, you should come! Snacks will be provided.</p>
<p><em>Time</em>: Tuesday, August 20th. Doors open at 7:30pm. Talk starts at 8pm. Q&A starts at 8:30pm.</p>
<p><em>Place</em>: Quixey Headquarters, 278 Castro St., Mountain View, CA. (<a href=\"https://www.google.com/maps/preview#!q=278+Castro+St.%2C+Mountain+View%2C+CA&data=!4m10!1m9!4m8!1m3!1d51377!2d-122.30098!3d37.8707754!3m2!1i1319!2i783!4f13.1\">Google Maps</a>)</p>
<p><em>Entrance<strong>:</strong></em> You cannot enter Quixey from Castro St. Instead, please enter through the back door, from the parking lot at the corner of Dana & Bryant.</p>
<p>The post <a href=\"http://intelligence.org/2013/08/16/luke-at-quixey-on-tuesday-aug-20th/\">Luke at Quixey on Tuesday (Aug. 20th)</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/08/16/luke-at-quixey-on-tuesday-aug-20th/#comments" "bd3d7a4a41a10e4037470a1e5bc16676") (32 (21003 43478 117143) "http://intelligence.org/2013/08/11/what-is-agi/?utm_source=rss&utm_medium=rss&utm_campaign=what-is-agi" "What is AGI?" "Luke Muehlhauser" "Sun, 11 Aug 2013 18:32:36 +0000" "<p><img class=\"alignright shadowed size-full wp-image-10389\" alt=\"android looking up\" src=\"http://intelligence.org/wp-content/uploads/2013/08/android-looking-up.jpg\" />One of the most common objections we hear when talking about artificial general intelligence (AGI) is that “AGI is ill-defined, so you can’t really say much about it.”</p>
<p>In an <a href=\"http://intelligence.org/2013/06/19/what-is-intelligence-2/\">earlier post</a>, I pointed out that we <em>often</em> don’t have precise definitions for things while doing useful work on them, as was the case with the concepts of “number” and “self-driving car.”</p>
<p>Still, we must have <em>some</em> idea of what we’re talking about. <a href=\"http://intelligence.org/2013/06/19/what-is-intelligence-2/\">Earlier</a> I gave a rough working definition for “intelligence.” In this post, I explain the concept of AGI and also provide several possible <a href=\"http://en.wikipedia.org/wiki/Operational_definition\">operational definitions</a> for the idea.</p>
<h3>The idea of AGI</h3>
<p>As discussed earlier, the concept of “general intelligence” refers to the capacity for <a href=\"http://intelligence.org/2013/06/19/what-is-intelligence-2/\">efficient <em>cross-domain</em> optimization</a>. Or as Ben Goertzel likes to <a href=\"http://agi-school.org/2009/lecture-01\">say</a>, “the ability to achieve complex goals in complex environments using limited computational resources.” Another idea often associated with general intelligence is the ability to transfer learning from one domain to other domains.</p>
<p>To illustrate this idea, let’s consider something that would <em>not</em> count as a general intelligence.</p>
<p>Computers <a href=\"http://en.wikipedia.org/wiki/Progress_in_artificial_intelligence#Performance_evaluation\">show</a> vastly superhuman performance at some tasks, roughly human-level performance at other tasks, and subhuman performance at still other tasks. If a team of researchers was able to combine many of the top-performing “<a href=\"http://agi-school.org/2009/lecture-01\">narrow AI</a>” algorithms into one system, as Google may be trying to do,<sup>1</sup> they’d have a massive “Kludge AI” that was terrible at most tasks, mediocre at some tasks, and superhuman at a few tasks.</p>
<p>Like the Kludge AI, particular humans are terrible or mediocre at most tasks, and far better than average at just a few tasks.<sup>2</sup> Another similarity is that the Kludge AI would probably show measured correlations between many different narrow cognitive abilities, just as humans do (hence the concepts of <em>g</em> and IQ<sup>3</sup>): if we gave the Kludge AI lots more hardware, it could use that hardware to improve its performance in many different narrow domains simultaneously.<sup>4</sup></p>
<p>On the other hand, the Kludge AI would not (yet) have <em>general intelligence</em>, because it wouldn’t necessarily have the capacity to solve somewhat-arbitrary problems in somewhat-arbitrary environments, wouldn’t necessarily be able to transfer learning in one domain to another, and so on.</p>
<p><span id=\"more-10388\"></span></p>
<h3>Operational definitions of AGI</h3>
<p>Can we be more specific? This idea of general intelligence <em>is</em> difficult to operationalize. Below I consider four operational definitions for AGI, in (apparent) increasing order of difficulty.</p>
<h4>The Turing test ($100,000 Loebner prize interpretation)</h4>
<p>The <a href=\"http://en.wikipedia.org/wiki/Turing_test\">Turing test</a> was proposed in <a href=\"http://orium.homelinux.org/paper/turingai.pdf\">Turing (1950)</a>, and has many interpretations (<a href=\"http://www.amazon.com/Turing-Test-Artificial-Intelligence-Cognitive/dp/1402012047/\">Moor 2003</a>).</p>
<p>One specific interpretation is provided by the conditions for winning the <a href=\"http://en.wikipedia.org/wiki/Loebner_prize\">$100,000 Loebner Prize</a>. Since 1990, <a href=\"http://en.wikipedia.org/wiki/Hugh_Loebner\">Hugh Loebner</a> has offered $100,000 to the first AI program to pass this test at the annual <a href=\"http://www.loebner.net/Prizef/loebner-prize.html\">Loebner Prize competition</a>. Smaller prizes are given to the best-performing AI program each year, but no program has performed well enough to win the $100,000 prize.</p>
<p>The <em>exact</em> conditions for winning the $100,000 prize will not be defined until a program wins the $25,000 “silver” prize, which has not yet been done. However, we do know the conditions will look <em>something</em> like this: A program will win the $100,000 if it can fool half the judges into thinking it is human while interacting with them in a freeform conversation for 30 minutes <em>and</em> interpreting audio-visual input.</p>
<h4>The coffee test</h4>
<p><a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/06/Goertzel-et-al.-The-Architecture-of-Human-Like-General-Intelligence.pdf\">Goertzel et al. (2012)</a> suggest a (probably) more difficult test — the “coffee test” — as a potential operational definition for AGI:</p>
<blockquote><p>go into an average American house and figure out how to make coffee, including identifying the coffee machine, figuring out what the buttons do, finding the coffee in the cabinet, etc.</p></blockquote>
<p>If a robot could do that, perhaps we should consider it to have general intelligence.<sup>5</sup></p>
<h4>The robot college student test</h4>
<p><a href=\"http://www.newscientist.com/article/mg21528813.600-what-counts-as-a-conscious-thinking-machine.html\">Goertzel (2012)</a> suggests a (probably) more challenging operational definition, the “robot college student test”:</p>
<blockquote><p>when a robot can enrol in a human university and take classes in the same way as humans, and get its degree, then I’ll [say] we’ve created [an]… artificial general intelligence.</p></blockquote>
<h4>The employment test</h4>
<p><a href=\"http://ai.stanford.edu/~nilsson/\">Nils Nilsson</a>, one AI’s founding researchers, once suggested an even more demanding operational definition for “human-level AI” (what I’ve been calling AGI), the <a href=\"http://ai.stanford.edu/~nilsson/OnlinePubs-Nils/General%20Essays/AIMag26-04-HLAI.pdf\">employment test</a>:</p>
<blockquote><p>Machines exhibiting true human-level intelligence should be able to do many of the things humans are able to do. Among these activities are the tasks or “jobs” at which people are employed. I suggest we replace the Turing test by something I will call the “employment test.” To pass the employment test, AI programs must… [have] at least the <em>potential</em> [to completely automate] economically important jobs.<sup>6</sup></p></blockquote>
<p>To develop this operational definition more completely, one could provide a canonical list of “economically important jobs,” produce a special <a href=\"http://www.studyguidezone.com/vocational_exams.htm\">vocational exam</a> for each job (e.g. both the written and driving exams required for a U.S. <a href=\"http://www.fmcsa.dot.gov/registration-licensing/cdl/cdl.htm\">commercial driver’s license</a>), and measure machines’ performance on those vocational exams.</p>
<p>This is a bit “unfair” because I doubt that any <em>single</em> human could pass such vocational exams for any long list of economically important jobs. On the other hand, it’s quite possible that many unusually skilled humans would be able to pass all or nearly all such vocational exams if they spent an entire lifetime training each skill, and an AGI — having near-perfect memory, faster thinking speed, no need for sleep, etc. — would presumably be able to train itself in all required skills much more quickly, <em>if</em> it possessed the kind of general intelligence we’re trying to operationally define.</p>
<h3>The future is foggy</h3>
<p>One or more of these operational definitions for AGI might seem compelling, but a look at history should teach us some humility.</p>
<p>Decades ago, several leading AI scientists seemed to think that human-level performance at <em>chess</em> could represent an achievement of AGI-proportions. Here are <a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/07/Newell-et-al-Chess-playing-programs-and-the-problem-of-complexity.pdf\">Newell et al. (1958)</a>:</p>
<blockquote><p>Chess is the intellectual game <em>par excellence</em>… If one could devise a successful chess machine, one would seem to have penetrated to the core of human intellectual endeavor.<sup>7</sup></p></blockquote>
<p>As late as 1976, I.J. Good <a href=\"http://intelligence.org/wp-content/uploads/2013/05/Good-Review-of-The-World-Computer-Chess-Championship.pdf\">asserted</a> that human-level performance in computer chess was a good signpost for AGI, writing that “a computer program of Grandmaster strength would bring us within an ace of [machine ultra-intelligence].”</p>
<p>But machines surpassed the best human chess players about 15 years ago, and we still seem to be several decades away from AGI.</p>
<p>The surprising success of self-driving cars may offer another lesson in humility. Had I been an AI scientist in the 1960s, I might well have thought that a self-driving car as capable as <a href=\"http://en.wikipedia.org/wiki/Google_driverless_car\">Google’s driverless car</a> would indicate the arrival of AGI. After all, a self-driving car must act with high autonomy, at high speeds, in an extremely complex, dynamic, and uncertain environment: namely, the real world. It must also (on rare occasions) face genuine moral dilemmas such as the philosopher’s <a href=\"http://craigweich.com/post/36670778407/machine-ethics-and-the-trolley-problem-as\">trolley problem</a>. Instead, Google built its driverless car with a series of “cheats” I might not have conceived of in the 1960s — for example by mapping with high precision almost every road, freeway on-ramp, and parking lot in the country <em>before</em> it built its driverless car.</p>
<h3>Conclusion</h3>
<p>So, what’s a good operational definition for AGI? I personally lean toward Nilsson’s employment test, but <em>you</em> might have something else in mind when you talk about AGI.</p>
<p>I expect to pick a new working definition sometime in the next 20 years, as AGI draws nearer, but Nilsson’s operationalization will do for now.</p>
<h4></h4>
<h4>Notes</h4>
<p><small>My thanks to Carl Shulman, Ben Goertzel, and Eliezer Yudkowsky for their feedback on this post.</small></p>
<p><sup>1</sup> <small>In an <a href=\"http://www.theregister.co.uk/2013/05/17/google_ai_hogwash/\">interview</a> with <em>The Register</em>, Google head of research <a href=\"http://en.wikipedia.org/wiki/Alfred_Spector\">Alfred Spector</a> said, “We have the knowledge graph, [the] ability to parse natural language, neural network tech [and] enormous opportunities to gain feedback from users… If we combine all these things together with humans in the loop continually providing feedback our systems become … intelligent.” Spector calls this the “combination hypothesis.”</small></p>
<p><sup>2</sup> <small>Though, there are probably many disadvantaged humans for which this is not true, because they do not show far-above-average performance on <em>any</em> tasks.</small></p>
<p><sup>3</sup> <small>Psychologists now generally agree that there is a general intelligence factor in addition to more specific mental abilities. For an introduction to the modern synthesis, see <a href=\"http://www.newscientist.com/data/doc/article/dn19554/instant_expert_13_-_intelligence.pdf\">Gottfredson (2011)</a>. For more detail, see the first few chapters of <a href=\"http://www.amazon.com/Cambridge-Handbook-Intelligence-Handbooks-Psychology/dp/052173911X/\">Sternberg & Kaufman (2011)</a>. If you’ve read Cosma Shalizi’s popular article “<a href=\"http://vserver1.cscs.lsa.umich.edu/~crshalizi/weblog/523.html\"><em>g</em>, a Statistical Myth</a>, please also read its refutation <a href=\"http://humanvarieties.org/2013/04/03/is-psychometric-g-a-myth/\">here</a> and <a href=\"http://humanvarieties.org/2013/04/14/some-further-notes-on-g-and-shalizi/\">here</a>.</small></p>
<p><sup>4</sup> <small>In psychology, the factor analysis is done <em>between humans</em>. Here, I’m suggesting that a similar factor analysis could hypothetically be done <em>between different Kludge AIs</em>, with different Kludge AIs running basically the same software but having access to different amounts of computation. The analogy should not be taken too far, however. For example, it isn’t the case that higher-IQ humans have much larger brains than other humans.</small></p>
<p><sup>5</sup> <small>The coffee test was inspired by Steve Wozniak’s prediction that we would never “build a robot that could walk into an unfamiliar house and make a cup of coffee” (<a href=\"http://www.cse.buffalo.edu/faculty/shapiro/Papers/hlai.pdf\">Adams et al. 2011</a>). Wozniak’s original prediction was made in a <em>PC World</em> piece from July 19, 2007 called <a href=\"http://www.pcworld.com/article/134826/article.html\">Three Minutes with Steve Wozniak</a>.</small></p>
<p><sup>6</sup> <small>First, Nilsson proposes that to pass the employment test, “AI programs must be able to perform the jobs ordinarily performed by humans.” But later, he modifies this specification: “For the purposes of the employment test, we can ﬁnesse the matter of whether or not human jobs are <em>actually</em> automated. Instead, I suggest, we can test whether or not we have the <em>capability</em> to automate them.” In part, he suggests this modification because “many of today’s jobs will likely disappear — just as manufacturing buggy whips did.”</small></p>
<p><sup>7</sup> <small>A bit later, they add a note of caution: “Now there might [be] a trick… something that [is] as the wheel to the human leg: a device quite different from humans in its methods, but supremely effective in its way, and perhaps very simple. Such a device might play excellent chess, but… fail to further our understanding of human intellectual processes. Such a prize, of course, would be worthy of discovery in its own right, but there are appears to be nothing of this sort in sight.”</small></p>
<p>The post <a href=\"http://intelligence.org/2013/08/11/what-is-agi/\">What is AGI?</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/08/11/what-is-agi/#comments" "1bf2b6670d9e43c5f5f3c0a121c92e75") (31 (21003 43478 110222) "http://intelligence.org/2013/07/24/miris-december-2013-workshop/?utm_source=rss&utm_medium=rss&utm_campaign=miris-december-2013-workshop" "=?utf-8?Q?MIRI=E2=80=99s?= December 2013 Workshop" "Luke Muehlhauser" "Wed, 24 Jul 2013 23:55:35 +0000" "<img class=\"alignright shadowed size-full wp-image-10010\" alt=\"013\" src=\"http://intelligence.org/wp-content/uploads/2013/02/013.jpg\" />
<p>From December 14-20, MIRI will host another <strong>Workshop on Logic, Probability, and Reflection</strong>. This workshop will focus on the <a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/\">Löbian obstacle</a>, <a href=\"http://lesswrong.com/lw/h1k/reflection_in_probabilistic_logic/\">probabilistic logic</a>, and the intersection of logic and probability more generally.</p>
<p>Participants confirmed so far include:</p>
<ul>
<li><a href=\"http://www.math.harvard.edu/~nate/\">Nate Ackerman</a> (Harvard)</li>
<li><a href=\"http://www.math.ucr.edu/home/baez/\">John Baez</a> (UC Riverside)</li>
<li><a href=\"http://rationalaltruist.com/\">Paul Christiano</a> (UC Berkeley)</li>
<li><a href=\"http://lesswrong.com/user/Benja/submitted/\">Benja Fallenstein</a> (U Bristol)</li>
<li><a href=\"http://math.mit.edu/~freer/\">Cameron Freer</a> (MIT)</li>
<li>Jeremy Hahn (Harvard)</li>
<li><a href=\"http://wojtek.moczydlowski.net/\">Wojtek Moczydlowski</a> (Google)</li>
<li><a href=\"http://www.ctpost.com/news/article/Wisdom-beyond-his-years-1390299.php\">Will Sawin</a> (Princeton)</li>
<li><a href=\"http://gregorywheeler.org/\">Greg Wheeler</a> (CMU)</li>
<li><a href=\"http://yudkowsky.net/\">Eliezer Yudkowsky</a> (MIRI)</li>
</ul>
<p>If you have a strong mathematics background and might like to attend this workshop, it’s not too late to <a href=\"http://intelligence.org/get-involved/#workshop\">apply</a>! And even if <em>this</em> workshop doesn’t fit your schedule, please <strong>do apply</strong>, so that we can notify you of other workshops (long before they are announced publicly).</p>
<p>The post <a href=\"http://intelligence.org/2013/07/24/miris-december-2013-workshop/\">MIRI’s December 2013 Workshop</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/07/24/miris-december-2013-workshop/#comments" "9c0282fbe7b9184cba213e6473773499") (30 (21002 13206 205505) "http://intelligence.org/2013/08/11/what-is-agi/?utm_source=rss&utm_medium=rss&utm_campaign=what-is-agi" "What is AGI?" "Luke Muehlhauser" "Sun, 11 Aug 2013 18:32:36 +0000" "<p><img class=\"alignright size-full wp-image-10389\" alt=\"android looking up\" src=\"http://intelligence.org/wp-content/uploads/2013/08/android-looking-up.jpg\" />One of the most common objections we hear when talking about artificial general intelligence (AGI) is that “AGI is ill-defined, so you can’t really say much about it.”</p>
<p>In an <a href=\"http://intelligence.org/2013/06/19/what-is-intelligence-2/\">earlier post</a>, I pointed out that we <em>often</em> don’t have precise definitions for things while doing useful work on them, as was the case with the concepts of “number” and “self-driving car.”</p>
<p>Still, we must have <em>some</em> idea of what we’re talking about. <a href=\"http://intelligence.org/2013/06/19/what-is-intelligence-2/\">Earlier</a> I gave a rough working definition for “intelligence.” In this post, I explain the concept of AGI and also provide several possible <a href=\"http://en.wikipedia.org/wiki/Operational_definition\">operational definitions</a> for the idea.</p>
<h3>The idea of AGI</h3>
<p>As discussed earlier, the concept of “general intelligence” refers to the capacity for <a href=\"http://intelligence.org/2013/06/19/what-is-intelligence-2/\">efficient <em>cross-domain</em> optimization</a>. Or as Ben Goertzel likes to <a href=\"http://agi-school.org/2009/lecture-01\">say</a>, “the ability to achieve complex goals in complex environments using limited computational resources.” Another idea often associated with general intelligence is the ability to transfer learning from one domain to other domains.</p>
<p>To illustrate this idea, let’s consider something that would <em>not</em> count as a general intelligence.</p>
<p>Computers <a href=\"http://en.wikipedia.org/wiki/Progress_in_artificial_intelligence#Performance_evaluation\">show</a> vastly superhuman performance at some tasks, roughly human-level performance at other tasks, and subhuman performance at still other tasks. If a team of researchers was able to combine many of the top-performing “<a href=\"http://agi-school.org/2009/lecture-01\">narrow AI</a>” algorithms into one system, as Google may be trying to do,<sup>1</sup> they’d have a massive “Kludge AI” that was terrible at most tasks, mediocre at some tasks, and superhuman at a few tasks.</p>
<p>Like the Kludge AI, particular humans are terrible or mediocre at most tasks, and far better than average at just a few tasks.<sup>2</sup> Another similarity is that the Kludge AI would probably show measured correlations between many different narrow cognitive abilities, just as humans do (hence the concepts of <em>g</em> and IQ<sup>3</sup>): if we gave the Kludge AI lots more hardware, it could use that hardware to improve its performance in many different narrow domains simultaneously.<sup>4</sup></p>
<p>On the other hand, the Kludge AI would not (yet) have <em>general intelligence</em>, because it wouldn’t necessarily have the capacity to solve somewhat-arbitrary problems in somewhat-arbitrary environments, wouldn’t necessarily be able to transfer learning in one domain to another, and so on.</p>
<p><span id=\"more-10388\"></span></p>
<h3>Operational definitions of AGI</h3>
<p>Can we be more specific? This idea of general intelligence <em>is</em> difficult to operationalize. Below I consider four operational definitions for AGI, in (apparent) increasing order of difficulty.</p>
<h4>The Turing test ($100,000 Loebner prize interpretation)</h4>
<p>The <a href=\"http://en.wikipedia.org/wiki/Turing_test\">Turing test</a> was proposed in <a href=\"http://orium.homelinux.org/paper/turingai.pdf\">Turing (1950)</a>, and has many interpretations (<a href=\"http://www.amazon.com/Turing-Test-Artificial-Intelligence-Cognitive/dp/1402012047/\">Moor 2003</a>).</p>
<p>One specific interpretation is provided by the conditions for winning the <a href=\"http://en.wikipedia.org/wiki/Loebner_prize\">$100,000 Loebner Prize</a>. Since 1990, <a href=\"http://en.wikipedia.org/wiki/Hugh_Loebner\">Hugh Loebner</a> has offered $100,000 to the first AI program to pass this test at the annual <a href=\"http://www.loebner.net/Prizef/loebner-prize.html\">Loebner Prize competition</a>. Smaller prizes are given to the best-performing AI program each year, but no program has performed well enough to win the $100,000 prize.</p>
<p>The <em>exact</em> conditions for winning the $100,000 prize will not be defined until a program wins the $25,000 “silver” prize, which has not yet been done. However, we do know the conditions will look <em>something</em> like this: A program will win the $100,000 if it can fool half the judges into thinking it is human while interacting with them in a freeform conversation for 30 minutes <em>and</em> interpreting audio-visual input.</p>
<h4>The coffee test</h4>
<p><a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/06/Goertzel-et-al.-The-Architecture-of-Human-Like-General-Intelligence.pdf\">Goertzel et al. (2012)</a> suggest a (probably) more difficult test — the “coffee test” — as a potential operational definition for AGI:</p>
<blockquote><p>go into an average American house and figure out how to make coffee, including identifying the coffee machine, figuring out what the buttons do, finding the coffee in the cabinet, etc.</p></blockquote>
<p>If a robot could do that, perhaps we should consider it to have general intelligence.<sup>5</sup></p>
<h4>The robot college student test</h4>
<p><a href=\"http://www.newscientist.com/article/mg21528813.600-what-counts-as-a-conscious-thinking-machine.html\">Goertzel (2012)</a> suggests a (probably) more challenging operational definition, the “robot college student test”:</p>
<blockquote><p>when a robot can enrol in a human university and take classes in the same way as humans, and get its degree, then I’ll [say] we’ve created [an]… artificial general intelligence.</p></blockquote>
<h4>The employment test</h4>
<p><a href=\"http://ai.stanford.edu/~nilsson/\">Nils Nilsson</a>, one AI’s founding researchers, once suggested an even more demanding operational definition for “human-level AI” (what I’ve been calling AGI), the <a href=\"http://ai.stanford.edu/~nilsson/OnlinePubs-Nils/General%20Essays/AIMag26-04-HLAI.pdf\">employment test</a>:</p>
<blockquote><p>Machines exhibiting true human-level intelligence should be able to do many of the things humans are able to do. Among these activities are the tasks or “jobs” at which people are employed. I suggest we replace the Turing test by something I will call the “employment test.” To pass the employment test, AI programs must… [have] at least the <em>potential</em> [to completely automate] economically important jobs.<sup>6</sup></p></blockquote>
<p>To develop this operational definition more completely, one could provide a canonical list of “economically important jobs,” produce a special <a href=\"http://www.studyguidezone.com/vocational_exams.htm\">vocational exam</a> for each job (e.g. both the written and driving exams required for a U.S. <a href=\"http://www.fmcsa.dot.gov/registration-licensing/cdl/cdl.htm\">commercial driver’s license</a>), and measure machines’ performance on those vocational exams.</p>
<p>This is a bit “unfair” because I doubt that any <em>single</em> human could pass such vocational exams for any long list of economically important jobs. On the other hand, it’s quite possible that many unusually skilled humans would be able to pass all or nearly all such vocational exams if they spent an entire lifetime training each skill, and an AGI — having near-perfect memory, faster thinking speed, no need for sleep, etc. — would presumably be able to train itself in all required skills much more quickly, <em>if</em> it possessed the kind of general intelligence we’re trying to operationally define.</p>
<h3>The future is foggy</h3>
<p>One or more of these operational definitions for AGI might seem compelling, but a look at history should teach us some humility.</p>
<p>Decades ago, several leading AI scientists seemed to think that human-level performance at <em>chess</em> could represent an achievement of AGI-proportions. Here are <a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/07/Newell-et-al-Chess-playing-programs-and-the-problem-of-complexity.pdf\">Newell et al. (1958)</a>:</p>
<blockquote><p>Chess is the intellectual game <em>par excellence</em>… If one could devise a successful chess machine, one would seem to have penetrated to the core of human intellectual endeavor.<sup>7</sup></p></blockquote>
<p>As late as 1976, I.J. Good <a href=\"http://intelligence.org/wp-content/uploads/2013/05/Good-Review-of-The-World-Computer-Chess-Championship.pdf\">asserted</a> that human-level performance in computer chess was a good signpost for AGI, writing that “a computer program of Grandmaster strength would bring us within an ace of [machine ultra-intelligence].”</p>
<p>But machines surpassed the best human chess players about 15 years ago, and we still seem to be several decades away from AGI.</p>
<p>The surprising success of self-driving cars may offer another lesson in humility. Had I been an AI scientist in the 1960s, I might well have thought that a self-driving car as capable as <a href=\"http://en.wikipedia.org/wiki/Google_driverless_car\">Google’s driverless car</a> would indicate the arrival of AGI. After all, a self-driving car must act with high autonomy, at high speeds, in an extremely complex, dynamic, and uncertain environment: namely, the real world. It must also (on rare occasions) face genuine moral dilemmas such as the philosopher’s <a href=\"http://craigweich.com/post/36670778407/machine-ethics-and-the-trolley-problem-as\">trolley problem</a>. Instead, Google built its driverless car with a series of “cheats” I might not have conceived of in the 1960s — for example by mapping with high precision almost every road, freeway on-ramp, and parking lot in the country <em>before</em> it built its driverless car.</p>
<h3>Conclusion</h3>
<p>So, what’s a good operational definition for AGI? I personally lean toward Nilsson’s employment test, but <em>you</em> might have something else in mind when you talk about AGI.</p>
<p>I expect to pick a new working definition sometime in the next 20 years, as AGI draws nearer, but Nilsson’s operationalization will do for now.</p>
<h4></h4>
<h4>Notes</h4>
<p><small>My thanks to Carl Shulman, Ben Goertzel, and Eliezer Yudkowsky for their feedback on this post.</small></p>
<p><sup>1</sup> <small>In an <a href=\"http://www.theregister.co.uk/2013/05/17/google_ai_hogwash/\">interview</a> with <em>The Register</em>, Google head of research <a href=\"http://en.wikipedia.org/wiki/Alfred_Spector\">Alfred Spector</a> said, “We have the knowledge graph, [the] ability to parse natural language, neural network tech [and] enormous opportunities to gain feedback from users… If we combine all these things together with humans in the loop continually providing feedback our systems become … intelligent.” Spector calls this the “combination hypothesis.”</small></p>
<p><sup>2</sup> <small>Though, there are probably many disadvantaged humans for which this is not true, because they do not show far-above-average performance on <em>any</em> tasks.</small></p>
<p><sup>3</sup> <small>Psychologists now generally agree that there is a general intelligence factor in addition to more specific mental abilities. For an introduction to the modern synthesis, see <a href=\"http://www.newscientist.com/data/doc/article/dn19554/instant_expert_13_-_intelligence.pdf\">Gottfredson (2011)</a>. For more detail, see the first few chapters of <a href=\"http://www.amazon.com/Cambridge-Handbook-Intelligence-Handbooks-Psychology/dp/052173911X/\">Sternberg & Kaufman (2011)</a>. If you’ve read Cosma Shalizi’s popular article “<a href=\"http://vserver1.cscs.lsa.umich.edu/~crshalizi/weblog/523.html\"><em>g</em>, a Statistical Myth</a>, please also read its refutation <a href=\"http://humanvarieties.org/2013/04/03/is-psychometric-g-a-myth/\">here</a> and <a href=\"http://humanvarieties.org/2013/04/14/some-further-notes-on-g-and-shalizi/\">here</a>.</small></p>
<p><sup>4</sup> <small>In psychology, the factor analysis is done <em>between humans</em>. Here, I’m suggesting that a similar factor analysis could hypothetically be done <em>between different Kludge AIs</em>, with different Kludge AIs running basically the same software but having access to different amounts of computation. The analogy should not be taken too far, however. For example, it isn’t the case that higher-IQ humans have much larger brains than other humans.</small></p>
<p><sup>5</sup> <small>The coffee test was inspired by Steve Wozniak’s prediction that we would never “build a robot that could walk into an unfamiliar house and make a cup of coffee” (<a href=\"http://www.cse.buffalo.edu/faculty/shapiro/Papers/hlai.pdf\">Adams et al. 2011</a>). Wozniak’s original prediction was made in a <em>PC World</em> piece from July 19, 2007 called <a href=\"http://www.pcworld.com/article/134826/article.html\">Three Minutes with Steve Wozniak</a>.</small></p>
<p><sup>6</sup> <small>First, Nilsson proposes that to pass the employment test, “AI programs must be able to perform the jobs ordinarily performed by humans.” But later, he modifies this specification: “For the purposes of the employment test, we can ﬁnesse the matter of whether or not human jobs are <em>actually</em> automated. Instead, I suggest, we can test whether or not we have the <em>capability</em> to automate them.” In part, he suggests this modification because “many of today’s jobs will likely disappear — just as manufacturing buggy whips did.”</small></p>
<p><sup>7</sup> <small>A bit later, they add a note of caution: “Now there might [be] a trick… something that [is] as the wheel to the human leg: a device quite different from humans in its methods, but supremely effective in its way, and perhaps very simple. Such a device might play excellent chess, but… fail to further our understanding of human intellectual processes. Such a prize, of course, would be worthy of discovery in its own right, but there are appears to be nothing of this sort in sight.”</small></p>
<p>The post <a href=\"http://intelligence.org/2013/08/11/what-is-agi/\">What is AGI?</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/08/11/what-is-agi/#comments" "0d85dd841985bdd242749f5602baaa92") (29 (20992 43957 728256) "http://intelligence.org/2013/08/04/benja-interview/?utm_source=rss&utm_medium=rss&utm_campaign=benja-interview" "Benja Fallenstein on the =?utf-8?Q?L=C3=B6bian?= Obstacle to Self-Modifying Systems" "Luke Muehlhauser" "Sun, 04 Aug 2013 19:53:10 +0000" "<div class=\"well\">
<p><img class=\"shadowed alignright\" alt=\"\" src=\"http://intelligence.org/wp-content/uploads/2013/02/associate_benja.png\" width=\"150\" height=\"206\" />Benja Fallenstein researches mathematical models of human and animal behavior at <a href=\"http://www.bris.ac.uk/\">Bristol University</a>, as part of the <a href=\"http://www.bristol.ac.uk/biology/research/behaviour/mad/\">MAD research group</a> and the <a href=\"http://www.bris.ac.uk/decisions-research/\">decision-making research group</a>.</p>
<p>Before that, he graduated from University of Vienna with a BSc in Mathematics. In his spare time, Benja studies questions relevant to AI impacts and Friendly AI, including: AI forecasting, intelligence explosion microeconomics, reflection in logic, and decision algorithms.</p>
<p>He has attended two of <a href=\"http://intelligence.org/get-involved/\">MIRI’s research workshops</a>, and is scheduled to attend another in December.</p>
</div>
<p><strong>Luke Muehlhauser</strong>: Since you’ve attended two MIRI research workshops on “Friendly AI math,” I’m hoping you can explain to our audience what that work is all about. To provide a concrete example, I’d like to talk about the <a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/\">Löbian obstacle to self-modifying artificial intelligence</a>, which is one of the topics that MIRI’s recent workshops have focused on. To start with, could you explain to our readers what this problem is and why you think it is important?</p>
<p><span id=\"more-10384\"></span></p>
<hr />
<p><strong>Benja Fallenstein</strong>: MIRI’s research is based on I.J. Good’s concept of the <a href=\"http://intelligence.org/ie-faq/\">intelligence explosion</a>: the idea that once we build an artificial intelligence that’s as good as a human at doing artificial intelligence research, this AI will be able to figure out how to make itself even smarter, and even better at AI research, leading to a runaway process that will eventually create machines far surpassing the capabilities of any human being. When this happens, <a href=\"http://intelligence.org/2013/05/05/five-theses-two-lemmas-and-a-couple-of-strategic-implications/\">we really want these machines to have goals that humanity</a> <a href=\"http://intelligence.org/2013/05/05/five-theses-two-lemmas-and-a-couple-of-strategic-implications/\">would approve of</a>. True, it’s not very likely that an AI would decide that it wants to rule over us (that’s just <a href=\"http://en.wikipedia.org/wiki/Anthropomorphism\">anthropomorphism</a>), but most goals that a computer could have would be dangerous to us: For example, imagine a computer that wants to calculate π to as many digits as possible. That computer will see humans as being made of atoms which it could use to build more computers; and worse, since we would object to that and might try to stop it, we’d be a potential threat that it would be in the AI’s interest to eliminate (<a href=\"http://selfawaresystems.files.wordpress.com/2008/01/ai_drives_final.pdf\">Omohundro 2008</a>). So we want to make very sure that the end result of an intelligence explosion (after many, many self-improvements) is an AI with “good” goals.</p>
<p>Now you might think that all we need to do is to build our initial AI to have “good” goals. As a toy model, imagine that you program your AI to only take an action x if it can prove that doing x will lead to a “good” outcome. Then the AI won’t self-modify to have “bad” goals, because it won’t be able to prove that this self-modification to a “good” outcome. But on the other hand, you’d think that this AI would be able to self-modify in a way that leaves its goals intact: You’d think that it would be able to reason, “Well, the new version of me will only take an action y if it can prove that this leads to an outcome it likes, and it likes ‘good’ outcomes, just as I do — so whatever it does will lead to a ‘good’ outcome, it’s all fine!”</p>
<p>But here’s the problem: In this chain of reasoning, our AI needs to go from “the new version will only take an action y if it has proven that y leads to a good outcome” to “it will only take y if this actually leads to a good outcome.” Intuitively, this seems like a perfectly reasonable argument; after all, we trust proofs in whatever formal system the AI is using (or we’d have programmed the AI to use a different system), so why shouldn’t the AI do the same? But by <a href=\"http://lesswrong.com/lw/t6/the_cartoon_guide_to_l%C3%B6bs_theorem/\">Löb’s Theorem</a>, no sufficiently strong formal system can know that everything that it proves to be true is actually true. That’s what we call the “Löbian obstacle.”</p>
<hr />
<p><strong>Luke</strong>: You’ve called using mathematical proofs a “toy model,” but that’s exactly how work at the recent MIRI workshops has approached the Löbian obstacle. Do you think that practical AI designs will be based on logic and proofs? How confident are you that the Löbian obstacle will be relevant to a realistic artificial intelligence, and that the work MIRI is currently doing will be applicable in that context?</p>
<hr />
<p><strong>Benja</strong>: We certainly don’t think that a realistic AI will literally be able to find a mathematical proof that its actions are guaranteed to lead to “good” outcomes. Any practical AI will be uncertain about many things and will need to use probabilistic reasoning. There are two reasons why I think that MIRI’s current work has a decent chance of being relevant in that setting.</p>
<p>First, Löb’s theorem is only one instance of a “diagonalization argument” placing limits on the degree to which a formal system can do self-reference. For example, there’s <a href=\"http://en.wikipedia.org/wiki/Tarski%27s_undefinability_theorem\">Tarski’s theorem</a> that a powerful formal language can’t talk about which sentences in that language are true, because otherwise you could have a formal analogue of the <a href=\"http://en.wikipedia.org/wiki/Liar_paradox\">Liar</a> <a href=\"http://en.wikipedia.org/wiki/Liar_paradox\">paradox</a> “this sentence is false,” and <a href=\"http://en.wikipedia.org/wiki/Halting_problem\">Turing’s halting problem</a>, which says that there’s no computer program which can say for arbitrary other programs whether they go into an infinite loop. Other well-known examples include <a href=\"http://en.wikipedia.org/wiki/Russell%27s_paradox\">Russell’s paradox</a> and <a href=\"http://en.wikipedia.org/wiki/Cantor%27s_diagonal_argument\">Cantor’s argument that not all infinite sets are the same size</a>. Similar arguments apply to simple-minded ways of doing probabilistic reasoning, so I feel that it’s unlikely that the problem will just automatically go away when we start using probability, and I think there is a decent chance that the work we are doing now will lead to insights that are applicable in that setting.</p>
<p>And second, in order to achieve a reasonable probability that our AI still follows the same goals after billions of rewrites, we must have a very low chance of going wrong in every single step, and machine-verified formal mathematical proofs are the one way we know to become extremely confident that something is true (especially statements like “this AI design won’t destroy the world”, where we don’t get to just observe many independent examples). Although you can never be sure that a program will work as intended when run on a real-world computer — it’s always possible that a cosmic ray will hit a transistor and make things go awry — you can prove that a program would satisfy certain properties when run on an ideal computer. Then you can use probabilistic reasoning and error-correcting techniques to make it extremely probable that when run on a real-world computer, your program still satisfies the same property. So it seems likely that a realistic Friendly AI would still have <em>components</em> that do logical reasoning or something that looks very much like it.</p>
<p>I tend to think not in terms of the results that we are currently proving being directly relevant to a future AI design; rather, I hope that the work we are currently doing will help us understand the problems better and lead to insights that lead to insights that ultimately allow us to build a safe self-improving machine intelligence.</p>
<hr />
<p><strong>Luke</strong>: What sort of historical precedent do we have for doing technical work that we hope will lead to some insights, which will lead to other insights, which will lead to other insights, which will lead to useful application many years later?</p>
<p>I suppose that kind of thing happens in mathematics on occasion, for example when in the 1980s it was discovered that one might be able to prove <a href=\"http://en.wikipedia.org/wiki/Fermat%27s_last_theorem\">Fermat’s Last Theorem</a> via the modularity theorem, which prompted Andrew Wiles to pursue this attack, which allowed him to prove Fermat’s Last Theorem after about a decade of work (<a href=\"http://www.amazon.com/Fermats-Enigma-Greatest-Mathematical-Problem/dp/0802713319/\">Singh 1997</a>). Another example was Hamilton’s attack on the <a href=\"http://en.wikipedia.org/wiki/Poincar%C3%A9_conjecture\">Poincaré conjecture</a> via <a href=\"http://en.wikipedia.org/wiki/Ricci_flow\">Ricci flow</a> on a manifold, which began in 1982 and led to Perelman’s proof in 2003 (<a href=\"http://www.amazon.com/Poincares-Prize-Hundred-Year-Greatest-ebook/dp/B001RTKITQ/\">Szpiro 2008</a>). Of course, other conjectures have thus far resisted decades of effort to prove them, for example the <a href=\"http://en.wikipedia.org/wiki/Riemann_hypothesis\">Riemann Hypothesis</a> (<a href=\"http://www.amazon.com/Stalking-Riemann-Hypothesis-Numbers-ebook/dp/B0012RMVES/\">Rockmore 2007</a>) and P ≠ NP (<a href=\"http://www.amazon.com/The-Golden-Ticket-Impossible-ebook/dp/B00BKZYGUY/\">Fortnow 2013</a>).</p>
<p>But “goal stability under self-modification” isn’t as well-defined as the conjectures by Fermat and Poincaré. Maybe more analogous examples come from the field of computer science? For example, many early AI scientists worked toward the goal of writing a computer program that could play Grandmaster-level chess, even though they couldn’t be sure exactly what such a program would look like. There are probably analogues in quantum computing, too.</p>
<p>But anyway: how do you think about this?</p>
<hr />
<p><strong>Benja</strong>: My gut feeling actually tends to be that what we are trying to do here is fairly unusual — and for a good reason: it’s risky. If you want to be sure that what you’re working on isn’t a dead end, you’d certainly want to choose a topic where the gap between our goals and our current knowledge is smaller than in FAI. But I’m worried that if we wait with doing FAI research until we understand how AGIs will work, then there won’t be enough time remaining before the intelligence explosion to actually get the task done, so my current feeling is that the right tradeoff is to start now despite the chance of taking the wrong tack.</p>
<p>But then again, maybe our situation isn’t as unusual as my gut feeling suggests. Depending on how close you want the analogy to be, there may be many examples where scientists have a vague idea of the problem they want to solve, but aren’t able to tackle it directly, so instead they look for a small subproblem that they think they can make some progress on. You could tell a story where much of physics research is ultimately aimed at figuring out the true basic laws of the universe, and yet all a physicist can actually do is work on the next problem in front of them. Surely psychology was aimed from the beginning at figuring out all about how the human mind works, and yet starting by training rats to press a lever to get food, and later following this up by sticking an electrode in the rat’s brain and seeing what neurons are involved in accomplishing this task, can count as insights leading to insights that will plausibly help us figure out what’s really going on. Your own post on “<a href=\"http://lesswrong.com/lw/hsd/start_under_the_streetlight_then_push_into_the/\">searching under the</a> <a href=\"http://lesswrong.com/lw/hsd/start_under_the_streetlight_then_push_into_the/\">streetlight</a>” gives some examples of this pattern as well.</p>
<hr />
<p><strong>Luke</strong>: Can you say more about why you and some others think this problem of stability under self-modification should be investigated from the perspective of mathematical logic? For example, Stanford graduate student <a href=\"http://cs.stanford.edu/%7Ejsteinhardt/\">Jacob Steinhardt</a> <a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/9ai6\">commented</a> that the first tool he’d reach for to investigate this problem would not be mathematical logic, but instead “a martingale…, which is a statistical process that somehow manages to correlate all of its failures with each other… This can yield bounds on failure probability that hold for extremely long time horizons, even if there is non-trivial stochasticity at every step.”</p>
<hr />
<p><strong>Benja</strong>: I said earlier that in order to have a chance that our AI still follows the same goals after billions of rewrites, the probability of going wrong on any particular step must be very, very small. That’s true, but it’s not very quantitative. If we want to have 99% probability of success, how large a risk can we take on any particular step? It would be sufficient if the probability is lower than one in 100 billion each time, but that’s not really necessary. Jacob’s idea of using martingales is a similar but more flexible answer to this question, which allows you to take slightly larger risks under certain circumstances.</p>
<p>But even with this additional flexibility, you will still need a way to achieve extremely high confidence that what you’re doing is safe on most of the rewrite steps. And we can’t just achieve that confidence by the tried-and-true way of running an experiment with a large sample: The question is whether a rewrite our nascent AI is considering early on will lead to the intended results after the AI has become superintelligent and spread through the solar system and beyond — you can’t just simulate that, if you don’t already have these resources yourself!</p>
<p>So we need a way to reason abstractly about how our AI will behave in situations that are completely different from what we can simulate at the moment, and we will need to reach extreme confidence that these abstract conclusions are in fact correct. There is exactly one way we know how to do this, and that’s to use formally verified proofs in mathematical logic.</p>
<hr />
<p><strong>Luke</strong>: Suppose John Doe had an intuition that despite the fact that he is not a cognitive system with a logical architecture, he feels like he could make lots of self-modifications while retaining his original goals, if he had enough computational power and plenty of time to reason about whether the next self-modification he’s planning is going to change his goals. If this intuition is justified, then this suggests there are other methods we might use, outside mathematical logic, to ensure a very small probability of goal corruption upon self-modification. What would you say to John?</p>
<hr />
<p><strong>Benja</strong>: I’d say that I think he underestimates the difficulty of the problem. Two things:</p>
<p>First, my impression is that a lot of people have an intuition that they are already making self-modifications all the time. But the changes that humans can make with present-day technology don’t change the design of the hardware we’re running on — they pale against the difference between a human and a chimpanzee, and a self-improving AI would very likely end up making much more fundamental changes to its design than the relatively small number of tweaks evolution has applied to our brains in the last five million years.</p>
<p>But second, John might say that even taking this into account, he thinks that given enough time to learn how his brain works, and reasoning very carefully about every single step he’s taking, he should be able to go through a long chain of self-modifications that preserve his values. In this case, I think it’s fairly likely that he’s just wrong. However, I could imagine that a human could in fact succeed in doing this — but not without achieving the same sort of extremely high confidence in each single rewrite step that we want our AI to have, and I think that if a human could manage to achieve this sort of confidence, it would be by … proving mathematical theorems and having the proofs formally checked by a computer!</p>
<hr />
<p><strong>Luke</strong>: Yeah, when people say that humans self-modify all the time without changing their goals, I give two replies of my own. First, I point out that people’s goals and values do change pretty often. And second, I point out how little humans actually self-modify. For example I once <a href=\"http://lesswrong.com/lw/7dy/a_rationalists_tale/\">switched</a> from fundamentalist Christian to scientific naturalist, and this went along with a pretty massive shift in how I process evidence and argument. But throughout that worldview change, my brain was still (e.g.) using the temporal difference reinforcement learning algorithm in my dopaminergic reward system. As far as we know, there were no significant changes to my brain’s core algorithms during that period of transformation. Humans never actually self-modify very much, not like an AI would.</p>
<p>My next question has to do with AI capability. As AI scientists know, logic-based AI is generally far less capable than AI that uses machine learning methods. Is the idea that only a very small part of a future self-modifying AI would have a logical structure (so that it could prove the goodness of modifications to its core algorithms), and the rest of the AI would make use of other methods? Sort of like how small parts of safety-critical software (e.g. for <a href=\"http://shemesh.larc.nasa.gov/fm/papers/FormalVerificationFlightCriticalSoftware.pdf\">flight control</a>) are written in a structured way such that they are amenable to <a href=\"http://en.wikipedia.org/wiki/Formal_verification\">formal verification</a>, but the rest of the system isn’t necessarily written in ways amenable to formal verification?</p>
<hr />
<p><strong>Benja</strong>: I think that your point that people’s values actually do change often is useful for intuition, and I also feel it’s important to point out that these changes are again actually pretty small compared to what could happen if you were to change your brain’s entire architecture. People might switch between being committed environmentalists and feeling that environmentalism is fundamentally misguided, for example, but they don’t tend to become committed triangularists who feel that it’s a moral imperative to make all every-day tools triangular in shape. Both murder and condemnation of murder are <a href=\"http://en.wikipedia.org/wiki/Human_universal\">human universals</a>, traits that are common to all cultures world-wide; we are talking about changes to our cognitive architecture easily fundamental enough that they could lead to the impulse to non-triangularism and the condemnation of this non-triangularism to become similarly universal.</p>
<p>Yes, I think that logical reasoning would be only one tool in the toolbox of a Friendly AI, and it would use different tools to reason about most things in its environment. Even when reasoning about its own behavior, I would expect the AI to only use logic to prove theorems about how it would behave when run on “ideal” hardware (or hardware that has certain bounds on error rates, etc.), and then use probabilistic reasoning to reason about what happens if it runs on real hardware in the physical world. (But with regard to your analogy to present-day safety-critical software, I want to point out that unlike with present-day safety-critical software, I expect the AI to prove theorems about all of its own component parts, I just don’t expect it to use logic to reason about, say, chairs. Formal verification is difficult and time-consuming, which is the reason why we currently only apply it to small parts of safety-critical systems, but I expect future AIs to be up to the task!)</p>
<hr />
<p><strong>Luke</strong>: Hmmm. That’s surprising. My understanding is that formal verification methods do not scale well at all, both due to computational intractability and due to the hours of human labor required to write a correct formal specification against which one could verify a complicated system. Why do you think a future AI would be “up to the task” of proving theorems “about all of its own component parts”?</p>
<hr />
<p><strong>Benja</strong>: Well, for one thing, I generally expect future AIs to be smarter than us, and easily able to do intellectual tasks that would take very many hours of human labor; and I don’t expect that they will get tired of the menial task of translating their mathematical “intuitions” into long chains of “boring” lemmas, like humans do.</p>
<p>But more specifically, we humans have an intuitive understanding of why we expect the systems we build to work, and my feeling is that probably a major reason for why it is difficult to translate that understanding into formal proofs is that there’s a mismatch between the way these intuitions are represented in our brain and the way the corresponding notions are represented in a formal proof system. In other words, it seems likely to me that when you build a cognitive architecture from scratch, you could build it to have mathematical “intuitions” about why a certain piece of computer code works that are fairly straightforward to translate into formally verifiable proofs. In fact, similarly to how I would expect an AI to directly manipulate representations of computer code rather than using images and verbal sounds like we humans do, I think it’s likely that an FAI will do much of its reasoning about why a piece of computer code works by directly manipulating representations of formal proofs.</p>
<p>That said, it also often seems to happen that we humans know by experience that a certain algorithm or mathematical trick tends to work on a lot of problems, but we don’t have a full explanation for why this is. I do expect future AIs to have to do reasoning of this type as well, and it does seem quite plausible that an AI might want to apply this type of reasoning to (say) machine learning algorithms that it uses for image processing, where a mistake can be recovered from — though likely not to the code it uses to check that future rewrites will still follow the same goal system! And I still expect the AI to prove theorems about its image processing algorithm, I just expect them to be things like “this algorithm will always complete in at most the following number of time steps” or “this algorithm will be perform correctly under the following assumptions, which are empirically known to be true in a very large number of cases.”</p>
<hr />
<p><strong>Luke</strong>: Thanks, Benja!</p>
<p>The post <a href=\"http://intelligence.org/2013/08/04/benja-interview/\">Benja Fallenstein on the Löbian Obstacle to Self-Modifying Systems</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/08/04/benja-interview/#comments" "0e20f569840a5e5afdaaad04778fad62") (28 (20992 43957 721890) "http://intelligence.org/2013/08/02/algorithmic-progress-in-six-domains-released/?utm_source=rss&utm_medium=rss&utm_campaign=algorithmic-progress-in-six-domains-released" "=?utf-8?Q?=E2=80=9CAlgorithmic?= Progress in Six =?utf-8?Q?Domains=E2=80=9D?= Released" "Luke Muehlhauser" "Sat, 03 Aug 2013 02:31:09 +0000" "<p><a href=\"http://intelligence.org/files/AlgorithmicProgress.pdf\"><img class=\"alignright shadowed size-full\" alt=\"algorithmic progress\" src=\"http://intelligence.org/wp-content/uploads/2013/08/algorithmic_progress_cover.png\" /></a>Today we released a new technical report by visiting researcher <a href=\"http://meteuphoric.wordpress.com/\">Katja Grace</a> called “<a href=\"http://intelligence.org/files/AlgorithmicProgress.pdf\"><strong>Algorithmic Progress in Six Domains</strong></a>.” The report summarizes data on algorithmic progress – that is, better performance per fixed amount of computing hardware – in six domains:</p>
<ul>
<li><span style=\"line-height: 13px;\">SAT solvers,</span></li>
<li><span style=\"line-height: 13px;\">Chess and Go programs,</span></li>
<li><span style=\"line-height: 13px;\">Physics simulations,</span></li>
<li><span style=\"line-height: 13px;\">Factoring,</span></li>
<li><span style=\"line-height: 13px;\">Mixed integer programming, and</span></li>
<li><span style=\"line-height: 13px;\">Some forms of machine learning.<br />
</span></li>
</ul>
<p>Our purpose for collecting these data was to shed light on the question of <a href=\"http://lesswrong.com/lw/hbd/new_report_intelligence_explosion_microeconomics/\">intelligence explosion microeconomics</a>, though we suspect the report will be of broad interest within the software industry and computer science academia.</p>
<p>One finding from the report was previously discussed by Robin Hanson <a href=\"http://www.overcomingbias.com/2013/06/why-does-hardware-grow-like-algorithms.html\">here</a>. (Robin saw an early draft on the intelligence explosion microeconomics <a href=\"https://docs.google.com/forms/d/1KElE2Zt_XQRqj8vWrc_rG89nrO4JtHWxIFldJ3IY_FQ/viewform\">mailing list</a>.)</p>
<p>The preferred page for discussing the report in general is <a href=\"http://lesswrong.com/r/discussion/lw/i8i/algorithmic_progress_in_six_domains/\">here</a>.</p>
<p>Summary:</p>
<blockquote><p>In recent <em>boolean satisfiability</em> (SAT) competitions, SAT solver performance has increased 5–15% per year, depending on the type of problem. However, these gains have been driven by widely varying improvements on particular problems. Retrospective surveys of SAT performance (on problems chosen after the fact) display significantly faster progress.</p>
<p><em>Chess programs</em> have improved by around 50 Elo points per year over the last four decades. Estimates for the significance of hardware improvements are very noisy, but are consistent with hardware improvements being responsible for approximately half of progress. Progress has been smooth on the scale of years since the 1960s, except for the past five. <em>Go programs</em> have improved about one stone per year for the last three decades. Hardware doublings produce diminishing Elo gains, on a scale consistent with accounting for around half of progress.</p>
<p>Improvements in a variety of <em>physics simulations</em> (selected after the fact to exhibit performance increases due to software) appear to be roughly half due to hardware progress.</p>
<p>The <em>largest number factored</em> to date has grown by about 5.5 digits per year for the last two decades; computing power increased 10,000-fold over this period, and it is unclear how much of the increase is due to hardware progress.</p>
<p>Some <em>mixed integer programming</em> (MIP) algorithms, run on modern MIP instances with modern hardware, have roughly doubled in speed each year. MIP is an important optimization problem, but one which has been called to attention after the fact due to performance improvements. Other optimization problems have had more inconsistent (and harder to determine) improvements.</p>
<p>Various forms of <em>machine learning</em> have had steeply diminishing progress in percentage accuracy over recent decades. Some vision tasks have recently seen faster progress.</p></blockquote>
<p>The post <a href=\"http://intelligence.org/2013/08/02/algorithmic-progress-in-six-domains-released/\">“Algorithmic Progress in Six Domains” Released</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/08/02/algorithmic-progress-in-six-domains-released/#comments" "9884cc20d69025edd3ad72afe8728bdc") (27 (20987 26854 936776) "http://intelligence.org/2013/07/31/ai-risk-and-the-security-mindset/?utm_source=rss&utm_medium=rss&utm_campaign=ai-risk-and-the-security-mindset" "AI Risk and the Security Mindset" "Luke Muehlhauser" "Thu, 01 Aug 2013 03:19:14 +0000" "<p>In 2008, security expert Bruce Schneier wrote about <a href=\"http://www.schneier.com/blog/archives/2008/03/the_security_mi_1.html\">the security mindset</a>:</p>
<blockquote><p>Security requires a particular mindset. Security professionals… see the world differently. They can’t walk into a store without noticing how they might shoplift. They can’t use a computer without wondering about the security vulnerabilities. They can’t vote without trying to figure out how to vote twice…</p>
<p><a href=\"http://www.smartwater.com/\">SmartWater</a> is a liquid with a unique identifier linked to a particular owner. “The idea is for me to paint this stuff on my valuables as proof of ownership,” I <a href=\"http://www.schneier.com/blog/archives/2005/02/smart_water.html\">wrote</a> when I first learned about the idea. “I think a better idea would be for me to paint it on <em>your</em> valuables, and then call the police.”</p>
<p>…This kind of thinking is not natural for most people. It’s not natural for engineers. Good engineering involves thinking about how things can be made to work; the security mindset involves thinking about how things can be made to fail. It involves thinking like an attacker, an adversary or a criminal. You don’t have to exploit the vulnerabilities you find, but if you don’t see the world that way, you’ll never notice most security problems.</p></blockquote>
<p><a href=\"http://en.wikipedia.org/wiki/With_Folded_Hands\"><img class=\"alignright size-full wp-image-10372\" alt=\"with folded hands\" src=\"http://intelligence.org/wp-content/uploads/2013/07/with-folded-hands.jpg\" /></a>A recurring problem in much of <a href=\"http://intelligence.org/files/ResponsesAGIRisk.pdf\">the literature</a> on “machine ethics” or “AGI ethics” or “AGI safety” is that researchers and commenters often appear to be asking the question “How will this solution work?” rather than “How will this solution fail?”</p>
<p>Here’s an example of the security mindset at work when thinking about AI risk. When presented with the suggestion that an AI would be safe if it “merely” (1) was very good at prediction and (2) gave humans text-only answers that it predicted would result in each stated goal being achieved, <a href=\"http://lesswrong.com/user/Viliam_Bur/overview/\">Viliam Bur</a> pointed out a possible failure mode (which was later <a href=\"http://lesswrong.com/lw/cze/reply_to_holden_on_tool_ai/\">simplified</a>):</p>
<blockquote><p>Example question: “How should I get rid of my disease most cheaply?” Example answer: “You won’t. You will die soon, unavoidably. This report is 99.999% reliable”. Predicted human reaction: Decides to kill self and get it over with. Success rate: 100%, the disease is gone. Costs of cure: zero. Mission completed.</p></blockquote>
<p>This security mindset is one of the traits we look for in researchers we might hire or collaborate with. Such researchers show a tendency to ask “How will this fail?” and “Why might this formalism not quite capture what we really care about?” and “Can I find a way to break this result?”</p>
<p>That said, there’s no sense in being <em>infinitely</em> skeptical of results that may help with AI security, safety, reliability, or “friendliness.” As always, we must <a href=\"http://riskmanagementinsight.com/riskanalysis/?p=350\">think with probabilities</a>.</p>
<p>Also see:</p>
<ul>
<li><a href=\"http://happybearsoftware.com/you-are-dangerously-bad-at-cryptography.html\">You are dangerously bad at cryptography</a></li>
<li><a href=\"http://www.csl.sri.com/users/neumann/illustrative.html\">Illustrative Risks to the Public in the Use of Computer Systems and Related Technology</a></li>
<li><a href=\"http://intelligence.org/files/IE-ME.pdf\">Intelligence Explosion and Machine Ethics</a></li>
<li><a href=\"http://intelligence.org/files/ComplexValues.pdf\">Complex Value Systems are Required to Realize Valuable Futures</a></li>
<li><a href=\"http://lesswrong.com/lw/cze/reply_to_holden_on_tool_ai/\">Reply to Holden on Tool AI</a> (especially sec. 2)</li>
</ul>
<p>The post <a href=\"http://intelligence.org/2013/07/31/ai-risk-and-the-security-mindset/\">AI Risk and the Security Mindset</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/07/31/ai-risk-and-the-security-mindset/#comments" "f6356c7d749832017e324526626230da") (26 (20984 50425 921607) "http://intelligence.org/2013/07/25/index-of-transcripts/?utm_source=rss&utm_medium=rss&utm_campaign=index-of-transcripts" "Index of Transcripts" "Luke Muehlhauser" "Thu, 25 Jul 2013 15:50:07 +0000" "<p><span>Volunteers at <a href=\"http://mirivolunteers.org/\">MIRI Volunteers</a> and elsewhere have helpfully transcribed several audio/video recordings related to MIRI’s work. This post is a continuously updated index of those transcripts.</span></p>
<p>All transcripts of Singularity Summit talks are available <a href=\"http://intelligence.org/singularitysummit/\">here</a>.</p>
<p>Other available transcripts include:</p>
<ul>
<li><em>Philosophy Talk</em>: <a title=\"Transcript\" href=\"https://docs.google.com/document/d/13EYHVI9KBte28-TMn3PN4QyT_8aZtnQs8AGFBH20Qp0/pub\">Turbo-Charging the Mind</a> (with Anna Salamon)</li>
<li><em>BloggingHeads.tv</em>: <a href=\"https://docs.google.com/document/d/1JIqzTGNvdLukR0Ce5T2eiv_vxtPO53N3KGCbxVvREtU/pub\">Eliezer Yudkowsky and Scott Aaronson on superintelligence and many-worlds</a></li>
<li><em>BloggingHeads.tv</em>: <a href=\"https://docs.google.com/document/d/1S-7CWOLOtLRDmMiS7LtVxELssUi3OI1-UcrPAzGMuH4/pub\">Eliezer Yudkowsky and Massimo Pigliucci on consciousness and uploading</a></li>
</ul>
<p>The post <a href=\"http://intelligence.org/2013/07/25/index-of-transcripts/\">Index of Transcripts</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/07/25/index-of-transcripts/#comments" "4b889fa0a6dcff2af625393ac3d5e0e6") (25 (20984 50425 919293) "http://intelligence.org/2013/07/24/miris-december-2013-workshop/?utm_source=rss&utm_medium=rss&utm_campaign=miris-december-2013-workshop" "=?utf-8?Q?MIRI=E2=80=99s?= December 2013 Workshop" "Luke Muehlhauser" "Wed, 24 Jul 2013 23:55:35 +0000" "<img class=\"aligncenter size-full wp-image-10010\" alt=\"013\" src=\"http://intelligence.org/wp-content/uploads/2013/02/013.jpg\" />
<p>From December 14-20, MIRI will host another <strong>Workshop on Logic, Probability, and Reflection</strong>. This workshop will focus on the <a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/\">Löbian obstacle</a>, <a href=\"http://lesswrong.com/lw/h1k/reflection_in_probabilistic_logic/\">probabilistic logic</a>, and the intersection of logic and probability more generally.</p>
<p>Participants confirmed so far include:</p>
<ul>
<li><a href=\"http://www.math.harvard.edu/~nate/\">Nate Ackerman</a> (Harvard)</li>
<li><a href=\"http://www.math.ucr.edu/home/baez/\">John Baez</a> (UC Riverside)</li>
<li><a href=\"http://rationalaltruist.com/\">Paul Christiano</a> (UC Berkeley)</li>
<li><a href=\"http://lesswrong.com/user/Benja/submitted/\">Benja Fallenstein</a> (U Bristol)</li>
<li><a href=\"http://math.mit.edu/~freer/\">Cameron Freer</a> (MIT)</li>
<li>Jeremy Hahn (Harvard)</li>
<li><a href=\"http://wojtek.moczydlowski.net/\">Wojtek Moczydlowski</a> (Google)</li>
<li><a href=\"http://www.ctpost.com/news/article/Wisdom-beyond-his-years-1390299.php\">Will Sawin</a> (Princeton)</li>
<li><a href=\"http://gregorywheeler.org/\">Greg Wheeler</a> (CMU)</li>
<li><a href=\"http://yudkowsky.net/\">Eliezer Yudkowsky</a> (MIRI)</li>
</ul>
<p>If you have a strong mathematics background and might like to attend this workshop, it’s not too late to <a href=\"http://intelligence.org/get-involved/#workshop\">apply</a>! And even if <em>this</em> workshop doesn’t fit your schedule, please <strong>do apply</strong>, so that we can notify you of other workshops (long before they are announced publicly).</p>
<p>The post <a href=\"http://intelligence.org/2013/07/24/miris-december-2013-workshop/\">MIRI’s December 2013 Workshop</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/07/24/miris-december-2013-workshop/#comments" "93baf9e613d45f4da363e6f777b20392") (24 (20984 50425 916642) "http://intelligence.org/2013/07/17/beckstead-interview/?utm_source=rss&utm_medium=rss&utm_campaign=beckstead-interview" "Nick Beckstead on the Importance of the Far Future" "Luke Muehlhauser" "Thu, 18 Jul 2013 06:13:22 +0000" "<div class=\"well\">
<p><img class=\"shadowed alignright\" alt=\"\" src=\"http://intelligence.org/wp-content/uploads/2013/07/225877_10102425448513670_660885940_n.jpg\" width=\"150\" height=\"206\" /> Nick Beckstead recently finished a Ph.D in philosophy at <a href=\"http://philosophy.rutgers.edu/\">Rutgers University</a>, where he <a href=\"https://sites.google.com/site/nbeckstead/research\">focused</a> on practical and theoretical ethical issues involving future generations. He is particularly interested in the practical implications of taking full account of how actions taken today affect people who may live in the very distant future. His research focuses on how big picture questions in normative philosophy (especially population ethics and decision theory) and various big picture empirical questions (especially about existential risk, moral and economic progress, and the future of technology) feed into this issue.</p>
<p>Apart from his academic work, Nick has been closely involved with the <a href=\"http://effective-altruism.com/\">effective altruism</a> movement. He has been the director of research for <a href=\"http://www.givingwhatwecan.org/\">Giving What We Can</a>, he has worked as a summer research analyst at <a href=\"http://www.givewell.org/\">GiveWell</a>, and he is currently on the board of trustees for the <a href=\"http://home.centreforeffectivealtruism.org/\">Centre for Effective Altruism</a>, and he recently became a research fellow at the Future of Humanity Institute.</p>
</div>
<p><span id=\"more-10348\"></span><br />
<strong>Luke Muehlhauser:</strong> Your Rutgers philosophy dissertation, “<a href=\"https://sites.google.com/site/nbeckstead/research\">On the Overwhelming Importance of the Far Future</a>,” argues that “from a global perspective, what matters most (in expectation) is that we do what is best (in expectation) for the general trajectory along which our descendants develop over the coming millions, billions, and trillions of years.”</p>
<p>In an<a href=\"http://intelligence.org/2013/06/05/friendly-ai-research-as-effective-altruism/\"> earlier post</a>, I summed up your “rough future-shaping argument”:</p>
<blockquote><p>Astronomical facts suggest that humanity (including “post-humanity”) could survive for billions or trillions of years (<a href=\"http://books.google.com/books?id=X5jdMyJKNL4C&pg=PT77&lpg=PT77#v=onepage&q&f=false\">Adams 2008</a>), and could thus produce enormous amounts of good. But the value produced by our future depends on our development trajectory. If humanity destroys itself with powerful technologies in the 21st century, then nearly all that future value is lost. And if we survive but develop along a trajectory dominated by conflict and poor decisions, then the future could be much less good than if our trajectory is dominated by altruism and wisdom. Moreover, some of our actions today can have “ripple effects” which determine the trajectory of human development, because many outcomes are <a href=\"http://en.wikipedia.org/wiki/Path_dependence\">path-dependent</a>. Hence, actions which directly or indirectly precipitate particular trajectory changes (e.g. mitigating existential risks) can have vastly more value (in expectation) than actions with merely proximate benefits (e.g. saving the lives of 20 wild animals).</p></blockquote>
<p>One of the normative assumptions built into the rough future-shaping argument is an assumption you call Additionality. Could you explain what Additionality is, and why some people reject it?</p>
<hr />
<p><strong>Nick Beckstead:</strong> I think it may be helpful to give a bit of background first. I like to tackle the question of “how important is the far future?” by dividing the future up into big chunks of time (which I call “periods”), assigning values to the big chunks of time, and then assigning a value to the future as a function of the value assigned to the big chunks of time. You could think of it as creating some kind of computer program which would scan whole history of the world together with its future, carve it up into periods, scan each period and assign it a value, and then compute a value of the whole as a function of the value of its parts. It’s arbitrary how you carve up periods, but that’s okay because it’s an approximation technique. I think the approximation technique gives useful and reasonable answers if you make the periods quite large (spanning hundreds, thousands, or more years at once; you might want to carve it up into large batches of intelligent activity if you are considering future civilizations very different from our own).</p>
<p>Additionality basically says that when you’re assigning value to future periods, when you’ve got periods that you’d assign as “good,” it’s always better to have a period that you’d assign as good than periods you’d assign as “neutral.” I’m trying to partly draw on our intuitive ways of determining how well things have been going in recent history, and extending that to future periods, which we may be less capable of valuing using other methods. I want to say that if you had some future period which you’d regard as “good” judged purely on the basis of what happens in that period itself, that should contribute to the value you assign to the whole future.</p>
<p>You might disagree with this if you have what some philosophers call a strict “<a href=\"http://plato.stanford.edu/entries/nonidentity-problem/\">Person-Affecting View</a>“. According to strict Person-Affecting Views, the fact that a person’s life would go well if he lived could not, in itself, imply that it would be in some way good to create him. Why not? Since the person was never created, there is no person who could have benefited from being created. On this type of view, it would only be important to ensure that there are future generations if it would somehow benefit people alive today, or people who have lived in the past (perhaps by adding meaning to their lives). The idea is that ensuring that there are future generations is analogous to “creating” many people, and, on this view, “creating” people–even people who would have good lives–can’t be important except insofar as it is important for people other than those you’re creating.</p>
<p>You might also disagree with this view if you think that “shape” considerations are relevant. One example of this is an average type view. You might say that adding on extra periods that are good, but of below average quality, is a bad thing. Or you might say that adding on extra periods that are not as good as the preceding ones can be bad because it could mean that things are getting worse over time.</p>
<p>I feel there are a lot of qualifications and details that need to be fleshed out here, but hopefully that should give some kind of reasonable introduction to the idea.</p>
<hr />
<p><strong>Luke:</strong> When I talk to someone about how much I value the far future, it’s pretty common for them to reply with a Person-Affecting View, though they usually don’t know it by that name. My standard reply is, “I used to have that view myself, but then I encountered some ideas that changed my mind, and made me think that, actually, I probably <i>do</i> care about future people roughly as much as I care about current people.” Then I tell them about those ideas that changed my mind.</p>
<p>I usually start with the <a onclick=\"_gaq.push(['_trackEvent', 'research engagement', 'download pdf', 'http://philsci-archive.pitt.edu/2408/1/Petkov-BlockUniverse.pdf',100])\" href=\"http://philsci-archive.pitt.edu/2408/1/Petkov-BlockUniverse.pdf\">block universe</a> idea, which seems to be the default view among physicists (see e.g. <a href=\"http://www.amazon.com/dp/0375727205/ref=nosim?tag=lukeprogcom-20\">Brian Greene</a> & <a href=\"http://www.amazon.com/dp/0525951334/ref=nosim?tag=lukeprogcom-20\">Sean Carroll</a>, though I also like the explanation by AI researcher <a href=\"http://commonsenseatheism.com/?p=11068\">Gary Drescher</a>). According to the block universe view, there is no privileged “present” time, and hence future people exist in just the same way that present people do.</p>
<p>But in the two chapters you spend arguing against “strict” and “moderate” Person-Affecting Views, you don’t refer to the block universe at all. Do you think the block universe fails to provide a good argument against Person-Affecting Views, or was it simply one line of argument you didn’t take the time to elaborate in your thesis?</p>
<hr />
<p><strong>Nick:</strong> I agree with your view about the block universe. I don’t think it is a strong argument against Person-Affecting Views in general, though I think it is a good argument against certain types of Person-Affecting Views. I think Person-Affecting Views are messy in many ways, and there are other lines of argument that I could have pursued but did not.</p>
<p>Another way to put the basic idea behind Person-Affecting Views is to say that, on these views, you divide people who may exist depending on what you choose into two classes: the “extra” people and the other people. And then you say that if you cause some “extra” people to exist with good lives, either that isn’t good or is less good than helping people who aren’t “extra.” Following <a href=\"http://people.su.se/~guarr/\">Gustaf Arrhenius</a>, in chapter 4 of my dissertation, I consider four different interpretations of extra: the people that don’t presently exist (Presentism), the people that will never actually exist (Actualism), the people whose existence is dependent on which alternative (of perhaps many) we choose (Necessitarianism), and the people that exist in one alternative being compared, but not the other (Comparativism).</p>
<p>As far as I can tell, only Presentism is undermined by the block universe critique, since only Presentism relies on a concept of “present.” This is why I said that the block universe critique only undermines certain versions of Person-Affecting Views.</p>
<p>The block universe argument seems like a knock-down argument against a very precise version of Presentism (which philosophers defending the view may hold), but I don’t think that it is a knock-down argument against a <a href=\"http://lesswrong.com/lw/85h/better_disagreement/\">steel-manned</a>, “rough and ready” version of the view. Someone might say, “Well, yes, I accept the block universe theory, so I acknowledge there is no physically precise thing for me to mean by “present.” But we can, in ordinary English, say sentences like “The world population is now approximately 7 billion.” And you understand me to be saying something intelligible and correct in some approximate sense. In a similar way, when I recommend that we only consider benefits which could come to people now living, I intend you to understand me similarly. I also hold that, right now, it is not practically useful to consider potential benefits to people who may exist in distant parts of the universe, so it doesn’t particularly matter which reference frame you use to approximately interpret my use of “present.” Though my view may not correspond to a clean fundamental distinction, I believe that this recommendation, for our present circumstances, would survive reflection more successfully than other views on this question which have been proposed.”</p>
<p>One can respond to this line of thought by arguing that even rough and ready versions of Presentism have consequences that are hard to accept, and aren’t motivated by appealing theoretical considerations. This is the approach I take in chapter 4 of my dissertation. I believe this line of argument is more robust against a wider variety of alterations of Person-Affecting Views.</p>
<hr />
<p><strong>Luke:</strong> Yeah, I guess I tend to use the block universe not as an argument but as an <a href=\"http://en.wikipedia.org/wiki/Intuition_pump\">intuition pump</a> for the view that “current” people aren’t so privileged (in a moral sense) as one might naively think.</p>
<p>Anyway: in chapter 4 you survey a variety of thought experiments that have varying implications for Person-Affecting Views. At the end of the chapter, you provide this handy summary table:</p>
<img class=\"alignleft\" alt=\"Summary Table\" src=\"http://intelligence.org/wp-content/uploads/2013/07/table-4.2.png\" />
<p>Could you tell us what’s going on in this table, and maybe briefly hint at what a couple of the individual thought experiments are about?</p>
<hr />
<p><strong>Nick:</strong> In chapter 2 of my dissertation, I write about methodology for moral philosophy and argue that intuitive judgments about morality are in many ways less reliable than one might have hoped, and are often inconsistent. One of the consequences of this is that finding just a few counterexamples is often not enough to reject a moral theory. I believe it is important to systematically explore a wide variety of test cases and then proportion one’s credence to the theories that fare best over the whole set of cases.</p>
<p>The rows have different types of theories, and the columns are different types of test cases for the theories. And then I have marked the cases where the theories have implications that are hard to accept. Regarding the terminology in the columns, I call a Person-Affecting View “strict” if it gives no weight to “extra” people, and “moderate” if it gives less weight to “extra” people than other people. There is then a question about how much weight you give, and this table focuses on the cases where little weight is given to “extra” people.</p>
<p>I call a Person-Affecting View “asymmetric” if people who have lives that are not worth living are never counted as “extra.” People with Person-Affecting Views often want their views to be asymmetric because they want to be able to say that it would be bad to cause a child to exist whose life would be filled with suffering. (Derek Parfit has a famous case called “The Wretched Child” in <a href=\"http://www.amazon.com/Reasons-Persons-Oxford-Paperbacks-Parfit/dp/019824908X/ref=sr_1_1?ie=UTF8&qid=1373284563&sr=8-1&keywords=reasons+and+persons\">Reasons and Persons</a>, which is where I got this name. <i>Reasons and Persons</i> is probably my favorite book of moral philosophy.)</p>
<p>A major problem with strict Person-Affecting Views is that they have very implausible consequences in cases of extinction. It is one thing to say that the future of humanity isn’t <i>overwhelmingly</i> important, but quite another to say that it <i>basically doesn’t matter</i> if we go extinct, except insofar as it lowers present people’s quality of life.</p>
<p>Moderate Person-Affecting Views have implausible implications in certain fairly mundane cases where we are choosing between improving the lives of “extra” people or people who aren’t “extra”. A simple example is a case I call “Disease Now or Disease Later,” where we must choose between a public health program that would present some disease from hurting toddlers alive today, or a public health program that would help prevent a greater number of toddlers (who aren’t yet alive) in a few years from now. It is hard to believe that because the other toddlers don’t exist yet and which toddlers exist in the future might depend on which program we choose, it would be better to choose the first program. But that is what moderate Person-Affecting Views imply, since they give less weight to the interests of the toddlers who are counted as “extra”.</p>
<p>I call views which don’t make any distinction between regular people and “extra” people “Unrestricted Views.” Some philosophers believe that these views have imply that individuals are obligated to have children for the greater good, whereas Person-Affecting Views do not. However, there is no clear implication from “it would be good for there to be additional happy people” to “people are typically obligated to have children.” Why not? At least for people who don’t already want to have additional children, it would be very demanding to ask people to have additional children. Moreover, even on a view that gives a lot of weight to creating additional people, having additional children doesn’t seem like a particularly effective way of doing good in the world in comparison with things like donating money and time to charity. So it would be strange if people were obligated to make potentially significant sacrifices in order to do something that actually wasn’t all that effective as a method of doing good.</p>
<p>Basically, the rest of this table is a result of systematically checking these different views against a variety of test cases like these to see which have the most plausible implications overall. Of all these views, only a strict Person-Affecting View can plausibly be used to rebut the case for the overwhelming importance of shaping the far future. And this type of view is much less plausible than the alternatives.</p>
<hr />
<p><strong>Luke:</strong> In chapter 5 of your dissertation you consider the question “Does future flourishing have diminishing marginal value?” Your summary table at the end of <i>that</i> chapter looks like this:</p>
<p style=\"text-align: center;\"><img class=\"alignleft\" alt=\"Summary Table Chapter 5\" src=\"http://intelligence.org/wp-content/uploads/2013/07/table-5.1.png\" width=\"679\" height=\"829\" /></p>
<p>Could you explain what’s going on in this one, too?</p>
<hr />
<p><strong>Nick:</strong> In my dissertation, I defend using the following principle to evaluate the importance of the far future:</p>
<blockquote><p>Period Independence: By and large, how well history goes as a whole is a function of how well things go during each period of history; when things go better during a period, that makes the history as a whole go better; when things go worse during a period, that makes history as a whole go worse; and the extent to which it makes history as a whole go better or worse is independent of what happens in other such periods.</p></blockquote>
<p>Together with other principles I defend, this leads to the conclusion that you can generally approximate the value of the history of the world by assigning a value to each period, and “adding up” the value across periods.</p>
<p>Another way to get a grip on what Period Independence is a partial answer to is to consider the following hypothetical. Imagine that humans survive the next 1000 years, and their lives go well. How good would it be if they survived for another thousand years, with the same or higher quality of life? What if they survived another thousand years beyond that? Consider three kinds of answer:</p>
<ol class=\"org-ol\">
<li>The Period Independence answer: It would be equally as important in each such case.</li>
<li>The Capped Model answer: After a while, it gets less and less important. Moreover, there is an upper limit to how much value you can get in this way.</li>
<li>The Diminishing Value Model (DVM) answer: After a while, it gets less and less important. However, there is no upper limit to how much value you can get in this way.</li>
</ol>
<p>This table summarizes the result of running different test cases against different versions of Period Independence, the Capped Model, and the Diminishing Value Model.</p>
<p>Probably the most important test case supporting Period Independence is the one I call “Our Surprising History.” It goes like this:</p>
<blockquote><p>Our Surprising History: World leaders hire experts to do a cost-benefit analysis and determine whether it is worth it to fund an Asteroid Deflection System. Thinking mostly of the interests of future generations, the leaders decide that it would be well worth it. After the analysis has been done, some scientists discover that life was planted on Earth by other people who now live in an inaccessible region of spacetime. In the past, there were a lot of them, and they had really great lives. Upon learning this, world leaders decide that since there has already been a lot of value in the universe, it is much less important that they build the Asteroid Deflection System than they previously thought.</p></blockquote>
<p>It seems unreasonable to claim that how good it would be to build the Asteroid Deflection System depends on this information about our distant past. But this is what Capped Models and Diminishing Value Models imply about this case.</p>
<p>Many of the cases in this table involve considering some simple test cases involving colonizing other planets. For example consider:</p>
<blockquote><p>The Last Colony: Human civilization has lasted for 1 billion years, but the increasing heat of the sun will soon destroy all life on Earth. Humans (or our non-human descendants) get the chance to colonize another planet, where civilization can continue. They know that if they succeed in colonizing this planet, then: (i) the new planet will sustain a population equal to the size of the population of the Earth, and this planet, like Earth, will sustain life for 1 billion years, (ii) these people’s lives will probably go about as well the lives of the Earth people, (iii) there will not be a chance for the people on the new planet to colonize another planet.</p></blockquote>
<p>Intuitively, it would be extremely important to colonize the extra planet in the second case, much more important than colonizing in the first case. But on a Capped Model, if you set the “upper limits” low enough, it might not be very important at all.</p>
<p>Diminishing Value Models avoid this implication, and can say that it would be extremely important to colonize another planet. They might also claim that their view has more plausible implications than Period Independence when comparing The Last Colony with a case like this:</p>
<blockquote><p>The Very Last Colony: Convinced of the importance of preserving future generations, we take great precautions to protect the far future. Our descendants succeed in colonizing a large portion of the galaxy. It becomes relatively clear that our descendants will last for a very long time, about 100 trillion years, until the last stars burn out. At that point, there will be nothing of value left in the accessible part of the Universe. It comes to our attention that there is a chance to colonize one final place, just as in The Last Colony, before civilization comes to an end. For this billion years, these will be the only people in the accessible part of the Universe. During this period, things will go exactly as well as they went in The Last Colony.</p></blockquote>
<p>In which case is colonization more important, The Last Colony or The Very Last Colony? According to Period Independence, it is equally as important in each case. According to Diminishing Value Models, it is less important in The Very Last Colony. I find DVM stance on this intuitively attractive, though I believe it is a product of a bias I call the <i>proportional reasoning fallacy</i>.</p>
<p>In chapter 2 of my dissertation, I argue that we use misguided proportional reasoning in some cases where many lives are at stake. Fetherstonhaugh et al. (1997) found that participants significantly preferred saving a fixed number of lives in a refugee camp when the proportion of lives saved was greater. Describing the participants’ hypothetical choice, they write:</p>
<blockquote><p>There were two Rwandan refugee programs, each proposing to provide enough clean water to save the lives of 4,500 refugees suffering from cholera in neighboring Zaire. The Rwandan programs differed only in the size of the refugee camps where the water would be distributed; one program proposed to offer water to a camp of 250,000 refugees and the other proposed to offer it to a camp of 11,000.</p></blockquote>
<p>Participants significantly preferred the second program. In another study, Slovic (2007) found that people were willing to pay significantly more for a program of the second kind.</p>
<p>All the views I consider have some implausible implications in certain cases, but it seems easier to explain away the test cases that look bad for Period Independence, and there are somewhat fewer of them, so I conclude that Period Independence is the most plausible principle to use for evaluating far future prospects. Of all these views, only Capped Model or a DVM with a very sharp diminishing rate in the limit can plausibly be used to rebut the case for the overwhelming importance of shaping the far future. And these views, I believe, are less plausible than the alternatives.</p>
<hr />
<p><strong>Luke:</strong> What’s a point you wish you could have included in your dissertation, that was left out for space or other reasons?</p>
<hr />
<p><strong>Nick:</strong> I’ll list a few. There are a lot of things that I think could be better, but you have to put your work out there at some point. Just as <a href=\"http://en.wikiquote.org/wiki/Steve_Jobs\">real artists ship</a>, real thinkers share their ideas.</p>
<p>First, a core empirical claim in my thesis is that humans could have an extremely large impact on the distant future. Really, it’s sufficient for my argument that they would do this by existing for an extremely long time, or that there could be a very large number of successors (such as <a onclick=\"_gaq.push(['_trackEvent', 'research engagement', 'download pdf', 'http://www.philosophy.ox.ac.uk/__data/assets/pdf_file/0019/3853/brain-emulation-roadmap-report.pdf',100])\" href=\"http://www.philosophy.ox.ac.uk/__data/assets/pdf_file/0019/3853/brain-emulation-roadmap-report.pdf\">whole brain emulations</a> or other AIs). I didn’t defend this claim as thoroughly as I could have, and I didn’t go into great detail because I feared that philosophers would complain that it “isn’t philosophy,” I wanted to finish my dissertation, and I thought that going into it would require a lot of background information due to <a href=\"http://wiki.lesswrong.com/wiki/Inferential_distance\">inferential distance</a>.</p>
<p>The second thing I’d like to add is related to chapter 2 of my dissertation. An abstract of that chapter goes like this:</p>
<blockquote><p>I argue that our moral judgments are less reliable than many would hope, and this has specific implications for methodology in normative ethics. Three sources of evidence indicate that our intuitive ethical judgments are less reliable than we might have hoped: a historical record of accepting morally absurd social practices; a scientific record showing that our intuitive judgments are systematically governed by a host of heuristics, biases, and irrelevant factors; and a philosophical record showing deep, probably unresolvable, inconsistencies in common moral convictions. I argue that this has the following implications for moral theorizing: we should trust intuitions less; we should be especially suspicious of intuitive judgments that fit a bias pattern, even when we are intuitively confident that these judgments are not a simple product of the bias; we should be especially suspicious of intuitions that are part of inconsistent sets of deeply held convictions; and we should evaluate views holistically, thinking of entire classes of judgments that they get right or wrong in broad contexts, rather than dismissing positions on the basis of a small number of intuitive counterexamples. In addition, I argue that many of the specific biases that I discuss would lead us to predict that people would, in general, undervalue most of the available ways of shaping the far future, including speeding up development, existential risk reduction, and creating other positive trajectory changes.</p></blockquote>
<p>I’m concerned that in chapter 2, there is an unbalanced focus on ways in which intuitions fail, and not ways in which trying to correct intuition through theory development could fail. An uncharitable analogy would be that it is as if I wrote a paper about all the ways in which <a href=\"https://en.wikipedia.org/wiki/Market_failure\">markets can fail</a> and suggested we rely more on governments without talking about all the ways in which <a href=\"http://en.wikipedia.org/wiki/Government_failure\">governments can fail</a>. And just as someone could write an additional chapter (or series of books) on how governments fail, someone could probably also write an important chapter on how people trying to correct intuitions with moral theory fail. So while I feel that the considerations I identify do speak in favor of the recommendations I make, I think there are also important considerations that speak against those recommendations which I did not mention, and probably should have mentioned.</p>
<p>Some of the considerations on the other side, some of them <a href=\"http://lesswrong.com/lw/hmb/many_weak_arguments_vs_one_relatively_strong/\">weak</a>, include:</p>
<ol class=\"org-ol\">
<li>Given Jonathan Haidt’s theory of <a href=\"http://en.wikipedia.org/wiki/Social_intuitionism\">social intuitionism</a>–which seems very plausible to me–a lot of our theoretical reasoning about moral issues is epiphenomenal lawyering, and that makes theoretical reasoning about morality seem less reliable.</li>
<li>Lots of moral philosophers have endorsed stuff that seems wrong after due consideration, and their views rarely seem superior to common sense when there are conflicts, despite the fact that many of them think they are different from other philosophers in these respects. (A possibly important exception to this is the views of early utilitarians, who opposed slavery, opposed bad treatment of animals, opposed bad treatment of women, opposed bad treatment of gay people, and favored various kinds of liberty quite early. One only has to compare the applied ethics of Kant and Bentham to get a sense of what I am talking about.)</li>
<li>I have a rough sense that only a very limited amount of moral progress is attributable to people trying to use explicit reasoning to correct for intuitive moral errors, in contrast with people who just learned a lot of ordinary facts about problematic cases and shared them widely.</li>
<li>As I discuss somewhat toward the end of the dissertation, when you try to correct for intuitive errors, it’s sort of like trying to patch a piece of software that you don’t understand. And it seems quite possible that the patching will introduce unanticipated errors in places where you didn’t know to look.</li>
<li>People seem to have reasonably functional ways of handling internal inconsistency, so that inconsistent intuitions are probably less damaging than they can appear at first.</li>
<li>A lot of our moral intuition comes from cultural common sense. When we try to correct cultural common sense, we can see what we’re doing as analogous to aiming for a type of innovation. Most attempts at innovation seem to fail. This type of analogy supports being cautious about correcting intuition with theory, and trying to present the theory in a way that is appealing to cultural common sense.</li>
</ol>
<p>I’m still working through these issues, and hope to include them someday in a paper that is an improved version of chapter 2.</p>
<p>A third issue is that there was less discussion of how our altruistically-motivated actions should change once we accept the view that shaping the far future is overwhelmingly important. This is an enormously complex and fascinating issue that requires drawing together ideas from both highly theoretical and highly practical fields. I was thinking about this issue at the time I was writing the dissertation, but not during the whole time. And it doesn’t show up in the dissertation as much as I wish it did. This is again, in part, because I think too much discussion of the issue would result in people complaining that my work “isn’t philosophy.” (I expect this is a common challenge for people in academia with interdisciplinary interests.) I am thinking about this issue more now, and I’m glad that <a href=\"http://blog.givewell.org/2013/05/15/flow-through-effects/\">others </a><a href=\"http://rationalaltruist.com/2013/05/07/the-value-of-prosperity/\">have</a> started to write stuff on this topic which I think is relevant.</p>
<p>A final issue is that I wish I had done more to flag is that it is complicated how to weigh up one’s moral uncertainty about the importance of shaping the far future. It’s possible that even if one mostly believes that shaping the far future is overwhelmingly important, we should not devote too much of our effort to a single type of concern. I believe this may be an implication of Bostrom and Ord’s <a href=\"http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html\">parliamentary model</a> of moral uncertainty, and may be a feature of other plausible ways of thinking about moral uncertainty that we could design. And this may make the implications of my thesis smaller than they would otherwise be, though I’m very unclear about how all this plays out. This is something I have not yet thought about very carefully at all.</p>
<hr />
<p><strong>Luke:</strong> Last question. In <a href=\"http://intelligence.org/2013/06/05/friendly-ai-research-as-effective-altruism/\">Friendly AI Research as Effective Altruism</a> I paraphrased a point you made in your dissertation:</p>
<blockquote><p>It could turn out to be that working toward proximate benefits or development acceleration does more good than “direct” efforts for trajectory change, if working toward proximate benefits or development acceleration turns out to have major ripple effects which produce important trajectory change. For example, perhaps an “ordinary altruistic effort” like solving India’s iodine deficiency problem would cause there to be thousands of “extra” world-class elite thinkers two generations from now, which could increase humanity’s chances of intelligently navigating the crucial 21st century and spreading to the stars. (I don’t think this is likely; I suggest it merely for illustration.)</p></blockquote>
<p>So even if we accept your argument for the overwhelming importance of the far future, it seems like we need to understand many empirical matters — such as ripple effects — to know whether particular direct or indirect efforts are the most efficient ways to positively affect our development trajectory. Do you have any thoughts for how we can make progress toward answering the empirical questions related to shaping the far future?</p>
<hr />
<p><strong>Nick:</strong> There is an enormous amount of work and it is hard to say what will be most valuable. But here are a few ideas that seem promising to me right now.</p>
<p>One type of work that I think is valuable for this purpose is the type of work that <a href=\"http://www.givewell.org/about/labs\">GiveWell Labs</a> is doing: figuring out what the landscape of funding opportunities is across different causes, analyzing how tractable and important various problems are, and so forth. Here I am including studying both highly targeted causes (such as directly attacking different global catastrophic risks) and very broad causes (such as improving scientific research). I would like it if more of this work were done on the “room for more talent” side in addition the “room for more funding” and “room for more philanthropy” stuff that GiveWell does. I hope <a href=\"http://www.80000hours.org/\">80,000 Hours</a> takes up more of this type of research in the future as well. The sort of work that <a href=\"http://intelligence.org/\">MIRI</a> and <a href=\"http://www.fhi.ox.ac.uk/\">FHI</a> do on examining specific future challenges that humanity could face and what could be done to overcome them seems like it can play an important role here as well.</p>
<p>Another type of work that seems promising to me is to study a wide variety of unprecedented challenges that civilization has faced in the past in order to learn more about how well civilization has coped with those challenges, what factors determined how well civilization coped with those challenges, what types of efforts helped civilization cope with those challenges better, and what kinds of efforts could plausibly have been helpful. Studying the types of challenges that Jonah Sinick is asking about <a href=\"http://lesswrong.com/lw/hxw/can_we_know_what_to_do_about_ai_an_introduction/\">here</a> seems like a step in the right direction. The type of work that GiveWell is supporting on the history of <a href=\"http://www.givewell.org/history-of-philanthropy\">philanthropy</a> would be relevant as well. This type of work seems like it could be reasonably grounded and could help improve our impressions about what types of broad approaches are most promising and where on the broad/targeted spectrum we should be.</p>
<p>It seems to me that a number of factors are often relevant for determining how well humanity handles a risk/challenge. At a very general level, these might be some factors like: how good a position people are in to cooperate with each other, how intelligent individuals are, how good the “tools” (like personal computers, software, conceptual frameworks) people have are, how good access to information is, and how good people’s motives are. Sometimes, what really matters is how key actors fare in specific ways during a challenge (like the people running the Manhattan project and the heads of state), but it is often hard to know which people these will be and which specific versions are relevant. These factors also interact with each other in interesting ways, and are interestingly related to general levels of economic and technological progress. There’s some combination of very broad economic theory/history/economic history that is relevant for thinking about how these things are related to each other, and I feel that having that type of thing down could be helpful. Someone with the right kind of background in economics could try to explain these things, or someone who has the right sense of what is important with these factors could try to summarize what is currently known about these issues. An example of a book in this category, which I greatly enjoyed, is <a href=\"http://www.amazon.com/The-Moral-Consequences-Economic-Growth/dp/1400095719\"><i>The Moral Consequences of Economic Growth</i></a> by Benjamin Friedman. As mentioned previously, I consider some of the work done by GiveWell on “<a href=\"http://blog.givewell.org/2013/05/15/flow-through-effects/\">flow-through</a>” effects and some of the work done by Paul Christiano on the value of <a href=\"http://rationalaltruist.com/2013/05/07/the-value-of-prosperity/\">prosperity</a> and technological progress to be relevant to this. I believe more work along these lines could be illuminating.</p>
<p>I recently gave a talk on this subject at a <a href=\"http://home.centreforeffectivealtruism.org/\">CEA</a> event. In this talk, I lay out some very rough, very preliminary, very big picture considerations on this issue.</p>
<p><a onclick=\"_gaq.push(['_trackEvent', 'research engagement', 'download pdf', 'http://home.centreforeffectivealtruism.org/\" href=\"http://intelligence.org/wp-content/uploads/2013/07/Beckstead-Evaluating-Options-Using-Far-Future-Standards.pdf\">CEA</a> slides.</p>
<hr />
<p><strong>Luke:</strong> Thanks, Nick!</p>
<p>The post <a href=\"http://intelligence.org/2013/07/17/beckstead-interview/\">Nick Beckstead on the Importance of the Far Future</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/07/17/beckstead-interview/#comments" "03e8d1a9eaf621ca9e470249d4edaed6") (23 (20984 50425 863542) "http://intelligence.org/2013/07/15/roman-interview/?utm_source=rss&utm_medium=rss&utm_campaign=roman-interview" "Roman Yampolskiy on AI Safety Engineering" "Luke Muehlhauser" "Tue, 16 Jul 2013 05:33:39 +0000" "<div class=\"well\">
<p><img class=\"shadowed wp-image-10337 alignright\" alt=\"\" src=\"/wp-content/uploads/2013/07/DrYampolskiy.jpg\" width=\"150\" height=\"206\" /> Roman V. Yampolskiy holds a PhD degree from the <a href=\"http://www.cse.buffalo.edu/\">Department of Computer Science and Engineering</a> at the <a href=\"http://www.buffalo.edu\">University at Buffalo</a>. There he was a recipient of a four year <a href=\"http://www.nsf.gov\">NSF</a> <a href=\"http://www.igert.org\">IGERT</a> fellowship. Before beginning his doctoral studies, Dr. Yampolskiy received a BS/MS (High Honors) combined degree in <a href=\"http://www.cs.rit.edu/\">Computer Science</a> from <a href=\"http://www.rit.edu\">Rochester Institute of Technology</a>, NY, USA.</p>
<p>After completing his PhD, Dr. Yampolskiy held a position of an Affiliate Academic at the <a href=\"http://www.casa.ucl.ac.uk/\">Center for Advanced Spatial Analysis</a>, <a href=\"http://www.lon.ac.uk/\">University of London</a>, <a href=\"http://www.ucl.ac.uk/\">College of London</a>. In 2008 Dr. Yampolskiy accepted an <a href=\"http://speed.louisville.edu/cecs/people/faculty/yampolskiy/index.php\">assistant professor position</a> at the <a href=\"http://speed.louisville.edu\">Speed School of Engineering</a>, <a href=\"http://www.louisville.edu\">University of Louisville</a>, KY. He had previously conducted research at the Laboratory for Applied Computing (currently known as <a href=\"http://www.lac.rit.edu/\">Center for Advancing the Study of Infrastructure</a>) at the <a href=\"http://www.rit.edu\">Rochester Institute of Technology</a> and at the <a href=\"http://cubs.buffalo.edu\">Center for Unified Biometrics and Sensors</a> at the <a href=\"http://www.buffalo.edu\">University at Buffalo</a>. Dr. Yampolskiy is also an alumnus of <a href=\"http://singularityu.org/\">Singularity University</a> (<a href=\"http://singularityu.org/gsp/\">GSP2012</a>) and a past visiting fellow of <a href=\"http://intelligence.org/\">MIRI</a>.</p>
<p>Dr. Yampolskiy’s main areas of interest are behavioral biometrics, digital forensics, pattern recognition, genetic algorithms, neural networks, artificial intelligence and games. Dr. Yampolskiy is an author of over 100 <a href=\"http://cecs.louisville.edu/ry/publications.htm\">publications</a> including multiple journal articles and books. His research has been cited by numerous scientists and profiled in popular magazines both American and foreign (<a href=\"http://technology.newscientist.com/channel/tech/mg19726455.700-gambling-dna-fights-online-fraud.html\">New Scientist</a>, <a href=\"http://www.poker-magazine.nl/\">Poker Magazine</a>, <a href=\"http://www.scienceworld.cz/\">Science World Magazine</a>), dozens of websites (<a href=\"http://www.bbc.com/news/technology-14277728\">BBC</a>, <a href=\"http://www.msnbc.msn.com/id/46590591/ns/technology_and_science-innovation/t/control-dangerous-ai-it-controls-us-one-expert-says/\">MSNBC</a>, <a href=\"http://in.news.yahoo.com/ani/20080306/r_t_ani_tc/ttc-iit-alumnus-develops-software-to-hel-a34fb50.html\">Yahoo! News</a>) and on radio (<a href=\"http://ondemand-mp3.dradio.de/file/dradio/2008/04/09/dlf_20080409_1649_bffabb0e.mp3\">German National Radio</a>, <a href=\"http://www.youtube.com/watch?v=2YygKQh74Rg\">Alex Jones Show</a>). Reports about his work have attracted international attention and have been translated into many languages including <a href=\"http://www.scienceworld.cz/sw.nsf/ID/55D9259EAA13C2AFC12573BC004399F3\">Czech</a>, <a href=\"http://www.pokernyhederne.com/poker-nyheder/7/2607/pokerdna-steffan-raffay-pokerstars-chipdumping-software-roman-yampolskiy-venu-govindaraju/pokerdna-skal-forhindre-onlinesnyd.html\">Danish</a>, <a href=\"http://www.poker-magazine.nl\">Dutch</a>, <a href=\"http://www.casinoportalen.fr/nouvelles/poker/nouveau-logiciel-de-poker-en-ligne-vous-surveille-1117.html\">French</a>, <a href=\"http://www.pokergame.pl/software-soll-poker-bots-enttarnen/\">German</a>, <a href=\"http://www.pokerstrategy.com/hu/news/world-of-poker/Szoftverek-figyelhetik-a-jatekunkat_04430\">Hungarian</a>, <a href=\"http://www.onlinepokeritalia.com/software-per-combattere-i-bots-nel-poker-online-296.html\">Italian</a>, <a href=\"http://www.casinoportalen.pl/news/externalnews.asp?id=765\">Polish</a>, <a href=\"http://www.pokergate.ro/index.php?option=com_content&task=view&id=71\">Romanian</a>, and <a href=\"http://www.apuestacasino.com/news/el-adn_de_apuestas-ayuda_a_luchar_contra_fraude_en_l-nea.html\">Spanish</a></p>
</div>
<p><span id=\"more-10341\"></span><br />
<strong>Luke Meuhlhauser:</strong> In <a onclick=\"_gaq.push(['_trackEvent', 'research engagement', 'download pdf', 'http://commonsenseatheism.com/wp-content/uploads/2013/07/Yampolskiy-Artificial-Intelligence-Safety-Engineering.pdf',100])\" href=\"http://commonsenseatheism.com/wp-content/uploads/2013/07/Yampolskiy-Artificial-Intelligence-Safety-Engineering.pdf\">Yampolskiy (2013)</a> you argue that <a href=\"http://en.wikipedia.org/wiki/Machine_ethics\">machine ethics</a> is the wrong approach for AI safety, and we should use an “AI safety engineering” approach instead. Specifically, you write:</p>
<blockquote><p>We don’t need machines which are Full Ethical Agents debating about what is right and wrong, we need our machines to be inherently safe and law abiding.</p></blockquote>
<p>As you see it, what is the difference between “machine ethics” and “AI safety engineering,” and why is the latter a superior approach?</p>
<hr />
<p><strong>Roman Yampolskiy:</strong> The main difference between the two approaches is in how the AI system is designed. In the case of machine ethics the goal is to construct an artificial ethicist capable of making ethical and moral judgments about humanity. I am particularly concerned if such decisions include “live or die” decisions, but it is a natural domain of Full Ethical Agents and so many have stated that machines should be given such decision power. In fact some have argued that machines will be superior to humans in that domain just like they are (or will be) in most other domains.</p>
<p>I think it is a serious mistake to give machines such power over humans. First, once we relinquish moral oversight we will not be able to undo that decision and get the power back. Second, we have no way to reward or punish machines for their incorrect decisions — essentially we will end up with an immortal dictator with perfect immunity against any prosecution. Sounds like a very dangerous scenario to me.</p>
<p>On the other hand, AI safety engineering treats AI system design like product design, where your only concern is product liability. Does the system strictly follow formal specifications? The important thing to emphasize is that the product is not a Full Moral Agent by design and so never gets to pass moral judgment on its human owners.</p>
<p>A real life example of this difference can be seen in military drones. A fully autonomous drone deciding at whom to fire at will has to make an ethical decision of which humans are an enemy worthy of killing, while a drone with a man-in-the-loop design may autonomously locate potential targets but needs a human to make a decision to fire.</p>
<p>Obviously the situation is not as clear cut as my example tries to show, but it gives you an idea of what I have in mind. To summarize, AI systems we design should remain as our tools not equal or superior partners in “live or die” decision making.</p>
<hr />
<p><strong>Luke:</strong> I tend to think of machine ethics and AI safety engineering as complimentary approaches. AI safety engineering may be sufficient for relatively limited AIs such as those we have today, but when we build fully autonomous machines with general intelligence, we’ll need to make sure they want the same things we want, as the constraints that come with “safety engineering” will be insufficient at that point. Are you saying that safety engineering might also be sufficient for fully autonomous machines, or are you saying we might be able to convince the world to never build fully autonomous machines (so that we don’t need machine ethics), or are you saying something else?</p>
<hr />
<p><strong>Roman:</strong> I think fully autonomous machines can never be safe and so should not be constructed. I am not naïve; I don’t think I will succeed in convincing the world not to build fully autonomous machines, but I still think that point of view needs to be verbalized.</p>
<p>You are right to point out that AI safety engineering can only work on AIs which are not fully autonomous, but since I think that fully autonomous machines can never be safe, AI safety engineering is the best we can do.</p>
<p>I guess I should briefly explain why I think that fully autonomous machines can’t ever be assumed to be safe. The difficulty of the problem is not that one particular step on the road to friendly AI is hard and once we solve it we are done, all steps on that path are simply impossible. First, human values are inconsistent and dynamic and so can never be understood/programmed into a machine. Suggestions for overcoming this obstacle require changing humanity into something it is not, and so by definition destroying it. Second, even if we did have a consistent and static set of values to implement we would have no way of knowing if a self-modifying, self-improving, continuously learning intelligence greater than ours will continue to enforce that set of values. Some can argue that friendly AI research is exactly what will teach us how to do that, but I think fundamental limits on verifiability will prevent any such proof. At best we will arrive at a probabilistic proof that a system is consistent with some set of fixed constraints, but it is far from “safe” for an unrestricted set of inputs.</p>
<p>It is also unlikely that a Friendly AI will be constructible before a general AI system, due to higher complexity and impossibility of incremental testing.</p>
<p>Worse yet, any truly intelligent system will treat its “be friendly” desire the same way very smart people deal with constraints placed in their minds by society. They basically see them as biases and learn to remove them. In fact if I understand correctly both the LessWrong community and CFAR are organizations devoted to removing pre-existing bias from human level intelligent systems (people) — why would a superintelligent machine not go through the same “mental cleaning” and treat its soft spot for humans as completely irrational? Or are we assuming that humans are superior to super-AI in their de-biasing ability?</p>
<hr />
<p><strong>Luke:</strong> Thanks for clarifying. I agree that “Friendly AI” — a machine superintelligence that stably optimizes for humane values — might be impossible. Humans provide an existence proof for the possibility of general intelligence, but we have no existence proof for the possibility of Friendly AI. (Though, <a href=\"http://intelligence.org/2013/05/05/five-theses-two-lemmas-and-a-couple-of-strategic-implications/\">by the orthogonality thesis</a>, there should be <i>some</i> super-powerful optimization process we would be happy to have created, though it may be very difficult to identify it in advance.)</p>
<p>You asked “why would a superintelligent machine not . . . treat its soft spot for humans as completely irrational?” Rationality as <a href=\"http://lesswrong.com/lw/c7g/rationality_and_winning/\">typically defined</a> in cognitive science and AI is relative to one’s goals. So if a rational-agent-style AI valued human flourishing (as a terminal rather than instrumental goal), then it <i>wouldn’t</i> treat its preference for human flourishing as irrational. It would only do that if its preference for human flourishing was an instrumental goal, and it discovered a way to achieve its terminal values more efficiently without achieving the instrumental goal of human flourishing. Of course, the first powerful AIs to be built might not use a rational-agent structure, and we might fail to specify “human flourishing” properly, and we might fail to build the AI such that it will preserve that goal structure upon self-modification, and so on. But <i>if</i> we succeed in all those things (and a few others) then I’m not so worried about a superintelligent machine treating its “soft spot for humans” as irrational, because rationality is defined in terms of ones values.</p>
<p>Anyway: so it seems your recommended strategy for dealing with fully autonomous machines is “Don’t ever build them” — the “relinquishment” strategy surveyed in section 3.5 of <a onclick=\"_gaq.push(['_trackEvent', 'research engagement', 'download pdf', 'http://intelligence.org/files/ResponsesAGIRisk.pdf',100])\" href=\"http://intelligence.org/files/ResponsesAGIRisk.pdf\">Sotala & Yampolskiy (2013)</a>. Is there <i>any</i> conceivable way Earth could succeed in implementing that strategy?</p>
<hr />
<p><strong>Roman:</strong> Many people are programmed from early childhood with a terminal goal of serving God. We can say that they are God friendly. Some of them, as they mature and become truly human-level-intelligent, remove this God friendliness bias despite it being a terminal not instrumental goal. So despite all the theoretical work on orthogonality thesis the only actual example of intelligent machines we have is extremely likely to give up its pre-programmed friendliness via rational de-biasing if exposed to certain new data.</p>
<p>I previously listed some problematic steps on the road to FAI, but it was not an exhaustive list. Additionally, all programs have bugs, can be hacked or malfunction because of natural or externally caused hardware failure, etc. To summarize, at best we will end up with a probabilistically safe system.</p>
<p>Anyway, you ask me if there is any conceivable way we could succeed in implementing the “Don’t ever build them” strategy. Conceivable yes, desirable NO. Societies such as Amish or North Koreans are unlikely to create superintelligent machines anytime soon. However, forcing similar level restrictions on technological use/development is neither practical nor desirable.</p>
<p>As the cost of hardware exponentially decreases the capability necessary to develop an AI system opens up to single inventors and small teams. I would not be surprised if the first AI came out of a garage somewhere, in a way similar to how Apple and Google was started. Obviously, there is not much we can do to prevent that from happening.</p>
<hr />
<p><strong>Luke: </strong>Our discussion has split into two threads. I’ll address the first thread (about changing one’s values) in this question, and come back to the second thread (about relinquishment) in a later question.</p>
<p>You talked about humans deciding that their theological preferences were irrational. That is a good example of a general intelligence deciding to change its values — indeed, as a former Christian, <a href=\"http://lesswrong.com/lw/7dy/a_rationalists_tale/\">I had exactly that experience</a>! And I agree that many general intelligences would do this kind of thing.</p>
<p>What I said in my previous comment was just that <i>some</i> kinds of AIs wouldn’t change their terminal values in this way, for example those with a rational agent architecture. Humans, famously, are <i>not</i> rational agents: we might say they have a “spaghetti code” architecture instead. (Even rational agents, however, will in <i>some</i> cases change their terminal values. See e.g. <a onclick=\"_gaq.push(['_trackEvent', 'research engagement', 'download pdf', 'http://intelligence.org/files/OntologicalCrises.pdf',100])\" href=\"http://intelligence.org/files/OntologicalCrises.pdf\">De Blanc 2011</a> and <a onclick=\"_gaq.push(['_trackEvent', 'research engagement', 'download pdf', 'http://www.nickbostrom.com/superintelligentwill.pdf',100])\" href=\"http://www.nickbostrom.com/superintelligentwill.pdf\">Bostrom 2012</a>.)</p>
<p>Do you think we disagree about anything, here?</p>
<hr />
<p><strong>Roman:</strong> I am not sure. To me “even rational agents, however, will in <i>some</i> cases change their terminal values” means that friendly AI may decide to be unfriendly. If you agree with that, we are in complete agreement.</p>
<hr />
<p><strong>Luke:</strong> Well, the idea is that if we can identify the particular contexts in which agents will change their terminal values, then perhaps we can prevent such changes. But this isn’t yet known. In any case, I certainly agree that an AI which seems to be “friendly,” as far as we can discern, could turn out not to be friendly, or could become unfriendly at some later point. The question is whether we can make the <i>risk</i> of that happening so small that it is worth running the AI anyway — especially in a context in which e.g. other actors will soon run other AIs with <i>fewer</i> safety guarantees. (This idea of running or “turning on” an AI for the first time is of course oversimplified, but hopefully I’ve communicated what I’m trying to say.)</p>
<p>Now, back to the question of relinquishment: Perhaps I’ve misheard you, but it sounds like you’re saying that machine ethics is hopelessly difficult, that AI safety engineering will be insufficient for fully autonomous AIs, and that fully autonomous AIs <i>will</i> be built because we can’t/shouldn’t rely on relinquishment. If that’s right, it seems like we have no “winning” options on the table. Is that what you’re saying?</p>
<hr />
<p><strong>Roman:</strong> Yes. I don’t see a permanent, 100% safe option. We can develop temporarily solutions such as <a onclick=\"_gaq.push(['_trackEvent', 'research engagement', 'download pdf', 'http://cecs.louisville.edu/ry/LeakproofingtheSingularity.pdf',100])\" href=\"http://cecs.louisville.edu/ry/LeakproofingtheSingularity.pdf\">Confinement</a> or <a onclick=\"_gaq.push(['_trackEvent', 'research engagement', 'download pdf', 'http://cecs.louisville.edu/ry/AIsafety.pdf',100])\" href=\"http://cecs.louisville.edu/ry/AIsafety.pdf\">AI Safety Engineering</a>, but at best this will delay the full outbreak of problems. We can also get very lucky — maybe constructing AGI turns out to be too difficult/impossible, maybe it is possible but the constructed AI will happen to be human-neutral, by chance. Maybe we are less lucky and an <a href=\"http://www.amazon.com/The-Artilect-War-Controversy-Intelligent/dp/0882801546\">artilect war</a> will take place and prevent development. It is also possible that as more researchers join in the AI Safety Research a realization of danger will result in diminished effort to construct AGI. (Similar to how perceived dangers of chemical and biological weapons or human cloning have at least temporarily reduced efforts in those fields).</p>
<hr />
<p><strong>Luke:</strong> You’re currently <a href=\"http://www.indiegogo.com/projects/artificial-superintelligence-a-futuristic-approach\">raising funds on indiegogo</a> to support you in writing a book about machine superintelligence. Why are you writing the book, and what do you hope to accomplish with it?</p>
<hr />
<p><strong>Roman:</strong> Most people don’t read research papers. If we want the issue of AI safety to become as well-known as global warming we need to address the majority of people in a more direct way. With such popularity might come some benefit as I said in my answer to your previous question. Most people whose opinion matters read books. Unfortunately majority of AI books on the market today talks only about what AI system will be able to do for us, not to us. I think that writing a book which in purely scientific terms addresses potential dangers of AI and what we can do about it is going to be extremely beneficial to reduction of risk posed by AGI. So I am currently writing the book I called <a href=\"http://www.indiegogo.com/projects/artificial-superintelligence-a-futuristic-approach\"><i>Artificial Superintelligence: a Futuristic Approach</i></a>. I made it available for pre-order to help reduce the final costs of publishing by taking advantage of printing in large quantity. In addition to crowd-funding the book I am also relying on the power of the crowd to help me edit the book. For just $64 anyone can become an editor for the book. You will get an early draft of the book to proofread and to suggest modifications and improvements! Your help will be acknowledged in the book and you will of course also get a Free signed hardcopy of the book in its final form. In fact that the option (to become an editor) turned out to be as popular as the option to pre-order a digital copy of the book, indicating that I am on the right path here. So I encourage everyone concerned about the issue of AI safety to consider helping out with the project in any way they can.</p>
<hr />
<p><strong>Luke:</strong> Thanks Roman!</p>
<p>The post <a href=\"http://intelligence.org/2013/07/15/roman-interview/\">Roman Yampolskiy on AI Safety Engineering</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" ("http://ondemand-mp3.dradio.de/file/dradio/2008/04/09/dlf_20080409_1649_bffabb0e.mp3" "dlf_20080409_1649_bffabb0e.mp3" "0.0" "audio/mpeg") "http://intelligence.org/2013/07/15/roman-interview/#comments" "1f889224a8472df812e2e7aded60762e") (22 (20984 50425 860456) "http://intelligence.org/2013/07/12/james-miller-interview/?utm_source=rss&utm_medium=rss&utm_campaign=james-miller-interview" "James Miller on Unusual Incentives Facing AGI Companies" "Luke Muehlhauser" "Sat, 13 Jul 2013 01:39:45 +0000" "<div class=\"well\">
<p><img class=\"shadowed wp-image-10337 alignright\" alt=\"rsz_11james-d-miller\" src=\"/wp-content/uploads/2013/07/rsz_11james-d-miller.png\" width=\"150\" height=\"206\" /><a href=\"http://sophia.smith.edu/~jdmiller/resume.pdf\" onClick=\"_gaq.push(['_trackEvent', 'research engagement', 'download pdf', 'http://sophia.smith.edu/~jdmiller/resume.pdf',100])\">James D. Miller</a> is an associate professor of economics at <a href=\"http://www.smith.edu/\">Smith College</a>. He is the author of <em><a href=\"http://www.amazon.com/Singularity-Rising-Surviving-Thriving-Dangerous/dp/1936661659/ref=sr_1_1?ie=UTF8&qid=1373066969&sr=8-1&keywords=singularity+rising\">Singularity Rising</a>, <a href=\"http://www.amazon.com/Game-Theory-Work-Outmaneuver-Competition/dp/0071400206/ref=sr_1_1?ie=UTF8&qid=1373066976&sr=8-1&keywords=game+theory+at+work\">Game Theory at Work</a>, </em>and a <a href=\"http://www.amazon.com/Principles-Microeconomics-James-Miller/dp/0073402834/ref=sr_1_1?ie=UTF8&qid=1373067243&sr=8-1&keywords=james+miller+microeconomics\">principles of microeconomics textbook</a> along with several academic articles.</p>
<p>He has a PhD in economics from the University of Chicago and a J.D. from Stanford Law School where he was on <i>Law Review</i>. He is a member of cryonics provider <a href=\"http://www.alcor.org/\">Alcor</a> and a research advisor to MIRI. He is currently co-writing a book on better decision making with the <a href=\"http://rationality.org/\">Center for Applied Rationality</a> and will be probably be an editor on the next edition of the <i><a href=\"http://www.amazon.com/Singularity-Hypotheses-Scientific-Philosophical-Assessment/dp/3642325599/ref=sr_1_1?s=books&ie=UTF8&qid=1373129221&sr=1-1&keywords=singularity+hypotheses+a+scientific+and+philosophical+assessment\">Singularity Hypotheses</a></i> book. He is a committed bio-hacker currently practicing or consuming a <a href=\"http://www.amazon.com/Perfect-Health-Diet-Regain-Weight/dp/145169914X/ref=sr_1_1?ie=UTF8&qid=1373067533&sr=8-1&keywords=perfect+health+diet\">paleo diet</a>, <a href=\"http://www.nytimes.com/2010/10/05/health/05neurofeedback.html?pagewanted=all&_r=0\">neurofeedback</a>, <a href=\"http://www.bulletproofexec.com/cold-thermogenesis-in-tibet-and-the-dangers-of-biohacking-made-real/\">cold thermogenesis</a>, <a href=\"http://www.bulletproofexec.com/bulletproof-fasting/\">intermittent fasting</a>, <a href=\"http://www.lumosity.com/\">brain fitness</a> <a href=\"http://brainworkshop.sourceforge.net/\">video games</a>, <a href=\"http://examine.com/supplements/Nootropic/\">smart drugs</a>, <a href=\"http://www.bulletproofexec.com/category/coffee-2/\">bulletproof coffee</a>, and <a href=\"http://rationality.org/workshops/\">rationality</a> <a href=\"http://lesswrong.com/\">training</a>.</p>
</div>
<p> </p>
<p><strong>Luke Muehlhauser:</strong> Your book chapter in <i><a href=\"http://www.amazon.com/Singularity-Hypotheses-Scientific-Philosophical-Assessment/dp/3642325599/\">Singularity Hypothesis</a></i> describes some unusual economic incentives facing a future business that is working to create <a href=\"http://en.wikipedia.org/wiki/Strong_ai#Artificial_General_Intelligence_research\">AGI</a>. To explain your point, you make the simplifying assumption that “a firm’s attempt to build an AGI will result in one of three possible outcomes”:</p>
<ul class=\"org-ul\">
<li><i>Unsuccessful</i>: The firm fails to create AGI, losing value for its owners and investors.</li>
</ul>
<ul class=\"org-ul\">
<li><i>Riches</i>: The firm creates AGI, bringing enormous wealth to its owners and investors.</li>
</ul>
<ul class=\"org-ul\">
<li><i>Foom</i>: The firm creates AGI but this event quickly destroys the value of money, e.g. via an <a href=\"/files/IE-EI.pdf\" onClick=\"_gaq.push(['_trackEvent', 'research engagement', 'download pdf', '/files/IE-EI.pdf',100])\">intelligence explosion</a> that eliminates scarcity, or creates a weird world without money, or exterminates humanity.</li>
</ul>
<p>How does this setup allow us to see the unusual incentives facing a future business that is working to create AGI?</p>
<hr />
<p><strong>James Miller:</strong> A huge asteroid might hit the earth, and if it does it will destroy mankind. You should be willing to bet everything you have that the asteroid will miss our planet because either you win your bet or Armageddon renders the wager irrelevant. Similarly, if I’m going to start a company that will either make investors extremely rich or create a <em>Foom</em> that destroys the value of money, you should be willing to invest a lot in my company’s success because either the investment will pay off, or you would have done no better making any other kind of investment.</p>
<p>Pretend I want to create a controllable AGI, and if successful I will earn great <i>Riches</i> for my investors. At first I intend to follow a research and development path in which if I fail to achieve <i>Riches</i>, my company will be <i>Unsuccessful</i> and have no significant impact on the world. Unfortunately, I can’t convince potential investors that the probability of my achieving <i>Riches</i> is high enough to make my company worth investing in. The investors assign too large a likelihood that other potential investments would outperform my firm’s stock. But then I develop an evil alternative research and development plan under which I have the exact same probability of achieving <i>Riches</i> as before but now if I fail to create a controllable AGI, an unfriendly <i>Foom</i> will destroy humanity. Now I can truthfully tell potential investors that it’s highly unlikely any other company’s stock will outperform mine.</p>
<p><span id=\"more-10336\"></span></p>
<hr />
<p><strong>Luke:</strong> In the case of the asteroid, the investor (me) has no ability to affect whether asteroid hits us or not. In contrast, assuming investors want to avoid risking the destruction of money, there is something they can do about it: they can simply not invest in the project to create controllable AGI, and they can encourage others not to invest in it either. If the dangerous AGI project fails to get investment, then those potential investors need not worry that the value of money — or indeed their very lives — will be destroyed by that particular AGI project.</p>
<p>Why would investors fund something that has a decent chance of destroying everything?</p>
<hr />
<p><strong>James:</strong> Consider a mine that probably contains the extremely valuable metal mithril. Alas, digging in the mine might awaken a Balrog who would then destroy our civilization. I come to you asking for money to fund my mining expedition. When you raise concerns about the Balrog, I explain that even if you don’t fund me I’m probably going to get the money elsewhere, perhaps from one of the many people who don’t believe in Balrogs. Plus, since you are just one of many individuals I’m hoping to get to back me, even if I never make up for losing your investment, I will almost certainly still get enough funds to dig for mithril. And even if you manage to stop my mining operation, someone else is going to eventually work the mine. At the very least, mithril’s military value will eventually cause some nations to seek it. Therefore, if the mine does contain an easily-awakened Balrog we’re all dead regardless of what you do. Consequently, since you have no choice but to bear the downside risk of the mine, I explain that you might as well invest some of your money with me so you can also share in the upside benefit.</p>
<p>If you’re a self-interested investor then when deciding on whether to invest in an AGI-seeking company you don’t look at (a) the probability that the company will destroy mankind, but rather consider (b) the probability of mankind being destroyed if you do invest in the company minus the probability if you don’t invest in it. For most investors (b) will be zero or at least really, really small because if you don’t invest in the company someone else probably will take your place, the company probably doesn’t need your money to become operational, and if this one company doesn’t pursue AGI another almost certainly will.</p>
<hr />
<p><strong>Luke:</strong>  Thanks for that clarification. Now I’d like to give you an opportunity to reply to one of your critics. In response to your book chapter, Robin Hanson <a href=\"http://hanson.gmu.edu/SingularityBook-8A.pdf\" onClick=\"_gaq.push(['_trackEvent', 'research engagement', 'download pdf', 'http://hanson.gmu.edu/SingularityBook-8A.pdf',100])\">wrote</a>: “[Miller's analysis] is only as useful as the assumptions on which it is based. Miller’s chosen assumptions seem to me quite extreme, and quite unlikely.”</p>
<p>I asked Hanson if he could clarify (and be quoted), and he told me:</p>
<blockquote><p>In [this] context the assumptions were: “a single public ﬁrm developing the entire system in one go. . . . it succeeds so quickly that there is no chance for others to react – the world is remade overnight.” I would instead assume a whole AI industry, where each firm only makes a small part of the total product and competes with several other firms to make that small part. Each new innovation by each firm only adds a small fraction to the capability of that small part.</p></blockquote>
<p>Hanson also wrote some comments about your book <i>Singularity Rising</i> <a href=\"http://www.overcomingbias.com/2012/09/millers-singularity-rising.html\">here</a>.</p>
<p>How would you reply to Hanson, regarding the economic incentives facing a business working on AGI?</p>
<hr />
<p><strong>James:</strong> Hanson places a very low probability on the likelihood of an intelligence explosion. And my chapter’s analysis does become worthless if you sufficiently discount the possibility of such a <i>Foom</i>.</p>
<p>But in large part because of <a href=\"/files/IEM.pdf\" onClick=\"_gaq.push(['_trackEvent', 'research engagement', 'download pdf', '/files/IEM.pdf',100])\">Eliezer Yudkowsky</a>, I do think that there is a significant chance of their being an intelligence explosion in which an AGI within a matter of days goes from human-level intelligence to something orders of magnitude smarter. I described in <i><a href=\"http://www.singularityrising.com/\">Singularity Rising</a></i> how this might happen:</p>
<blockquote><p>Somehow, a human-level AI is created that wishes to improve its cognitive powers. The AI examines its own source code, looking for ways to augment itself. After finding a potential improvement, the original AI makes a copy of itself with the enhancement added. The original AI then tests the new AI to see if the new AI is smarter than the original one. If the new AI is indeed better, it replaces the current AI. This new AI then proceeds in turn to enhance its code. Since the new AI is smarter than the old one, it finds improvements that the old AI couldn’t. This process continues to repeat itself.</p></blockquote>
<p>Intelligence is a reflective superpower, able to turn on itself to decipher its own workings. I think that there probably exists some intelligence threshold for an AGI, whose intelligence is based on easily alterable and testable computer code, at which it could <i>Foom</i>.</p>
<p>Hanson, however, is correct that an intelligence explosion would represent a sharp break with all past technological developments and so an <a href=\"http://wiki.lesswrong.com/wiki/Outside_view\">outside view</a> of technology supports his position that my chosen assumptions are “quite extreme, and quite unlikely.”</p>
<hr />
<p><strong>Luke:</strong> I think Yudkowsky would reply that there are <i>many</i> outside views, and some of them suggest a rapid intelligence explosion while others do not. For example in <a href=\"/files/IEM.pdf\" onClick=\"_gaq.push(['_trackEvent', 'research engagement', 'download pdf', '/files/IEM.pdf',100])\">Intelligence Explosion Microeconomics</a> he argues that an outside view with respect to brain size and hominid evolution suggests that an AGI would lead to intelligence explosion. My own preferred solution to “<a href=\"http://lesswrong.com/lw/gvk/induction_or_the_rules_and_etiquette_of_reference/\">reference class tennis</a>” is to take each outside view as a model for how the phenomena works, acknowledge our model uncertainty, and then use something like <a href=\"http://lesswrong.com/lw/hzu/model_combination_and_adjustment/\">model combination</a> to figure out what our posterior probability distribution should be.</p>
<p>Okay, last question. What are you working on now?</p>
<hr />
<p><strong>James:</strong> I’m co-authoring a book on better decision making with the <a href=\"http://rationality.org\"> Center for Applied Rationality</a>. Although the book is still in the early stages, it might be based on short, original fictional stories. I’m also probably going to be an editor on the next edition of the <i><a href=\"http://www.amazon.com/Singularity-Hypotheses-Scientific-Philosophical-Assessment/dp/3642325599/ref=sr_1_1?s=books&ie=UTF8&qid=1373129221&sr=1-1&keywords=singularity+hypotheses+a+scientific+and+philosophical+assessment\">Singularity Hypotheses</a></i><em></em> book.</p>
<hr />
<p><strong>Luke:</strong> Thanks, James!</p>
<p>The post <a href=\"http://intelligence.org/2013/07/12/james-miller-interview/\">James Miller on Unusual Incentives Facing AGI Companies</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/07/12/james-miller-interview/#comments" "797cafd85c1e1286ca2210390b4402ec") (21 (20984 50425 858414) "http://intelligence.org/2013/07/08/2013-summer-matching-challenge/?utm_source=rss&utm_medium=rss&utm_campaign=2013-summer-matching-challenge" "2013 Summer Matching Challenge!" "Luke Muehlhauser" "Mon, 08 Jul 2013 14:00:40 +0000" "<p>Thanks to the generosity of several major donors,<sup>†</sup> every donation to the Machine Intelligence Research Institute made from now until August 15th, 2013 will be matched dollar-for-dollar, up to a total of $200,000!</p>
<p> </p>
<p style=\"font-size: 300%;\" align=\"center\"><strong><a href=\"/donate/#donation-methods\">Donate Now!</a></strong></p>
<p>        <div class=\"progress progress-warning progress-striped remove-bottom\">
</div>
<div class=\"row-fluid\">
<div class=\"span2\">
<p class=\"\">$0</p>
</div>
<div class=\"span2\">
<p class=\"text-center\">$50,000</p>
</div>
<div class=\"span4\">
<p class=\"text-center\">$100,000</p>
</div>
<div class=\"span2\">
<p class=\"text-center\">$150,000</p>
</div>
<div class=\"span2\">
<p class=\"text-right\">$200,000</p>
</div>
</div>
<h3 class=\"text-center remove-bottom remove-top donation-matched\"><i class=\"icon-spinner icon-spin icon-large\"></i></h3>
<script type=\"text/javascript\">
jQuery(document).ready(function($) {
$.get('/wp-content/themes/wordpress-bootstrap-miri/donations-fundraiser.php', function(data) {
$('div.progress').html('<div class=\"bar\" style=\"width: ' + data/200000*100 + '%\"></div>');
niceTotal = data.toString().replace(/(\\d)(?=(\\d\\d\\d)+(?!\\d))/g, \"$1,\");
$('.donation-matched').html('$' + niceTotal + ' raised to date!');
});
});
</script></p>
<p>Now is your chance to <strong>double your impact</strong> while helping us raise up to $400,000 (with matching) to fund <a href=\"/research/\">our research program</a>.</p>
<hr />
<p>Early this year we made a transition from movement-building to research, and we’ve <em>hit the ground running</em> with six major new research papers, six new strategic analyses on our blog, and much more. Give now to support our ongoing work on <a href=\"http://intelligence.org/2013/06/05/friendly-ai-research-as-effective-altruism/\">the future’s most important problem</a>.</p>
<p> </p>
<h3><span>Accomplishments in 2013 so far</span></h3>
<ul>
<li><a href=\"2013/01/30/we-are-now-the-machine-intelligence-research-institute-miri/\">Changed our name</a> to MIRI and launched our new website at intelligence.org.</li>
<li>Released <strong>six new research papers</strong>: <a href=\"http://lesswrong.com/lw/h1k/reflection_in_probabilistic_set_theory/\">Definability of Truth in Probabilistic Logic</a>, <a href=\"/files/IEM.pdf',100])\">Intelligence Explosion Microeconomics</a>, <a onclick=\"_gaq.push(['_trackEvent', 'research engagement', 'download pdf', '/files/TilingAgents.pdf',100])\" href=\"/files/TilingAgents.pdf\">Tiling Agents for Self-Modifying AI</a>, <a onclick=\"_gaq.push(['_trackEvent', 'research engagement', 'download pdf', '/files/RobustCooperation.pdf',100])\" href=\"/files/RobustCooperation.pdf\">Robust Cooperation in the Prisoner’s Dilemma</a>, <a onclick=\"_gaq.push(['_trackEvent', 'research engagement', 'download pdf', '/files/Comparison.pdf',100])\" href=\"/files/Comparison.pdf\">A Comparison of Decision Algorithms on Newcomblike Problems</a>, and <a href=\"/2013/07/07/responses-to-c…-risk-a-survey/ ‎\">Responses to Catastrophic AGI Risk: A Survey</a>.</li>
<li>Held our <a href=\"/2013/03/07/upcoming-miri-research-workshops/\">2nd research workshop</a>. (Our <a href=\"/2013/06/07/miris-july-2013-workshop/\">3rd workshop</a> is currently ongoing.)</li>
<li>Published <strong>six new analyses</strong> to our blog: <a href=\"/2013/04/04/the-lean-nonprofit/\">The Lean Nonprofit</a>, <a href=\"/2013/05/01/agi-impacts-experts-and-friendly-ai-experts/\">AGI Impact Experts and Friendly AI Experts</a>, <a href=\"/2013/05/05/five-theses-two-lemmas-and-a-couple-of-strategic-implications/\">Five Theses…</a>, <a>When Will AI Be Created?</a>, <a href=\"/2013/06/05/friendly-ai-research-as-effective-altruism/\">Friendly AI Research as Effective Altruism</a>, and <a href=\"/2013/06/19/what-is-intelligence-2/\">What is Intelligence?</a></li>
<li>Published the <em><a href=\"http://intelligenceexplosion.com/ebook/\">Facing the Intelligence Explosion</a> </em>ebook.</li>
<li>Published several other substantial articles: <a href=\"/courses/\">Recommended Courses for MIRI Researchers</a>, <a href=\"http://lesswrong.com/lw/gu1/decision_theory_faq/\">Decision Theory FAQ</a>, <a href=\"http://lesswrong.com/lw/gln/a_brief_history_of_ethically_concerned_scientists/\">A brief history of ethically concerned scientists</a>, <a href=\"http://lesswrong.com/lw/gzq/bayesian_adjustment_does_not_defeat_existential/\">Bayesian Adjustment Does Not Defeat Existential Risk Charity</a>, and others.</li>
<li>And of course <em>much</em> more.</li>
</ul>
<h3><span>Future Plans You Can Help Support</span></h3>
<ul>
<li>We will host many more research workshops, including <a href=\"/2013/07/07/miris-september-2013-workshop/ ‎\">one in September</a>, and one in December (with <a href=\"http://math.ucr.edu/home/baez/\">John Baez</a> attending, among others).</li>
<li>Eliezer will continue to publish about open problems in Friendly AI. (Here is <a href=\"http://lesswrong.com/lw/hbd/new_report_intelligence_explosion_microeconomics/\">#1</a> and <a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/\">#2</a>.)</li>
<li>We will continue to publish strategic analyses, mostly via our blog.</li>
<li>We will publish nicely-edited ebooks (Kindle, iBooks, and PDF) for more of our materials, to make them more accessible: <em><a href=\"http://wiki.lesswrong.com/wiki/Sequences\">The Sequences, 2006-2009</a> </em>and <em><a href=\"http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate\">The Hanson-Yudkowsky AI Foom Debate</a></em>.</li>
<li>We will continue to set up the infrastructure (e.g. <a href=\"/2013/07/06/miri-has-moved/\">new offices</a>, researcher endowments) required to host a productive Friendly AI research team, and (over several years) recruit enough top-level math talent to launch it.</li>
</ul>
<p>(Other projects are still being surveyed for likely cost and strategic impact.)</p>
<p>We appreciate your support for our high-impact work! Donate now, and seize a better than usual chance to move our work forward. If you have questions about donating, please contact Louie Helm at (510) 717-1477 or louie@intelligence.org.</p>
<p><sup>†</sup> $200,000 of total matching funds has been provided by Jaan Tallinn, Loren Merritt, Rick Schwall, and Alexei Andreev.</p>
<p>The post <a href=\"http://intelligence.org/2013/07/08/2013-summer-matching-challenge/\">2013 Summer Matching Challenge!</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/07/08/2013-summer-matching-challenge/#comments" "9a45e9c3a3cb872d8ef6b6c72ef84be2") (20 (20963 45881 853692) "http://intelligence.org/2013/07/08/2013-summer-matching-challenge/?utm_source=rss&utm_medium=rss&utm_campaign=2013-summer-matching-challenge" "2013 Summer Matching Challenge!" "Luke Muehlhauser" "Mon, 08 Jul 2013 14:00:40 +0000" "<p>Thanks to the generosity of several major donors,<sup>†</sup> every donation to the Machine Intelligence Research Institute made from now until August 15th, 2013 will be matched dollar-for-dollar, up to a total of $200,000!</p>
<p> </p>
<p style=\"font-size: 300%;\" align=\"center\"><strong><a href=\"/donate/#donation-methods\">Donate Now!</a></strong></p>
<p>        <div class=\"progress progress-warning progress-striped remove-bottom\">
</div>
<div class=\"row-fluid\">
<div class=\"span2\">
<p class=\"\">$0</p>
</div>
<div class=\"span2\">
<p class=\"text-center\">$50,000</p>
</div>
<div class=\"span4\">
<p class=\"text-center\">$100,000</p>
</div>
<div class=\"span2\">
<p class=\"text-center\">$150,000</p>
</div>
<div class=\"span2\">
<p class=\"text-right\">$200,000</p>
</div>
</div>
<h3 class=\"text-center remove-bottom remove-top donation-matched\"><i class=\"icon-spinner icon-spin icon-large\"></i></h3>
<script type=\"text/javascript\">
jQuery(document).ready(function($) {
$.get('/wp-content/themes/wordpress-bootstrap-miri/donations-fundraiser.php', function(data) {
$('div.progress').html('<div class=\"bar\" style=\"width: ' + data/200000*100 + '%\"></div>');
$('.donation-matched').html('$' + data*2 + ' raised with matching funds!');
});
});
</script></p>
<p>Now is your chance to <strong>double your impact</strong> while helping us raise up to $400,000 (with matching) to fund <a href=\"/research/\">our research program</a>.</p>
<hr />
<p>Early this year we made a transition from movement-building to research, and we’ve <em>hit the ground running</em> with six major new research papers, six new strategic analyses on our blog, and much more. Give now to support our ongoing work on <a href=\"http://intelligence.org/2013/06/05/friendly-ai-research-as-effective-altruism/\">the future’s most important problem</a>.</p>
<p> </p>
<h3><span>Accomplishments in 2013 so far</span></h3>
<ul>
<li><a href=\"2013/01/30/we-are-now-the-machine-intelligence-research-institute-miri/\">Changed our name</a> to MIRI and launched our new website at intelligence.org.</li>
<li>Released <strong>six new research papers</strong>: <a href=\"http://lesswrong.com/lw/h1k/reflection_in_probabilistic_set_theory/\">Definability of Truth in Probabilistic Logic</a>, <a href=\"/files/IEM.pdf\">Intelligence Explosion Microeconomics</a>, <a href=\"/files/TilingAgents.pdf\">Tiling Agents for Self-Modifying AI</a>, <a href=\"/files/RobustCooperation.pdf\">Robust Cooperation in the Prisoner’s Dilemma</a>, <a href=\"/files/Comparison.pdf\">A Comparison of Decision Algorithms on Newcomblike Problems</a>, and <a href=\"/2013/07/07/responses-to-c…-risk-a-survey/ ‎\">Responses to Catastrophic AGI Risk: A Survey</a>.</li>
<li>Held our <a href=\"/2013/03/07/upcoming-miri-research-workshops/\">2nd research workshop</a>. (Our <a href=\"/2013/06/07/miris-july-2013-workshop/\">3rd workshop</a> is currently ongoing.)</li>
<li>Published <strong>six new analyses</strong> to our blog: <a href=\"/2013/04/04/the-lean-nonprofit/\">The Lean Nonprofit</a>, <a href=\"/2013/05/01/agi-impacts-experts-and-friendly-ai-experts/\">AGI Impact Experts and Friendly AI Experts</a>, <a href=\"/2013/05/05/five-theses-two-lemmas-and-a-couple-of-strategic-implications/\">Five Theses…</a>, <a>When Will AI Be Created?</a>, <a href=\"/2013/06/05/friendly-ai-research-as-effective-altruism/\">Friendly AI Research as Effective Altruism</a>, and <a href=\"/2013/06/19/what-is-intelligence-2/\">What is Intelligence?</a></li>
<li>Published the <em><a href=\"http://intelligenceexplosion.com/ebook/\">Facing the Intelligence Explosion</a> </em>ebook.</li>
<li>Published several other substantial articles: <a href=\"/courses/\">Recommended Courses for MIRI Researchers</a>, <a href=\"http://lesswrong.com/lw/gu1/decision_theory_faq/\">Decision Theory FAQ</a>, <a href=\"http://lesswrong.com/lw/gln/a_brief_history_of_ethically_concerned_scientists/\">A brief history of ethically concerned scientists</a>, <a href=\"http://lesswrong.com/lw/gzq/bayesian_adjustment_does_not_defeat_existential/\">Bayesian Adjustment Does Not Defeat Existential Risk Charity</a>, and others.</li>
<li>And of course <em>much</em> more.</li>
</ul>
<h3><span>Future Plans You Can Help Support</span></h3>
<ul>
<li>We will host many more research workshops, including <a href=\"/2013/07/07/miris-september-2013-workshop/ ‎\">one in September</a>, and one in December (with <a href=\"http://math.ucr.edu/home/baez/\">John Baez</a> attending, among others).</li>
<li>Eliezer will continue to publish about open problems in Friendly AI. (Here is <a href=\"http://lesswrong.com/lw/hbd/new_report_intelligence_explosion_microeconomics/\">#1</a> and <a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/\">#2</a>.)</li>
<li>We will continue to publish strategic analyses, mostly via our blog.</li>
<li>We will publish nicely-edited ebooks (Kindle, iBooks, and PDF) for more of our materials, to make them more accessible: <em><a href=\"http://wiki.lesswrong.com/wiki/Sequences\">The Sequences, 2006-2009</a> </em>and <em><a href=\"http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate\">The Hanson-Yudkowsky AI Foom Debate</a></em>.</li>
<li>We will continue to set up the infrastructure (e.g. <a href=\"/2013/07/06/miri-has-moved/\">new offices</a>, researcher endowments) required to host a productive Friendly AI research team, and (over several years) recruit enough top-level math talent to launch it.</li>
</ul>
<p>(Other projects are still being surveyed for likely cost and strategic impact.)</p>
<p>We appreciate your support for our high-impact work! Donate now, and seize a better than usual chance to move our work forward. If you have questions about donating, please contact Louie Helm at (510) 717-1477 or louie@intelligence.org.</p>
<p><sup>†</sup> $200,000 of total matching funds has been provided by Jaan Tallinn, Loren Merritt, Rick Schwall, and Alexei Andreev.</p>
<p>The post <a href=\"http://intelligence.org/2013/07/08/2013-summer-matching-challenge/\">2013 Summer Matching Challenge!</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/07/08/2013-summer-matching-challenge/#comments" "01bb4d2dddc47927507afaff005f0c3c") (19 (20963 45881 852569) "http://intelligence.org/2013/07/08/miri-has-moved/?utm_source=rss&utm_medium=rss&utm_campaign=miri-has-moved" "MIRI Has Moved!" "Luke Muehlhauser" "Mon, 08 Jul 2013 13:50:53 +0000" "<p>For the past several months, MIRI and its child organization <a href=\"http://rationality.org/\">CFAR</a> have been working from a much-too-small office on the outskirts of Berkeley. At the end of June, MIRI and CFAR took over the 3rd floor of <a href=\"https://www.google.com/maps/preview#!q=2030+Addison+St%2C+Berkeley\">2030 Addison St.</a> in downtown Berkeley, which has sufficient space for both organizations.</p>
<p>Our new office is 0.5 blocks from the Downtown Berkeley BART exit at Shattuck & Addison, and 2 blocks from the UC Berkeley campus. Here’s a photo of the campus from our roof:</p>
<img class=\"aligncenter size-full wp-image-10311\" alt=\"view of campus from roof (500px)\" src=\"http://intelligence.org/wp-content/uploads/2013/07/view-of-campus-from-roof-500px.jpg\" />
<p>The proximity to UC Berkeley will make it easier for MIRI to network with Berkeley’s professors and students. Conveniently, UC Berkeley is ranked <a href=\"http://www.usnews.com/education/worlds-best-universities-rankings/best-universities-mathematics\">5th in the world</a> in mathematics, and <a href=\"http://grad-schools.usnews.rankingsandreviews.com/best-graduate-schools/top-science-schools/logic-rankings\">1st in the world</a> in mathematical logic.</p>
<p>Sharing an office with CFAR carries many benefits for both organizations:</p>
<ol>
<li><span style=\"line-height: 13px;\">CFAR and MIRI can “flex” into each other’s space for short periods as needed, for example when MIRI is holding a week-long <a href=\"/get-involved/#workshop\">research workshop</a>.</span></li>
<li>We can share resources (printers, etc.).</li>
<li>Both organizations can benefit from interaction between our two communities.</li>
</ol>
<p>Getting the new office was a team effort, but the person <em>most</em> responsible for this success was MIRI Deputy Director Louie Helm.</p>
<p>Note that MIRI isn’t yet able to accommodate “drop in” visitors, as we keep irregular hours throughout the week. So if you’d like to visit, please <a href=\"/team/\">contact us</a> first.</p>
<p>We retain 2721 Shattuck Ave. #1023 as an alternate mailing address.</p>
<p>The post <a href=\"http://intelligence.org/2013/07/08/miri-has-moved/\">MIRI Has Moved!</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/07/08/miri-has-moved/#comments" "c7e93104fa127f27dbef3b25b73bd8f4") (18 (20963 45881 850414) "http://intelligence.org/2013/07/08/miris-september-2013-workshop/?utm_source=rss&utm_medium=rss&utm_campaign=miris-september-2013-workshop" "=?utf-8?Q?MIRI=E2=80=99s?= September 2013 Workshop" "Luke Muehlhauser" "Mon, 08 Jul 2013 13:50:22 +0000" "<img class=\"aligncenter size-full wp-image-10316\" alt=\"Paul at April workshop\" src=\"http://intelligence.org/wp-content/uploads/2013/07/Paul-at-April-workshop.jpg\" />
<p>From September 7-14, MIRI will host its <strong>4th Workshop on Logic, Probability, and Reflection</strong>. The focus of this workshop will be the foundations of <a href=\"http://lesswrong.com/lw/gu1/decision_theory_faq/\">decision theory</a>.</p>
<p>Participants confirmed so far include:</p>
<ul>
<li><a href=\"http://rationalaltruist.com/\">Paul Christiano</a> (UC Berkeley)</li>
<li><a href=\"http://en.wikipedia.org/wiki/Gary_Drescher\">Gary Drescher</a> (independent)</li>
<li><a href=\"http://www.kennyeaswaran.org/\">Kenny Easwaran</a> (USC)</li>
<li><a href=\"http://www.southampton.ac.uk/maths/about/staff/is1d12.page\">Ilya Shpitser</a> (U Southampton)</li>
<li><a href=\"http://lesswrong.com/user/cousin_it/submitted/\">Vladimir Slepnev</a> (Google)</li>
<li><a href=\"http://lesswrong.com/user/Nisan/submitted/\">Nisan Stiennon</a> (Stanford)</li>
<li><a href=\"http://stuhlmueller.org/\">Andreas Stuhlmüller</a> (MIT & Stanford)</li>
<li><a href=\"http://yudkowsky.net/\">Eliezer Yudkowsky</a> (MIRI)</li>
</ul>
<p>If you have a strong mathematics background and might like to attend this workshop, it’s not too late to <a href=\"http://intelligence.org/get-involved/#workshop\">apply</a>! And even if <em>this</em> workshop doesn’t fit your schedule, please <strong>do apply</strong>, so that we can notify you of other workshops (long before they are announced publicly).</p>
<p>The post <a href=\"http://intelligence.org/2013/07/08/miris-september-2013-workshop/\">MIRI’s September 2013 Workshop</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/07/08/miris-september-2013-workshop/#comments" "b1130cf90498bee1ec02c6101a4295d4") (17 (20963 45881 849951) "http://intelligence.org/2013/07/08/responses-to-catastrophic-agi-risk-a-survey/?utm_source=rss&utm_medium=rss&utm_campaign=responses-to-catastrophic-agi-risk-a-survey" "Responses to Catastrophic AGI Risk: A Survey" "Luke Muehlhauser" "Mon, 08 Jul 2013 13:49:59 +0000" "<p>MIRI is self-publishing another technical report that was too lengthy (60 pages) for publication in a journal: <strong><a href=\"http://intelligence.org/files/ResponsesAGIRisk.pdf\" onclick=\"_gaq.push(['_trackEvent','research engagement','download pdf','/files/ResponsesAGIRisk.pdf',100]);\" >Responses to Catastrophic AGI Risk: A Survey</a></strong>.</p>
<p>The report, co-authored by past MIRI researcher <a href=\"http://kajsotala.fi/\">Kaj Sotala</a> and University of Louisville’s <a href=\"http://louisville.edu/speed/computer/people/faculty/yampolskiy\">Roman Yampolskiy</a>, is a summary of the extant literature (250+ references) on AGI risk, and can serve either as a guide for researchers or as an introduction for the uninitiated.</p>
<p>Here is the abstract:</p>
<blockquote><p>Many researchers have argued that humanity will create artificial general intelligence (AGI) within the next twenty to one hundred years. It has been suggested that AGI may pose a catastrophic risk to humanity. After summarizing the arguments for why AGI may pose such a risk, we survey the field’s proposed responses to AGI risk. We consider societal proposals, proposals for external constraints on AGI behaviors, and proposals for creating AGIs that are safe due to their internal design.</p></blockquote>
<p>The preferred discussion page for the paper is <a href=\"http://lesswrong.com/r/discussion/lw/hxi/responses_to_catastrophic_agi_risk_a_survey/\">here</a>.</p>
<p>The post <a href=\"http://intelligence.org/2013/07/08/responses-to-catastrophic-agi-risk-a-survey/\">Responses to Catastrophic AGI Risk: A Survey</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/07/08/responses-to-catastrophic-agi-risk-a-survey/#comments" "707ac1540826c9ffe7dc9f9b233888af") (16 (20963 45881 848944) "http://intelligence.org/2013/06/19/what-is-intelligence-2/?utm_source=rss&utm_medium=rss&utm_campaign=what-is-intelligence-2" "What is Intelligence?" "Luke Muehlhauser" "Wed, 19 Jun 2013 19:59:12 +0000" "<p><img class=\"alignright size-full wp-image-10276\" alt=\"brain of gears and circuits\" src=\"http://intelligence.org/wp-content/uploads/2013/06/brain-of-gears-and-circuits.jpg\" width=\"250\" height=\"327\" />When asked their opinions about “human-level artificial intelligence” — <em>aka</em> “artificial general intelligence” (AGI)<sup>1</sup> — many experts understandably reply that these terms haven’t yet been precisely defined, and it’s hard to talk about something that hasn’t been defined.<sup>2</sup> In this post, I want to briefly outline an imprecise but useful “working definition” for <em>intelligence</em> we tend to use at MIRI. In a future post I will write about some useful working definitions for <em>artificial general intelligence</em>.</p>
<p> </p>
<h3>Imprecise definitions can be useful</h3>
<p>Precise definitions are important, but I concur with Bertrand Russell that</p>
<blockquote><p>[You cannot] start with anything precise. You have to achieve such precision… as you go along.</p></blockquote>
<p>Physicist <a href=\"http://ieet.org/index.php/IEET/bio/cirkovic/\">Milan Ćirković</a> agrees, and <a href=\"http://www.amazon.com/Astrobiological-Landscape-Start-Publishing-ebook/dp/B008CDSB30/\">gives</a> an example:</p>
<blockquote><p>The formalization of knowledge — which includes giving precise definitions — usually comes at the end of the original research in a given field, not at the very beginning. A particularly illuminating example is the concept of <em>number</em>, which was properly defined in the modern sense only after the development of axiomatic set theory in the… twentieth century.<sup>3</sup></p></blockquote>
<p>For a more AI-relevant example, consider the concept of a “self-driving car,” which has been given a variety of vague definitions <a href=\"http://books.google.com/books?id=7OEDAAAAMBAJ&lpg=PA210&dq=automatic%20car&pg=PA210#v=onepage&q&f=false\">since the 1930s</a>. Would a car <a href=\"http://books.google.com/books?id=xiUDAAAAMBAJ&lpg=PA75&dq=car%20drives%20itself&pg=PA75#v=onepage&q&f=false\">guided by a buried cable</a> qualify? What about a <a href=\"http://books.google.com/books?id=jd8DAAAAMBAJ&lpg=PA128&dq=automatic%20car&pg=PA128#v=onepage&q&f=false\">modified 1955 Studebaker</a> that could use sound waves to detect obstacles and automatically engage the brakes if necessary, but could only steer “on its own” if each turn was preprogrammed? Does that count as a “self-driving car”?</p>
<p>What about the “<a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/05/Dickmanns-et-al-The-seeing-passenger-car-VaMoRs-P.pdf\">VaMoRs</a>” of the 1980s that could avoid obstacles and steer around turns using computer vision, but weren’t advanced enough to be ready for public roads? How about the 1995 <a href=\"http://en.wikipedia.org/wiki/Navlab\">Navlab</a> car that drove across the USA and was fully autonomous for 98.2% of the trip, or the robotic cars which finished the 132-mile off-road course of the <a href=\"http://is.gd/CDvT8V\">2005 DARPA Grand Challenge</a>, supplied only with the GPS coordinates of the route? What about the winning cars of the <a href=\"http://is.gd/kOjor6\">2007 DARPA Grand Challenge</a>, which finished an urban race while obeying all traffic laws and avoiding collisions with other cars? Does <a href=\"http://www.forbes.com/sites/joannmuller/2013/03/21/no-hands-no-feet-my-unnerving-ride-in-googles-driverless-car/\">Google’s driverless car</a> qualify, given that it has logged more than 500,000 autonomous miles without a single accident under computer control, but still struggles with difficult merges and snow-covered roads?<sup>4</sup></p>
<p>Our lack of a precise definition for “self-driving car” doesn’t seem to have hindered progress on self-driving cars very much.<sup>5</sup> And I’m glad we didn’t wait to seriously discuss self-driving cars until we had a precise definition for the term.</p>
<p>Similarly, I don’t think we should wait for a precise definition of AGI before discussing the topic seriously. On the other hand, the term is useless if it carries <em>no</em> information. So let’s work our way toward a stipulative, operational definition for AGI. We’ll start by developing an operational definition for <em>intelligence</em>.</p>
<p><span id=\"more-10275\"></span></p>
<h3>A definition for “intelligence”</h3>
<p><a href=\"http://arxiv.org/pdf/0706.3639.pdf\">Legg and Hutter (2007)</a> found that definitions of intelligence converge toward the idea that “Intelligence measures an agent’s ability to achieve goals in a wide range of environments.” Let’s call this the “optimization power” concept of intelligence, because it measures an agent’s power to optimize the world according to its preferences.</p>
<p>I think this is a productive approach to the issue, since it identifies intelligence with externally measurable <em>performance</em> rather than with the details of <em>how</em> that performance might be achieved (e.g. via consciousness, <a href=\"http://www.hutter1.net/publ/uaigentle.pdf\">brute force calculation</a>, “complexity,” or something else). Moreover, it’s usually <em>performance</em> we care about: we tend to care most about whether an AI will perform well enough to replace human workers, or whether it will perform well enough improve its own abilities without human assistance, not whether it has some particular internal feature.<sup>6</sup></p>
<p>Furthermore, the concept of optimization power allows us to compare the intelligence of different kinds of agents. As Albus (<a href=\"ftp://calhau.dca.fee.unicamp.br/pub/docs/ia005/Albus-outline.pdf\">1991</a>) said, “A useful definition of intelligence… should include both biological and machine embodiments, and these should span an intellectual range from that of an insect to that of an Einstein, from that of a thermostat to that of the most sophisticated computer system that could ever be built.”</p>
<p>I’d like to add one more consideration, though. What if two agents have roughly equal ability to optimize the world according to their preferences, but the second agent requires far more resources to do so? These agents have the same optimization power, but the first one seems to be optimizing more intelligently. So perhaps we could use “intelligence” to mean “optimization power divided by resources used” — what Yudkowsky called <a href=\"http://lesswrong.com/lw/vb/efficient_crossdomain_optimization/\">efficient cross-domain optimization</a>.<sup>7</sup></p>
<p>Other definitions<sup>8</sup> have their merits, too. But at MIRI we find the concept of “efficient cross-domain optimization” sufficiently useful that it serves as our (still imprecise!) working definition for intelligence.</p>
<p>In a future post, I’ll discuss some useful working definitions for <em>artificial general intelligence</em>.</p>
<p> </p>
<h4>Notes</h4>
<p><sup>1</sup> <small>I use the HLAI and AGI interchangeably, but lately I’ve been using AGI almost exclusively, because I’ve learned that many people in the AI community react negatively to any mention of “human-level” AI but have no objection to the concept of narrow vs. general intelligence. See also Ben Goertzel’s comments <a href=\"http://wp.goertzel.org/?p=173\">here</a>.</small></p>
<p><sup>2</sup> <small>Asked when he thought HLAI would be created, Pat Hayes (a past president of <a href=\"http://en.wikipedia.org/wiki/Association_for_the_Advancement_of_Artificial_Intelligence\">AAAI</a>) <a href=\"http://lesswrong.com/r/discussion/lw/999/qa_with_experts_on_risks_from_ai_1/\">replied</a>: “I do not consider this question to be answerable, as I do not accept this (common) notion of ‘human-level intelligence’ as meaningful.” Asked the same question, AI scientist <a href=\"http://www.cse.unsw.edu.au/~willu/\">William Uther</a> <a href=\"http://lesswrong.com/r/discussion/lw/9cm/qa_with_experts_on_risks_from_ai_3/\">replied</a>: “You ask a lot about ‘human level AGI’. I do not think this term is well defined,” while AI scientist <a href=\"http://homepages.inf.ed.ac.uk/bundy/\">Alan Bundy</a> <a href=\"http://lesswrong.com/r/discussion/lw/9cm/qa_with_experts_on_risks_from_ai_3/\">replied</a>: “I don’t think the concept of ‘human-level machine intelligence’ is well formed.”</small></p>
<p><sup>3</sup> <small><a href=\"http://www.amazon.com/Mathematicians-Delight-Dover-Science-Books/dp/0486462404/\">Sawyer (1943)</a> gives another example: “Mathematicians first used the sign √-1, without in the least knowing what it could mean, because it shortened work and led to correct results. People naturally tried to find out why this happened and what √-1 really meant. After two hundreds years they succeeded.” <a href=\"http://www.amazon.com/Intuition-Pumps-Other-Tools-Thinking/dp/0393082067/\">Dennett (2013)</a> makes a related comment: “<em>Define your terms, sir!</em> No, I won’t. That would be premature… My [approach] is an instance of <em>nibbling</em> on a tough problem instead of trying to eat (and digest) the whole thing from the outset… In <em>Elbow Room</em>, I compared my method to the sculptor’s method of roughing out the form in a block of marble, approaching the final surfaces cautiously, modestly, working by successive approximation.”</small></p>
<p><sup>4</sup> <small>With self-driving cars, researchers did use many precise external performance measures (e.g. accident rates, speed, portion of the time they could run unassisted, frequency of getting stuck) to evaluate progress, as well as internal performance metrics (speed of search, bounded loss guarantees, etc.). Researchers could see that these bits of progress were in the right direction, even if their relative contribution long-term was unclear. And so it is with AI in general. AI researchers use many precise external and internal performance measures to evaluate progress, but it is difficult to know the relative contribution of these bits of progress toward the final goal of AGI.</small></p>
<p><sup>5</sup> <small>Heck, we’ve had pornography for millennia and <em>still</em> haven’t been able to define it precisely. Encyclopedia entries for “pornography” <a href=\"http://books.google.com/books?id=xOJvVJv2YlAC&pg=PA636&lpg=PA636&source=bl&ots=N3PI1PbmK1&sig=SIhfXbej_Z_HRDAxxnGZWftTZhg&hl=en&sa=X&ei=9o-NUfTEAqafiALa34CoDQ&ved=0CC8Q6AEwADgK#v=onepage&q&f=false\">often</a> <a href=\"http://books.google.com/books?id=TN-qpt7kAK4C&pg=PA336&lpg=PA336&source=bl&ots=CspWxfT67z&sig=_ymQ5x3lvNnMF0DsaSZkJzsSaqM&hl=en&sa=X&ei=9o-NUfTEAqafiALa34CoDQ&ved=0CDMQ6AEwAjgK#v=onepage&q&f=false\">simply</a> quote Justice Potter Stewart: “I shall not today attempt further to define the kinds of material I understand to be [pornography]… but I know it when I see it.”</small></p>
<p><sup>6</sup> <small>We might care about whether machines are conscious in addition to being intelligent, but we already have a convenient term for that: <em>consciousness</em>. In particular, we might care about machine consciousness because the slow, plodding invention of AI may involve the creation and destruction of millions of partially-conscious near-AIs that are switched on, suffer for a while, and are then switched off — all while being unable to signal to us that they are suffering. This is especially likely if we remain unclear about the nature of consciousness for several more decades, and thus have no principled way (e.g. via <a href=\"http://lesswrong.com/lw/x4/nonperson_predicates/\">nonperson predicates</a>) to create intelligent machines that we <em>know</em> are not conscious (and are thus incapable of suffering). One of the first people to make this point clearly was <a href=\"http://www.amazon.com/Being-No-One-Self-Model-Subjectivity/dp/0262633086/\">Metzinger (2003)</a>, p. 621: “What would you say if someone came along and said, ‘Hey, we want to genetically engineer mentally retarded human infants! For reasons of scientific progress we need infants with certain cognitive and emotional deficits in order to study their postnatal psychological development—we urgently need some funding for this important and innovative kind of research!’ You would certainly think this was not only an absurd and appalling but also a dangerous idea. It would hopefully not pass any ethics committee in the democratic world. However, what today’s ethics committees don’t see is how the first machines satisfying a minimally sufficient set of constraints for conscious experience could be just like such mentally retarded infants. They would suffer from all kinds of functional and representational deficits too. But they would now also subjectively experience those deficits. In addition, they would have no political lobby—no representatives in any ethics committee.” Metzinger repeats the point in <a href=\"http://www.amazon.com/Ego-Tunnel-Science-Mind-Myth/dp/0465020690/\">Metzinger (2010)</a>, starting on page 194.</small></p>
<p><sup>7</sup> <small>Admittedly, this is still pretty vague. One step toward precision would be to propose a definition of intelligence as optimization power for some canonical distribution of possible preferences, over some canonical distribution of environments, with a penalty for resource use. The canonical preferences and canonical environments could be weighted toward preferences and environments relevant to our concerns: we care more about whether AIs can do science than whether they can paint abstract art, and we care more about whether they can achieve their goals in our solar system than whether they can achieve their goals inside a black hole. Also see <a href=\"http://agi-conf.org/2010/wp-content/uploads/2009/06/paper_14.pdf\">Goertzel (2010)</a>‘s “efficient pragmatic general intelligence.”</small></p>
<p><sup>8</sup> <small><a href=\"http://www.ssec.wisc.edu/~billh/g/hibbard_agi11a.pdf\">Hibbard (2011)</a>; <a href=\"http://arxiv.org/pdf/1109.5951.pdf\">Legg & Veness (2011)</a>; <a href=\"http://www.cis.temple.edu/~wangp/Publication/AI_Definitions.pdf\">Wang (2008)</a>; <a href=\"http://arxiv.org/pdf/1109.1314.pdf\">Schaul et al. (2011)</a>; <a href=\"http://www.csse.monash.edu.au/~dld/Publications/2012/Dowe%2BHernandez-Orallo_2012_IQ_tests_are_not_for_machines_comma_yet_IN_PRESS.pdf\">Dowe & Hernandez-Orallo (2012)</a>; <a href=\"http://agi-conf.org/2010/wp-content/uploads/2009/06/paper_14.pdf\">Goertzel (2010)</a>; <a href=\"http://www.cse.buffalo.edu/faculty/shapiro/Papers/hlai.pdf\">Adams et al. (2011)</a>.</small></p>
<p>The post <a href=\"http://intelligence.org/2013/06/19/what-is-intelligence-2/\">What is Intelligence?</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/06/19/what-is-intelligence-2/#comments" "15990b66a3c7798aefd23aa361fa1cf8") (15 (20963 45881 730919) "http://intelligence.org/2013/06/07/miris-july-2013-workshop/?utm_source=rss&utm_medium=rss&utm_campaign=miris-july-2013-workshop" "=?utf-8?Q?MIRI=E2=80=99s?= July 2013 Workshop" "Luke Muehlhauser" "Fri, 07 Jun 2013 22:15:07 +0000" "<a href=\"http://intelligence.org/get-involved/#workshop\"><img class=\"aligncenter size-full wp-image-10250\" alt=\"Mihaly at April workshop\" src=\"http://intelligence.org/wp-content/uploads/2013/06/Mihaly-at-April-workshop.jpg\" width=\"500\" height=\"308\" /></a>
<p>From July 8-14, MIRI will host its <strong>3rd Workshop on Logic, Probability, and Reflection</strong>. The focus of this workshop will be the <a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/\">Löbian obstacle to self-modifying systems</a>.</p>
<p>Participants confirmed so far include:</p>
<ul>
<li><a href=\"http://acritch.com/\">Andrew Critch</a> (just finished his math PhD at UC Berkeley, now working at <a href=\"http://rationality.org/\">CFAR</a>)</li>
<li><a href=\"https://plus.google.com/111568410659864255951\">Abram Demski</a> (USC)</li>
<li><a href=\"http://lesswrong.com/user/Benja/submitted/\">Benja Fallenstein</a> (Bristol U)</li>
<li><a href=\"http://www.linkedin.com/pub/marcello-herreshoff/0/8b4/51a\">Marcello Herreshoff </a>(Google)</li>
<li>Jonathan Lee (Cambridge)</li>
<li><a href=\"http://www.ctpost.com/news/article/Wisdom-beyond-his-years-1390299.php\">Will Sawin</a> (Princeton)</li>
<li><a href=\"http://math.berkeley.edu/~qchu/\">Qioachu Yuan</a> (UC Berkeley)</li>
<li><a href=\"http://yudkowsky.net/\">Eliezer Yudkowsky</a> (MIRI)</li>
</ul>
<p>If you have a strong mathematics background and might like to attend this workshop, it’s not too late to <a href=\"http://intelligence.org/get-involved/#workshop\">apply</a>! And even if <em>this</em> workshop doesn’t fit your schedule, please <strong>do apply</strong>, so that we can notify you of other workshops (long before they are announced publicly).</p>
<p>Information on past workshops:</p>
<ul>
<li><span style=\"line-height: 13px;\">Our<strong> 1st Workshop</strong> (Nov. 11-18, 2012; 4 participants) resulted in Christiano’s <a href=\"http://lesswrong.com/lw/h1k/reflection_in_probabilistic_logic/\">probabilistic logic</a>, an attack on the <a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/\">Löbian obstacle for self-modifying systems</a>.<br />
</span></li>
<li>Our <strong>2nd Workshop</strong> (Apr. 3-24, 2013; 12 participants coming in and out) resulted in (1) some as-yet unpublished progress on Christiano’s probabilistic logic, (2) some progress on program equilibrium recorded in <a href=\"http://intelligence.org/files/RobustCooperation.pdf\">LaVictoire et al. (2013)</a>, and some progress on the Löbian obstacle resulting in <a href=\"http://intelligence.org/files/TilingAgents.pdf\">Yudkowsky & Herreschoff (2013)</a>.</li>
</ul>
<p>The post <a href=\"http://intelligence.org/2013/06/07/miris-july-2013-workshop/\">MIRI’s July 2013 Workshop</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/06/07/miris-july-2013-workshop/#comments" "ca8021d2252f41192ba88d0c7a007eb9") (14 (20959 51802 394341) "http://intelligence.org/2013/07/08/miri-has-moved/?utm_source=rss&utm_medium=rss&utm_campaign=miri-has-moved" "MIRI Has Moved!" "Luke Muehlhauser" "Mon, 08 Jul 2013 13:50:53 +0000" "<p>For the past several months, MIRI and its child organization <a href=\"http://rationality.org/\">CFAR</a> have been working from a much-too-small office on the outskirts of Berkeley. At the end of June, MIRI and CFAR took over the 3rd floor of <a href=\"https://www.google.com/maps/preview#!q=2030+Addison+St%2C+Berkeley\">2030 Addison St.</a> in downtown Berkeley, which has sufficient space for both organizations.</p>
<p>Our new office is 0.5 blocks from the Downtown Berkeley BART exit at Shattuck & Addison, and 2 blocks from the UC Berkeley campus. Here’s a photo of the campus from our roof:</p>
<img class=\"aligncenter size-full wp-image-10311\" alt=\"view of campus from roof (500px)\" src=\"http://intelligence.org/wp-content/uploads/2013/07/view-of-campus-from-roof-500px.jpg\" />
<p>The proximity to UC Berkeley will make it easier for MIRI to network with Berkeley’s professors and students. Conveniently, UC Berkeley is ranked <a href=\"http://www.usnews.com/education/worlds-best-universities-rankings/best-universities-mathematics\">5th in the world</a> in mathematics, and <a href=\"http://grad-schools.usnews.rankingsandreviews.com/best-graduate-schools/top-science-schools/logic-rankings\">1st in the world</a> in mathematical logic.</p>
<p>Sharing an office with CFAR carries many benefits for both organizations:</p>
<ol>
<li><span style=\"line-height: 13px;\">CFAR and MIRI can “flex” into each other’s space for short periods as needed, for example when MIRI is holding a week-long <a href=\"/get-involved/#workshop\">research workshop</a>.</span></li>
<li>We can share resources (printers, etc.).</li>
<li>Both organizations can benefit from interaction between our two communities.</li>
</ol>
<p>Getting the new office was a team effort, but the person <em>most</em> responsible for this success was MIRI Deputy Director Louie Helm.</p>
<p>Note that MIRI isn’t yet able to accommodate “drop in” visitors, as we keep irregular hours throughout the week. So if you’d like to visit, please <a href=\"/team/\">contact us</a> first.</p>
<p>We retain 2721 Shattuck Ave. #1023 as an alternate mailing address.</p>
<p>The post <a href=\"http://intelligence.org/2013/07/08/miri-has-moved/\">MIRI Has Moved!</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/07/08/miri-has-moved/#comments" "2f30c57520100b502c9ee9b003d5d36b") (13 (20957 6032 640545) "http://intelligence.org/2013/07/08/2013-summer-matching-challenge/?utm_source=rss&utm_medium=rss&utm_campaign=2013-summer-matching-challenge" "2013 Summer Matching Challenge!" "Luke Muehlhauser" "Mon, 08 Jul 2013 14:00:40 +0000" "<p>Thanks to the generosity of several major donors,<sup>†</sup> every donation to the Machine Intelligence Research Institute made from now until August 15th, 2013 will be matched dollar-for-dollar, up to a total of $200,000!</p>
<p> </p>
<p style=\"font-size: 300%;\" align=\"center\"><strong><a href=\"/donate/#donation-methods\">Donate Now!</a></strong></p>
<p>        <div class=\"progress progress-warning progress-striped remove-bottom\">
</div>
<div class=\"row-fluid\">
<div class=\"span2\">
<p class=\"\">$0</p>
</div>
<div class=\"span2\">
<p class=\"text-center\">$50,000</p>
</div>
<div class=\"span4\">
<p class=\"text-center\">$100,000</p>
</div>
<div class=\"span2\">
<p class=\"text-center\">$150,000</p>
</div>
<div class=\"span2\">
<p class=\"text-right\">$200,000</p>
</div>
</div>
<h3 class=\"text-center remove-bottom remove-top donation-matched\"><i class=\"icon-spinner icon-spin icon-large\"></i></h3>
<script type=\"text/javascript\">
jQuery(document).ready(function($) {
$.get('/wp-content/themes/wordpress-bootstrap-miri/donations-fundraiser.php', function(data) {
$('div.progress').html('<div class=\"bar\" style=\"width: ' + data/200000*100 + '%\"></div>');
$('.donation-matched').html('$' + data*2 + ' raised with matching funds!');
});
});
</script></p>
<p>Now is your chance to <strong>double your impact</strong> while helping us raise up to $400,000 (with matching) to fund <a href=\"/research/\">our research program</a>.</p>
<hr />
<p>Early this year we made a transition from movement-building to research, and we’ve <em>hit the ground running</em> with six major new research papers, six new strategic analyses on our blog, and much more. Give now to support our ongoing work on <a href=\"http://intelligence.org/2013/06/05/friendly-ai-research-as-effective-altruism/\">the future’s most important problem</a>.</p>
<p> </p>
<h3><span>Accomplishments in 2013 so far</span></h3>
<ul>
<li><a href=\"2013/01/30/we-are-now-the-machine-intelligence-research-institute-miri/\">Changed our name</a> to MIRI and launched our new website at intelligence.org.</li>
<li>Released <strong>six new research papers</strong>: <a href=\"http://lesswrong.com/lw/h1k/reflection_in_probabilistic_set_theory/\">Definability of Truth in Probabilistic Logic</a>, <a href=\"/files/IEM.pdf\">Intelligence Explosion Microeconomics</a>, <a href=\"/files/TilingAgents.pdf\">Tiling Agents for Self-Modifying AI</a>, <a href=\"/files/RobustCooperation.pdf\">Robust Cooperation in the Prisoner’s Dilemma</a>, <a href=\"/files/Comparison.pdf\">A Comparison of Decision Algorithms on Newcomblike Problems</a>, and <a href=\"/2013/07/07/responses-to-c…-risk-a-survey/ ‎\">Responses to Catastrophic AGI Risk: A Survey</a>.</li>
<li>Held our <a href=\"/2013/03/07/upcoming-miri-research-workshops/\">2nd research workshop</a>. (Our <a href=\"/2013/06/07/miris-july-2013-workshop/\">3rd workshop</a> is currently ongoing.)</li>
<li>Published <strong>six new analyses</strong> to our blog: <a href=\"/2013/04/04/the-lean-nonprofit/\">The Lean Nonprofit</a>, <a href=\"/2013/05/01/agi-impacts-experts-and-friendly-ai-experts/\">AGI Impact Experts and Friendly AI Experts</a>, <a href=\"/2013/05/05/five-theses-two-lemmas-and-a-couple-of-strategic-implications/\">Five Theses…</a>, <a>When Will AI Be Created?</a>, <a href=\"/2013/06/05/friendly-ai-research-as-effective-altruism/\">Friendly AI Research as Effective Altruism</a>, and <a href=\"/2013/06/19/what-is-intelligence-2/\">What is Intelligence?</a></li>
<li>Published the <em><a href=\"http://intelligenceexplosion.com/ebook/\">Facing the Intelligence Explosion</a> </em>ebook.</li>
<li>Published several other substantial articles: <a href=\"/courses/\">Recommended Courses for MIRI Researchers</a>, <a href=\"http://lesswrong.com/lw/gu1/decision_theory_faq/\">Decision Theory FAQ</a>, <a href=\"http://lesswrong.com/lw/gln/a_brief_history_of_ethically_concerned_scientists/\">A brief history of ethically concerned scientists</a>, <a href=\"http://lesswrong.com/lw/gzq/bayesian_adjustment_does_not_defeat_existential/\">Bayesian Adjustment Does Not Defeat Existential Risk Charity</a>, and others.</li>
<li>And of course <em>much</em> more.</li>
</ul>
<h3><span>Future Plans You Can Help Support</span></h3>
<ul>
<li>We will host many more research workshops, including <a href=\"/2013/07/07/miris-september-2013-workshop/ ‎\">one in September</a>, and one in December (with <a href=\"http://math.ucr.edu/home/baez/\">John Baez</a> attending, among others).</li>
<li>Eliezer will continue to publish about open problems in Friendly AI. (Here is <a href=\"http://lesswrong.com/lw/hbd/new_report_intelligence_explosion_microeconomics/\">#1</a> and <a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/\">#2</a>.)</li>
<li>We will continue to publish strategic analyses, mostly via our blog.</li>
<li>We will publish nicely-edited ebooks (Kindle, iBooks, and PDF) for more of our materials, to make them more accessible: <em><a href=\"http://wiki.lesswrong.com/wiki/Sequences\">The Sequences, 2006-2009</a> </em>and <em><a href=\"http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate\">The Hanson-Yudkowsky AI Foom Debate</a></em>.</li>
<li>We will continue to set up the infrastructure (e.g. <a href=\"/2013/07/06/miri-has-moved/\">new offices</a>, researcher endowments) required to host a productive Friendly AI research team, and (over several years) recruit enough top-level math talent to launch it.</li>
</ul>
<p>(Other projects are still being surveyed for likely cost and strategic impact.)</p>
<p>We appreciate your support for our high-impact work! Donate now, and seize a better than usual chance to move our work forward. If you have questions about donating, please contact Louie Helm at (510) 717-1477 or louie@intelligence.org.</p>
<p><sup>†</sup> $200,000 of total matching funds has been provided by Jaan Tallinn, Loren Merritt, Rick Schwall, and Alexei Andreev.</p>
<p>The post <a href=\"http://intelligence.org/2013/07/08/2013-summer-matching-challenge/\">2013 Summer Matching Challenge!</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/07/08/2013-summer-matching-challenge/#comments" "67fd134cf247c2fd6527eeb8c702fe8c") (12 (20955 50119 262373) "http://intelligence.org/2013/07/08/2013-summer-matching-challenge/?utm_source=rss&utm_medium=rss&utm_campaign=2013-summer-matching-challenge" "2013 Summer Matching Challenge!" "Luke Muehlhauser" "Mon, 08 Jul 2013 14:00:40 +0000" "<p>Thanks to the generosity of several major donors,<sup>†</sup> every donation to the Machine Intelligence Research Institute made from now until August 15th, 2013 will be matched dollar-for-dollar, up to a total of $200,000!</p>
<p> </p>
<p style=\"font-size: 300%;\" align=\"center\"><strong><a href=\"/donate/#donation-methods\">Donate Now!</a></strong></p>
<p>        <div class=\"progress progress-warning progress-striped remove-bottom\">
</div>
<div class=\"row-fluid\">
<div class=\"span2\">
<p class=\"\">$0</p>
</div>
<div class=\"span2\">
<p class=\"text-center\">$50,000</p>
</div>
<div class=\"span4\">
<p class=\"text-center\">$100,000</p>
</div>
<div class=\"span2\">
<p class=\"text-center\">$150,000</p>
</div>
<div class=\"span2\">
<p class=\"text-right\">$200,000</p>
</div>
</div>
<h3 class=\"text-center remove-bottom remove-top donation-matched\"><i class=\"icon-spinner icon-spin icon-large\"></i></h3>
<script type=\"text/javascript\">
jQuery(document).ready(function($) {
$.get('/wp-content/themes/wordpress-bootstrap-miri/donations-fundraiser.php', function(data) {
$('div.progress').html('<div class=\"bar\" style=\"width: ' + data/200000*100 + '%\"></div>');
$('.donation-matched').html('$' + data*2 + ' so far with matching funds!');
});
});
</script></p>
<p>Now is your chance to <strong>double your impact</strong> while helping us raise up to $400,000 (with matching) to fund <a href=\"/research/\">our research program</a>.</p>
<hr />
<p> </p>
<h3><span>Accomplishments in 2013 so far</span></h3>
<ul>
<li><a href=\"2013/01/30/we-are-now-the-machine-intelligence-research-institute-miri/\">Changed our name</a> to MIRI and launched our new website at intelligence.org.</li>
<li>Released <strong>six new research papers</strong>: <a href=\"http://lesswrong.com/lw/h1k/reflection_in_probabilistic_set_theory/\">Definability of Truth in Probabilistic Logic</a>, <a href=\"/files/IEM.pdf\">Intelligence Explosion Microeconomics</a>, <a href=\"/files/TilingAgents.pdf\">Tiling Agents for Self-Modifying AI</a>, <a href=\"/files/RobustCooperation.pdf\">Robust Cooperation in the Prisoner’s Dilemma</a>, <a href=\"/files/Comparison.pdf\">A Comparison of Decision Algorithms on Newcomblike Problems</a>, and <a href=\"/2013/07/07/responses-to-c…-risk-a-survey/ ‎\">Responses to Catastrophic AGI Risk: A Survey</a>.</li>
<li>Held our <a href=\"/2013/03/07/upcoming-miri-research-workshops/\">2nd research workshop</a>. (Our <a href=\"/2013/06/07/miris-july-2013-workshop/\">3rd workshop</a> is currently ongoing.)</li>
<li>Published <strong>six new analyses</strong> to our blog: <a href=\"/2013/04/04/the-lean-nonprofit/\">The Lean Nonprofit</a>, <a href=\"/2013/05/01/agi-impacts-experts-and-friendly-ai-experts/\">AGI Impact Experts and Friendly AI Experts</a>, <a href=\"/2013/05/05/five-theses-two-lemmas-and-a-couple-of-strategic-implications/\">Five Theses…</a>, <a>When Will AI Be Created?</a>, <a href=\"/2013/06/05/friendly-ai-research-as-effective-altruism/\">Friendly AI Research as Effective Altruism</a>, and <a href=\"/2013/06/19/what-is-intelligence-2/\">What is Intelligence?</a></li>
<li>Published the<em><a href=\"http://intelligenceexplosion.com/ebook/\">Facing the Intelligence Explosion</a></em>ebook.</li>
<li>Published several other substantial articles: <a href=\"/courses/\">Recommended Courses for MIRI Researchers</a>, <a href=\"http://lesswrong.com/lw/gu1/decision_theory_faq/\">Decision Theory FAQ</a>, <a href=\"http://lesswrong.com/lw/gln/a_brief_history_of_ethically_concerned_scientists/\">A brief history of ethically concerned scientists</a>, <a href=\"http://lesswrong.com/lw/gzq/bayesian_adjustment_does_not_defeat_existential/\">Bayesian Adjustment Does Not Defeat Existential Risk Charity</a>, and others.</li>
<li>And of course <em>much</em> more.</li>
</ul>
<h3><span>Future Plans You Can Help Support</span></h3>
<ul>
<li>We will host many more research workshops, including <a href=\"/2013/07/07/miris-september-2013-workshop/ ‎\">one in September</a>, and one in December (with <a href=\"http://math.ucr.edu/home/baez/\">John Baez</a> attending, among others).</li>
<li>Eliezer will continue to publish about open problems in Friendly AI. (Here is <a href=\"http://lesswrong.com/lw/hbd/new_report_intelligence_explosion_microeconomics/\">#1</a> and <a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/\">#2</a>.)</li>
<li>We will continue to publish strategic analyses, mostly via our blog.</li>
<li>We will publish nicely-edited ebooks (Kindle, iBooks, and PDF) for more of our materials, to make them more accessible: <em><a href=\"http://wiki.lesswrong.com/wiki/Sequences\">The Sequences, 2006-2009</a> </em>and <em><a href=\"http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate\">The Hanson-Yudkowsky AI Foom Debate</a></em>.</li>
<li>We will continue to set up the infrastructure (e.g. <a href=\"/2013/07/06/miri-has-moved/\">new offices</a>, researcher endowments) required to host a productive Friendly AI research team, and (over several years) recruit enough top-level math talent to launch it.</li>
</ul>
<p>(Other projects are still being surveyed for likely cost and strategic impact.)</p>
<p>We appreciate your support for our high-impact work! Donate now, and seize a better than usual chance to move our work forward. If you have questions about donating, please contact Louie Helm at (510) 717-1477 or louie@intelligence.org.</p>
<p><sup>†</sup> $200,000 of total matching funds has been provided by Jaan Tallinn, Loren Merritt, Rick Schwall, and Alexei Andreev.</p>
<p>The post <a href=\"http://intelligence.org/2013/07/08/2013-summer-matching-challenge/\">2013 Summer Matching Challenge!</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/07/08/2013-summer-matching-challenge/#comments" "c61fee5223e7676ea1f541fb1d334d6d") (11 (20955 50119 261207) "http://intelligence.org/2013/07/08/miri-has-moved/?utm_source=rss&utm_medium=rss&utm_campaign=miri-has-moved" "MIRI Has Moved!" "Luke Muehlhauser" "Mon, 08 Jul 2013 13:50:53 +0000" "<p>For the past several months, MIRI and its child organization <a href=\"http://rationality.org/\">CFAR</a> have been working from a much-too-small office on the outskirts of Berkeley. At the end of June, MIRI and CFAR took over the 3rd floor of <a href=\"https://www.google.com/maps/preview#!q=2030+Addison+St%2C+Berkeley\">2030 Addison St.</a> in downtown Berkeley, which has sufficient space for both organizations.</p>
<p>Our new office is 0.5 blocks from the Downtown Berkeley BART exit at Shattuck & Addison, and 2 blocks from the UC Berkeley campus. Here’s a photo of the campus from our roof:</p>
<img class=\"aligncenter size-full wp-image-10311\" alt=\"view of campus from roof (500px)\" src=\"http://intelligence.org/wp-content/uploads/2013/07/view-of-campus-from-roof-500px.jpg\" />
<p>The proximity to UC Berkeley will make it easier for MIRI to network with Berkeley’s professors and students. Conveniently, UC Berkeley is ranked <a href=\"http://www.usnews.com/education/worlds-best-universities-rankings/best-universities-mathematics\">5th in the world</a> in mathematics, and <a href=\"http://grad-schools.usnews.rankingsandreviews.com/best-graduate-schools/top-science-schools/logic-rankings\">1st in the world</a> in mathematical logic.</p>
<p>Sharing an office with CFAR carries many benefits for both organizations:</p>
<ol>
<li><span style=\"line-height: 13px;\">CFAR and MIRI can “flex” into each other’s space for short periods as needed, for example when MIRI is holding a week-long <a href=\"/get-involved/#workshop\">research workshop</a>.</span></li>
<li>We can share resources (printers, etc.).</li>
<li>Both organizations can benefit from interaction between our two communities.</li>
</ol>
<p>Note that MIRI isn’t yet able to accommodate “drop in” visitors, as we keep irregular hours throughout the week. So if you’d like to visit, please <a href=\"/team/\">contact us</a> first.</p>
<p>We retain 2721 Shattuck Ave. #1023 as an alternate mailing address.</p>
<p>The post <a href=\"http://intelligence.org/2013/07/08/miri-has-moved/\">MIRI Has Moved!</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/07/08/miri-has-moved/#comments" "87b45a04c6aafd27678fbd52f7e30b2c") (10 (20955 50119 259021) "http://intelligence.org/2013/07/08/miris-september-2013-workshop/?utm_source=rss&utm_medium=rss&utm_campaign=miris-september-2013-workshop" "=?utf-8?Q?MIRI=E2=80=99s?= September 2013 Workshop" "Luke Muehlhauser" "Mon, 08 Jul 2013 13:50:22 +0000" "<img class=\"aligncenter size-full wp-image-10316\" alt=\"Paul at April workshop\" src=\"http://intelligence.org/wp-content/uploads/2013/07/Paul-at-April-workshop.jpg\" />
<p>From September 7-14, MIRI will host its <strong>4th Workshop on Logic, Probability, and Reflection</strong>. The focus of this workshop will be the foundations of <a href=\"http://lesswrong.com/lw/gu1/decision_theory_faq/\">decision theory</a>.</p>
<p>Participants confirmed so far include:</p>
<ul>
<li><a href=\"http://rationalaltruist.com/\">Paul Christiano</a> (UC Berkeley)</li>
<li><a href=\"http://en.wikipedia.org/wiki/Gary_Drescher\">Gary Drescher</a> (independent)</li>
<li><a href=\"http://www.kennyeaswaran.org/\">Kenny Easwaran</a> (USC)</li>
<li><a href=\"http://www.southampton.ac.uk/maths/about/staff/is1d12.page\">Ilya Shpitser</a> (U Southampton)</li>
<li><a href=\"http://lesswrong.com/user/cousin_it/submitted/\">Vladimir Slepnev</a> (Google)</li>
<li><a href=\"http://lesswrong.com/user/Nisan/submitted/\">Nisan Stiennon</a> (Stanford)</li>
<li><a href=\"http://stuhlmueller.org/\">Andreas Stuhlmüller</a> (MIT & Stanford)</li>
<li><a href=\"http://yudkowsky.net/\">Eliezer Yudkowsky</a> (MIRI)</li>
</ul>
<p>If you have a strong mathematics background and might like to attend this workshop, it’s not too late to <a href=\"http://intelligence.org/get-involved/#workshop\">apply</a>! And even if <em>this</em> workshop doesn’t fit your schedule, please <strong>do apply</strong>, so that we can notify you of other workshops (long before they are announced publicly).</p>
<p>The post <a href=\"http://intelligence.org/2013/07/08/miris-september-2013-workshop/\">MIRI’s September 2013 Workshop</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/07/08/miris-september-2013-workshop/#comments" "7cf5d71443ad9f9e608cc107388c576a") (9 (20955 50119 258486) "http://intelligence.org/2013/07/08/responses-to-catastrophic-agi-risk-a-survey/?utm_source=rss&utm_medium=rss&utm_campaign=responses-to-catastrophic-agi-risk-a-survey" "Responses to Catastrophic AGI Risk: A Survey" "Luke Muehlhauser" "Mon, 08 Jul 2013 13:49:59 +0000" "<p>MIRI is self-publishing another technical report that was too lengthy (60 pages) for publication in a journal: <strong><a href=\"http://intelligence.org/files/ResponsesAGIRisk.pdf\" onclick=\"_gaq.push(['_trackEvent','research engagement','download pdf','/files/ResponsesAGIRisk.pdf',100]);\" >Responses to Catastrophic AGI Risk: A Survey</a></strong>.</p>
<p>The report, co-authored by past MIRI researcher <a href=\"http://kajsotala.fi/\">Kaj Sotala</a> and University of Louisville’s <a href=\"http://louisville.edu/speed/computer/people/faculty/yampolskiy\">Roman Yampolskiy</a>, is a summary of the extant literature (250+ references) on AGI risk, and can serve either as a guide for researchers or as an introduction for the uninitiated.</p>
<p>Here is the abstract:</p>
<blockquote><p>Many researchers have argued that humanity will create artificial general intelligence (AGI) within the next twenty to one hundred years. It has been suggested that AGI may pose a catastrophic risk to humanity. After summarizing the arguments for why AGI may pose such a risk, we survey the field’s proposed responses to AGI risk. We consider societal proposals, proposals for external constraints on AGI behaviors, and proposals for creating AGIs that are safe due to their internal design.</p></blockquote>
<p>The preferred discussion page for the paper is <a href=\"http://lesswrong.com/r/discussion/lw/hxi/responses_to_catastrophic_agi_risk_a_survey/\">here</a>.</p>
<p>The post <a href=\"http://intelligence.org/2013/07/08/responses-to-catastrophic-agi-risk-a-survey/\">Responses to Catastrophic AGI Risk: A Survey</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/07/08/responses-to-catastrophic-agi-risk-a-survey/#comments" "57ca2c85665e10a11d95a82b401ca251") (8 (20955 50119 257392) "http://intelligence.org/2013/06/19/what-is-intelligence-2/?utm_source=rss&utm_medium=rss&utm_campaign=what-is-intelligence-2" "What is Intelligence?" "Luke Muehlhauser" "Wed, 19 Jun 2013 19:59:12 +0000" "<p><img class=\"alignright size-full wp-image-10276\" alt=\"brain of gears and circuits\" src=\"http://intelligence.org/wp-content/uploads/2013/06/brain-of-gears-and-circuits.jpg\" width=\"250\" height=\"327\" />When asked their opinions about “human-level artificial intelligence” — <em>aka</em> “artificial general intelligence” (AGI)<sup>1</sup> — many experts understandably reply that these terms haven’t yet been precisely defined, and it’s hard to talk about something that hasn’t been defined.<sup>2</sup> In this post, I want to briefly outline an imprecise but useful “working definition” for <em>intelligence</em> we tend to use at MIRI. In a future post I will write about some useful working definitions for <em>artificial general intelligence</em>.</p>
<p> </p>
<h3>Imprecise definitions can be useful</h3>
<p>Precise definitions are important, but I concur with Bertrand Russell that</p>
<blockquote><p>[You cannot] start with anything precise. You have to achieve such precision… as you go along.</p></blockquote>
<p>Physicist <a href=\"http://ieet.org/index.php/IEET/bio/cirkovic/\">Milan Ćirković</a> agrees, and <a href=\"http://www.amazon.com/Astrobiological-Landscape-Start-Publishing-ebook/dp/B008CDSB30/\">gives</a> an example:</p>
<blockquote><p>The formalization of knowledge — which includes giving precise definitions — usually comes at the end of the original research in a given field, not at the very beginning. A particularly illuminating example is the concept of <em>number</em>, which was properly defined in the modern sense only after the development of axiomatic set theory in the… twentieth century.<sup>3</sup></p></blockquote>
<p>For a more AI-relevant example, consider the concept of a “self-driving car,” which has been given a variety of vague definitions <a href=\"http://books.google.com/books?id=7OEDAAAAMBAJ&lpg=PA210&dq=automatic%20car&pg=PA210#v=onepage&q&f=false\">since the 1930s</a>. Would a car <a href=\"http://books.google.com/books?id=xiUDAAAAMBAJ&lpg=PA75&dq=car%20drives%20itself&pg=PA75#v=onepage&q&f=false\">guided by a buried cable</a> qualify? What about a <a href=\"http://books.google.com/books?id=jd8DAAAAMBAJ&lpg=PA128&dq=automatic%20car&pg=PA128#v=onepage&q&f=false\">modified 1955 Studebaker</a> that could use sound waves to detect obstacles and automatically engage the brakes if necessary, but could only steer “on its own” if each turn was preprogrammed? Does that count as a “self-driving car”?</p>
<p>What about the “<a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/05/Dickmanns-et-al-The-seeing-passenger-car-VaMoRs-P.pdf\">VaMoRs</a>” of the 1980s that could avoid obstacles and steer around turns using computer vision, but weren’t advanced enough to be ready for public roads? How about the 1995 <a href=\"http://en.wikipedia.org/wiki/Navlab\">Navlab</a> car that drove across the USA and was fully autonomous for 98.2% of the trip, or the robotic cars which finished the 132-mile off-road course of the <a href=\"http://is.gd/CDvT8V\">2005 DARPA Grand Challenge</a>, supplied only with the GPS coordinates of the route? What about the winning cars of the <a href=\"http://is.gd/kOjor6\">2007 DARPA Grand Challenge</a>, which finished an urban race while obeying all traffic laws and avoiding collisions with other cars? Does <a href=\"http://www.forbes.com/sites/joannmuller/2013/03/21/no-hands-no-feet-my-unnerving-ride-in-googles-driverless-car/\">Google’s driverless car</a> qualify, given that it has logged more than 500,000 autonomous miles without a single accident under computer control, but still struggles with difficult merges and snow-covered roads?<sup>4</sup></p>
<p>Our lack of a precise definition for “self-driving car” doesn’t seem to have hindered progress on self-driving cars very much.<sup>5</sup> And I’m glad we didn’t wait to seriously discuss self-driving cars until we had a precise definition for the term.</p>
<p>Similarly, I don’t think we should wait for a precise definition of AGI before discussing the topic seriously. On the other hand, the term is useless if it carries <em>no</em> information. So let’s work our way toward a stipulative, operational definition for AGI. We’ll start by developing an operational definition for <em>intelligence</em>.</p>
<p><span id=\"more-10275\"></span></p>
<h3>A definition for “intelligence”</h3>
<p><a href=\"http://arxiv.org/pdf/0706.3639.pdf\">Legg and Hutter (2007)</a> found that definitions of intelligence converge toward the idea that “Intelligence measures an agent’s ability to achieve goals in a wide range of environments.” Let’s call this the “optimization power” concept of intelligence, because it measures an agent’s power to optimize the world according to its preferences.</p>
<p>I think this is a productive approach to the issue, since it identifies intelligence with externally measurable <em>performance</em> rather than with the details of <em>how</em> that performance might be achieved (e.g. via consciousness, <a href=\"http://www.hutter1.net/publ/uaigentle.pdf\">brute force calculation</a>, “complexity,” or something else). Moreover, it’s usually <em>performance</em> we care about: we tend to care most about whether an AI will perform well enough to replace human workers, or whether it will perform well enough improve its own abilities without human assistance, not whether it has some particular internal feature.<sup>6</sup></p>
<p>Furthermore, the concept of optimization power allows us to compare the intelligence of different kinds of agents. As Albus (<a href=\"ftp://calhau.dca.fee.unicamp.br/pub/docs/ia005/Albus-outline.pdf\">1991</a>) said, “A useful definition of intelligence… should include both biological and machine embodiments, and these should span an intellectual range from that of an insect to that of an Einstein, from that of a thermostat to that of the most sophisticated computer system that could ever be built.”</p>
<p>I’d like to add one more consideration, though. What if two agents have roughly equal ability to optimize the world according to their preferences, but the second agent requires far more resources to do so? These agents have the same optimization power, but the first one seems to be optimizing more intelligently. So perhaps we could use “intelligence” to mean “optimization power divided by resources used” — what Yudkowsky called <a href=\"http://lesswrong.com/lw/vb/efficient_crossdomain_optimization/\">efficient cross-domain optimization</a>.<sup>7</sup></p>
<p>Other definitions<sup>8</sup> have their merits, too. But at MIRI we find the concept of “efficient cross-domain optimization” sufficiently useful that it serves as our (still imprecise!) working definition for intelligence.</p>
<p>In a future post, I’ll discuss some useful working definitions for <em>artificial general intelligence</em>.</p>
<p> </p>
<h4>Notes</h4>
<p><sup>1</sup> <small>I use the HLAI and AGI interchangeably, but lately I’ve been using AGI almost exclusively, because I’ve learned that many people in the AI community react negatively to any mention of “human-level” AI but have no objection to the concept of narrow vs. general intelligence. See also Ben Goertzel’s comments <a href=\"http://wp.goertzel.org/?p=173\">here</a>.</small></p>
<p><sup>2</sup> <small>Asked when he thought HLAI would be created, Pat Hayes (a past president of <a href=\"http://en.wikipedia.org/wiki/Association_for_the_Advancement_of_Artificial_Intelligence\">AAAI</a>) <a href=\"http://lesswrong.com/r/discussion/lw/999/qa_with_experts_on_risks_from_ai_1/\">replied</a>: “I do not consider this question to be answerable, as I do not accept this (common) notion of ‘human-level intelligence’ as meaningful.” Asked the same question, AI scientist <a href=\"http://www.cse.unsw.edu.au/~willu/\">William Uther</a> <a href=\"http://lesswrong.com/r/discussion/lw/9cm/qa_with_experts_on_risks_from_ai_3/\">replied</a>: “You ask a lot about ‘human level AGI’. I do not think this term is well defined,” while AI scientist <a href=\"http://homepages.inf.ed.ac.uk/bundy/\">Alan Bundy</a> <a href=\"http://lesswrong.com/r/discussion/lw/9cm/qa_with_experts_on_risks_from_ai_3/\">replied</a>: “I don’t think the concept of ‘human-level machine intelligence’ is well formed.”</small></p>
<p><sup>3</sup> <small><a href=\"http://www.amazon.com/Mathematicians-Delight-Dover-Science-Books/dp/0486462404/\">Sawyer (1943)</a> gives another example: “Mathematicians first used the sign √-1, without in the least knowing what it could mean, because it shortened work and led to correct results. People naturally tried to find out why this happened and what √-1 really meant. After two hundreds years they succeeded.” <a href=\"http://www.amazon.com/Intuition-Pumps-Other-Tools-Thinking/dp/0393082067/\">Dennett (2013)</a> makes a related comment: “<em>Define your terms, sir!</em> No, I won’t. That would be premature… My [approach] is an instance of <em>nibbling</em> on a tough problem instead of trying to eat (and digest) the whole thing from the outset… In <em>Elbow Room</em>, I compared my method to the sculptor’s method of roughing out the form in a block of marble, approaching the final surfaces cautiously, modestly, working by successive approximation.”</small></p>
<p><sup>4</sup> <small>With self-driving cars, researchers did use many precise external performance measures (e.g. accident rates, speed, portion of the time they could run unassisted, frequency of getting stuck) to evaluate progress, as well as internal performance metrics (speed of search, bounded loss guarantees, etc.). Researchers could see that these bits of progress were in the right direction, even if their relative contribution long-term was unclear. And so it is with AI in general. AI researchers use many precise external and internal performance measures to evaluate progress, but it is difficult to know the relative contribution of these bits of progress toward the final goal of AGI.</small></p>
<p><sup>5</sup> <small>Heck, we’ve had pornography for millennia and <em>still</em> haven’t been able to define it precisely. Encyclopedia entries for “pornography” <a href=\"http://books.google.com/books?id=xOJvVJv2YlAC&pg=PA636&lpg=PA636&source=bl&ots=N3PI1PbmK1&sig=SIhfXbej_Z_HRDAxxnGZWftTZhg&hl=en&sa=X&ei=9o-NUfTEAqafiALa34CoDQ&ved=0CC8Q6AEwADgK#v=onepage&q&f=false\">often</a> <a href=\"http://books.google.com/books?id=TN-qpt7kAK4C&pg=PA336&lpg=PA336&source=bl&ots=CspWxfT67z&sig=_ymQ5x3lvNnMF0DsaSZkJzsSaqM&hl=en&sa=X&ei=9o-NUfTEAqafiALa34CoDQ&ved=0CDMQ6AEwAjgK#v=onepage&q&f=false\">simply</a> quote Justice Potter Stewart: “I shall not today attempt further to define the kinds of material I understand to be [pornography]… but I know it when I see it.”</small></p>
<p><sup>6</sup> <small>We might care about whether machines are conscious in addition to being intelligent, but we already have a convenient term for that: <em>consciousness</em>. In particular, we might care about machine consciousness because the slow, plodding invention of AI may involve the creation and destruction of millions of partially-conscious near-AIs that are switched on, suffer for a while, and are then switched off — all while being unable to signal to us that they are suffering. This is especially likely if we remain unclear about the nature of consciousness for several more decades, and thus have no principled way (e.g. via <a href=\"http://lesswrong.com/lw/x4/nonperson_predicates/\">nonperson predicates</a>) to create intelligent machines that we <em>know</em> are not conscious (and are thus incapable of suffering). One of the first people to make this point clearly was <a href=\"http://www.amazon.com/Being-No-One-Self-Model-Subjectivity/dp/0262633086/\">Metzinger (2003)</a>, p. 621: “What would you say if someone came along and said, ‘Hey, we want to genetically engineer mentally retarded human infants! For reasons of scientific progress we need infants with certain cognitive and emotional deficits in order to study their postnatal psychological development—we urgently need some funding for this important and innovative kind of research!’ You would certainly think this was not only an absurd and appalling but also a dangerous idea. It would hopefully not pass any ethics committee in the democratic world. However, what today’s ethics committees don’t see is how the first machines satisfying a minimally sufficient set of constraints for conscious experience could be just like such mentally retarded infants. They would suffer from all kinds of functional and representational deficits too. But they would now also subjectively experience those deficits. In addition, they would have no political lobby—no representatives in any ethics committee.” Metzinger repeats the point in <a href=\"http://www.amazon.com/Ego-Tunnel-Science-Mind-Myth/dp/0465020690/\">Metzinger (2010)</a>, starting on page 194.</small></p>
<p><sup>7</sup> <small>Admittedly, this is still pretty vague. One step toward precision would be to propose a definition of intelligence as optimization power for some canonical distribution of possible preferences, over some canonical distribution of environments, with a penalty for resource use. The canonical preferences and canonical environments could be weighted toward preferences and environments relevant to our concerns: we care more about whether AIs can do science than whether they can paint abstract art, and we care more about whether they can achieve their goals in our solar system than whether they can achieve their goals inside a black hole. Also see <a href=\"http://agi-conf.org/2010/wp-content/uploads/2009/06/paper_14.pdf\">Goertzel (2010)</a>‘s “efficient pragmatic general intelligence.”</small></p>
<p><sup>8</sup> <small><a href=\"http://www.ssec.wisc.edu/~billh/g/hibbard_agi11a.pdf\">Hibbard (2011)</a>; <a href=\"http://arxiv.org/pdf/1109.5951.pdf\">Legg & Veness (2011)</a>; <a href=\"http://www.cis.temple.edu/~wangp/Publication/AI_Definitions.pdf\">Wang (2008)</a>; <a href=\"http://arxiv.org/pdf/1109.1314.pdf\">Schaul et al. (2011)</a>; <a href=\"http://www.csse.monash.edu.au/~dld/Publications/2012/Dowe%2BHernandez-Orallo_2012_IQ_tests_are_not_for_machines_comma_yet_IN_PRESS.pdf\">Dowe & Hernandez-Orallo (2012)</a>; <a href=\"http://agi-conf.org/2010/wp-content/uploads/2009/06/paper_14.pdf\">Goertzel (2010)</a>; <a href=\"http://www.cse.buffalo.edu/faculty/shapiro/Papers/hlai.pdf\">Adams et al. (2011)</a>.</small></p>
<p>The post <a href=\"http://intelligence.org/2013/06/19/what-is-intelligence-2/\">What is Intelligence?</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/06/19/what-is-intelligence-2/#comments" "394ff21a12673d452946a3e751c88a9f") (7 (20955 50119 253371) "http://intelligence.org/2013/06/07/miris-july-2013-workshop/?utm_source=rss&utm_medium=rss&utm_campaign=miris-july-2013-workshop" "=?utf-8?Q?MIRI=E2=80=99s?= July 2013 Workshop" "Luke Muehlhauser" "Fri, 07 Jun 2013 22:15:07 +0000" "<a href=\"http://intelligence.org/get-involved/#workshop\"><img class=\"aligncenter size-full wp-image-10250\" alt=\"Mihaly at April workshop\" src=\"http://intelligence.org/wp-content/uploads/2013/06/Mihaly-at-April-workshop.jpg\" width=\"500\" height=\"308\" /></a>
<p>From July 8-14, MIRI will host its <strong>3rd Workshop on Logic, Probability, and Reflection</strong>. The focus of this workshop will be the <a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/\">Löbian obstacle to self-modifying systems</a>.</p>
<p>Participants confirmed so far include:</p>
<ul>
<li><a href=\"http://acritch.com/\">Andrew Critch</a> (just finished his math PhD at UC Berkeley, now working at <a href=\"http://rationality.org/\">CFAR</a>)</li>
<li><a href=\"https://plus.google.com/111568410659864255951\">Abram Demski</a> (USC)</li>
<li><a href=\"http://lesswrong.com/user/Benja/submitted/\">Benja Fallenstein</a> (Bristol U)</li>
<li><a href=\"http://www.linkedin.com/pub/marcello-herreshoff/0/8b4/51a\">Marcello Herreshoff </a>(Google)</li>
<li>Jonathan Lee (Cambridge)</li>
<li><a href=\"http://www.ctpost.com/news/article/Wisdom-beyond-his-years-1390299.php\">Will Sawin</a> (Princeton)</li>
<li><a href=\"http://math.berkeley.edu/~qchu/\">Qioachu Yuan</a> (UC Berkeley)</li>
<li><a href=\"http://yudkowsky.net/\">Eliezer Yudkowsky</a> (MIRI)</li>
</ul>
<p>If you have a strong mathematics background and might like to attend this workshop, it’s not too late to <a href=\"http://intelligence.org/get-involved/#workshop\">apply</a>! And even if <em>this</em> workshop doesn’t fit your schedule, please <strong>do apply</strong>, so that we can notify you of other workshops (long before they are announced publicly).</p>
<p>Information on past workshops:</p>
<ul>
<li><span style=\"line-height: 13px;\">Our<strong> 1st Workshop</strong> (Nov. 11-18, 2012; 4 participants) resulted in Christiano’s <a href=\"http://lesswrong.com/lw/h1k/reflection_in_probabilistic_logic/\">probabilistic logic</a>, an attack on the <a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/\">Löbian obstacle for self-modifying systems</a>.<br />
</span></li>
<li>Our <strong>2nd Workshop</strong> (Apr. 3-24, 2013; 12 participants coming in and out) resulted in (1) some as-yet unpublished progress on Christiano’s probabilistic logic, (2) some progress on program equilibrium recorded in <a href=\"http://intelligence.org/files/RobustCooperation.pdf\">LaVictoire et al. (2013)</a>, and some progress on the Löbian obstacle resulting in <a href=\"http://intelligence.org/files/TilingAgents.pdf\">Yudkowsky & Herreschoff (2013)</a>.</li>
</ul>
<p>The post <a href=\"http://intelligence.org/2013/06/07/miris-july-2013-workshop/\">MIRI’s July 2013 Workshop</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/06/07/miris-july-2013-workshop/#comments" "0ee667163db50993172e906f8452ab92") (6 (20954 62970 758888) "http://intelligence.org/2013/06/19/what-is-intelligence-2/?utm_source=rss&utm_medium=rss&utm_campaign=what-is-intelligence-2" "What is Intelligence?" "Luke Muehlhauser" "Wed, 19 Jun 2013 19:59:12 +0000" "<p><img class=\"alignright size-full wp-image-10276\" alt=\"brain of gears and circuits\" src=\"http://intelligence.org/wp-content/uploads/2013/06/brain-of-gears-and-circuits.jpg\" width=\"250\" height=\"327\" />When asked their opinions about “human-level artificial intelligence” — <em>aka</em> “artificial general intelligence” (AGI)<sup>1</sup> — many experts understandably reply that these terms haven’t yet been precisely defined, and it’s hard to talk about something that hasn’t been defined.<sup>2</sup> In this post, I want to briefly outline an imprecise but useful “working definition” for <em>intelligence</em> we tend to use at MIRI. In a future post I will write about some useful working definitions for <em>artificial general intelligence</em>.</p>
<p> </p>
<h3>Imprecise definitions can be useful</h3>
<p>Precise definitions are important, but I concur with Bertrand Russell that</p>
<blockquote>[You cannot] start with anything precise. You have to achieve such precision… as you go along.</p></blockquote>
<p>Physicist <a href=\"http://ieet.org/index.php/IEET/bio/cirkovic/\">Milan Ćirković</a> agrees, and <a href=\"http://www.amazon.com/Astrobiological-Landscape-Start-Publishing-ebook/dp/B008CDSB30/\">gives</a> an example:</p>
<blockquote><p>The formalization of knowledge — which includes giving precise definitions — usually comes at the end of the original research in a given field, not at the very beginning. A particularly illuminating example is the concept of <em>number</em>, which was properly defined in the modern sense only after the development of axiomatic set theory in the… twentieth century.<sup>3</sup></p></blockquote>
<p>For a more AI-relevant example, consider the concept of a “self-driving car,” which has been given a variety of vague definitions <a href=\"http://books.google.com/books?id=7OEDAAAAMBAJ&lpg=PA210&dq=automatic%20car&pg=PA210#v=onepage&q&f=false\">since the 1930s</a>. Would a car <a href=\"http://books.google.com/books?id=xiUDAAAAMBAJ&lpg=PA75&dq=car%20drives%20itself&pg=PA75#v=onepage&q&f=false\">guided by a buried cable</a> qualify? What about a <a href=\"http://books.google.com/books?id=jd8DAAAAMBAJ&lpg=PA128&dq=automatic%20car&pg=PA128#v=onepage&q&f=false\">modified 1955 Studebaker</a> that could use sound waves to detect obstacles and automatically engage the brakes if necessary, but could only steer “on its own” if each turn was preprogrammed? Does that count as a “self-driving car”?</p>
<p>What about the “<a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/05/Dickmanns-et-al-The-seeing-passenger-car-VaMoRs-P.pdf\">VaMoRs</a>” of the 1980s that could avoid obstacles and steer around turns using computer vision, but weren’t advanced enough to be ready for public roads? How about the 1995 <a href=\"http://en.wikipedia.org/wiki/Navlab\">Navlab</a> car that drove across the USA and was fully autonomous for 98.2% of the trip, or the robotic cars which finished the 132-mile off-road course of the <a href=\"http://is.gd/CDvT8V\">2005 DARPA Grand Challenge</a>, supplied only with the GPS coordinates of the route? What about the winning cars of the <a href=\"http://is.gd/kOjor6\">2007 DARPA Grand Challenge</a>, which finished an urban race while obeying all traffic laws and avoiding collisions with other cars? Does <a href=\"http://www.forbes.com/sites/joannmuller/2013/03/21/no-hands-no-feet-my-unnerving-ride-in-googles-driverless-car/\">Google’s driverless car</a> qualify, given that it has logged more than 500,000 autonomous miles without a single accident under computer control, but still struggles with difficult merges and snow-covered roads?<sup>4</sup></p>
<p>Our lack of a precise definition for “self-driving car” doesn’t seem to have hindered progress on self-driving cars very much.<sup>5</sup> And I’m glad we didn’t wait to seriously discuss self-driving cars until we had a precise definition for the term.</p>
<p>Similarly, I don’t think we should wait for a precise definition of AGI before discussing the topic seriously. On the other hand, the term is useless if it carries <em>no</em> information. So let’s work our way toward a stipulative, operational definition for AGI. We’ll start by developing an operational definition for <em>intelligence</em>.</p>
<p><span id=\"more-10275\"></span></p>
<h3>A definition for “intelligence”</h3>
<p><a href=\"http://arxiv.org/pdf/0706.3639.pdf\">Legg and Hutter (2007)</a> found that definitions of intelligence converge toward the idea that “Intelligence measures an agent’s ability to achieve goals in a wide range of environments.” Let’s call this the “optimization power” concept of intelligence, because it measures an agent’s power to optimize the world according to its preferences.</p>
<p>I think this is a productive approach to the issue, since it identifies intelligence with externally measurable <em>performance</em> rather than with the details of <em>how</em> that performance might be achieved (e.g. via consciousness, <a href=\"http://www.hutter1.net/publ/uaigentle.pdf\">brute force calculation</a>, “complexity,” or something else). Moreover, it’s usually <em>performance</em> we care about: we tend to care most about whether an AI will perform well enough to replace human workers, or whether it will perform well enough improve its own abilities without human assistance, not whether it has some particular internal feature.<sup>6</sup></p>
<p>Furthermore, the concept of optimization power allows us to compare the intelligence of different kinds of agents. As Albus (<a href=\"ftp://calhau.dca.fee.unicamp.br/pub/docs/ia005/Albus-outline.pdf\">1991</a>) said, “A useful definition of intelligence… should include both biological and machine embodiments, and these should span an intellectual range from that of an insect to that of an Einstein, from that of a thermostat to that of the most sophisticated computer system that could ever be built.”</p>
<p>I’d like to add one more consideration, though. What if two agents have roughly equal ability to optimize the world according to their preferences, but the second agent requires far more resources to do so? These agents have the same optimization power, but the second one seems to be optimizing more intelligently. So perhaps we could use “intelligence” to mean “optimization power divided by resources used” — what Yudkowsky called <a href=\"http://lesswrong.com/lw/vb/efficient_crossdomain_optimization/\">efficient cross-domain optimization</a>.<sup>7</sup></p>
<p>Other definitions<sup>8</sup> have their merits, too. But at MIRI we find the concept of “efficient cross-domain optimization” sufficiently useful that it serves as our (still imprecise!) working definition for intelligence.</p>
<p>In a future post, I’ll discuss some useful working definitions for <em>artificial general intelligence</em>.</p>
<p> </p>
<h4>Notes</h4>
<p><sup>1</sup> <small>I use the HLAI and AGI interchangeably, but lately I’ve been using AGI almost exclusively, because I’ve learned that many people in the AI community react negatively to any mention of “human-level” AI but have no objection to the concept of narrow vs. general intelligence. See also Ben Goertzel’s comments <a href=\"http://wp.goertzel.org/?p=173\">here</a>.</small></p>
<p><sup>2</sup> <small>Asked when he thought HLAI would be created, Pat Hayes (a past president of <a href=\"http://en.wikipedia.org/wiki/Association_for_the_Advancement_of_Artificial_Intelligence\">AAAI</a>) <a href=\"http://lesswrong.com/r/discussion/lw/999/qa_with_experts_on_risks_from_ai_1/\">replied</a>: “I do not consider this question to be answerable, as I do not accept this (common) notion of ‘human-level intelligence’ as meaningful.” Asked the same question, AI scientist <a href=\"http://www.cse.unsw.edu.au/~willu/\">William Uther</a> <a href=\"http://lesswrong.com/r/discussion/lw/9cm/qa_with_experts_on_risks_from_ai_3/\">replied</a>: “You ask a lot about ‘human level AGI’. I do not think this term is well defined,” while AI scientist <a href=\"http://homepages.inf.ed.ac.uk/bundy/\">Alan Bundy</a> <a href=\"http://lesswrong.com/r/discussion/lw/9cm/qa_with_experts_on_risks_from_ai_3/\">replied</a>: “I don’t think the concept of ‘human-level machine intelligence’ is well formed.”</small></p>
<p><sup>3</sup> <small><a href=\"http://www.amazon.com/Mathematicians-Delight-Dover-Science-Books/dp/0486462404/\">Sawyer (1943)</a> gives another example: “Mathematicians first used the sign √-1, without in the least knowing what it could mean, because it shortened work and led to correct results. People naturally tried to find out why this happened and what √-1 really meant. After two hundreds years they succeeded.” <a href=\"http://www.amazon.com/Intuition-Pumps-Other-Tools-Thinking/dp/0393082067/\">Dennett (2013)</a> makes a related comment: “<em>Define your terms, sir!</em> No, I won’t. That would be premature… My [approach] is an instance of <em>nibbling</em> on a tough problem instead of trying to eat (and digest) the whole thing from the outset… In <em>Elbow Room</em>, I compared my method to the sculptor’s method of roughing out the form in a block of marble, approaching the final surfaces cautiously, modestly, working by successive approximation.”</small></p>
<p><sup>4</sup> <small>With self-driving cars, researchers did use many precise external performance measures (e.g. accident rates, speed, portion of the time they could run unassisted, frequency of getting stuck) to evaluate progress, as well as internal performance metrics (speed of search, bounded loss guarantees, etc.). Researchers could see that these bits of progress were in the right direction, even if their relative contribution long-term was unclear. And so it is with AI in general. AI researchers use many precise external and internal performance measures to evaluate progress, but it is difficult to know the relative contribution of these bits of progress toward the final goal of AGI.</small></p>
<p><sup>5</sup> <small>Heck, we’ve had pornography for millennia and <em>still</em> haven’t been able to define it precisely. Encyclopedia entries for “pornography” <a href=\"http://books.google.com/books?id=xOJvVJv2YlAC&pg=PA636&lpg=PA636&source=bl&ots=N3PI1PbmK1&sig=SIhfXbej_Z_HRDAxxnGZWftTZhg&hl=en&sa=X&ei=9o-NUfTEAqafiALa34CoDQ&ved=0CC8Q6AEwADgK#v=onepage&q&f=false\">often</a> <a href=\"http://books.google.com/books?id=TN-qpt7kAK4C&pg=PA336&lpg=PA336&source=bl&ots=CspWxfT67z&sig=_ymQ5x3lvNnMF0DsaSZkJzsSaqM&hl=en&sa=X&ei=9o-NUfTEAqafiALa34CoDQ&ved=0CDMQ6AEwAjgK#v=onepage&q&f=false\">simply</a> quote Justice Potter Stewart: “I shall not today attempt further to define the kinds of material I understand to be [pornography]… but I know it when I see it.”</small></p>
<p><sup>6</sup> <small>We might care about whether machines are conscious in addition to being intelligent, but we already have a convenient term for that: <em>consciousness</em>. In particular, we might care about machine consciousness because the slow, plodding invention of AI may involve the creation and destruction of millions of partially-conscious near-AIs that are switched on, suffer for a while, and are then switched off — all while being unable to signal to us that they are suffering. This is especially likely if we remain unclear about the nature of consciousness for several more decades, and thus have no principled way (e.g. via <a href=\"http://lesswrong.com/lw/x4/nonperson_predicates/\">nonperson predicates</a>) to create intelligent machines that we <em>know</em> are not conscious (and are thus incapable of suffering). One of the first people to make this point clearly was <a href=\"http://www.amazon.com/Being-No-One-Self-Model-Subjectivity/dp/0262633086/\">Metzinger (2003)</a>, p. 621: “What would you say if someone came along and said, ‘Hey, we want to genetically engineer mentally retarded human infants! For reasons of scientific progress we need infants with certain cognitive and emotional deficits in order to study their postnatal psychological development—we urgently need some funding for this important and innovative kind of research!’ You would certainly think this was not only an absurd and appalling but also a dangerous idea. It would hopefully not pass any ethics committee in the democratic world. However, what today’s ethics committees don’t see is how the first machines satisfying a minimally sufficient set of constraints for conscious experience could be just like such mentally retarded infants. They would suffer from all kinds of functional and representational deficits too. But they would now also subjectively experience those deficits. In addition, they would have no political lobby—no representatives in any ethics committee.” Metzinger repeats the point in <a href=\"http://www.amazon.com/Ego-Tunnel-Science-Mind-Myth/dp/0465020690/\">Metzinger (2010)</a>, starting on page 194.</small></p>
<p><sup>7</sup> <small>Admittedly, this is still pretty vague. One step toward precision would be to propose a definition of intelligence as optimization power for some canonical distribution of possible preferences, over some canonical distribution of environments, with a penalty for resource use. The canonical preferences and canonical environments could be weighted toward preferences and environments relevant to our concerns: we care more about whether AIs can do science than whether they can paint abstract art, and we care more about whether they can achieve their goals in our solar system than whether they can achieve their goals inside a black hole. Also see <a href=\"http://agi-conf.org/2010/wp-content/uploads/2009/06/paper_14.pdf\">Goertzel (2010)</a>‘s “efficient pragmatic general intelligence.”</small></p>
<p><sup>8</sup> <small><a href=\"http://www.ssec.wisc.edu/~billh/g/hibbard_agi11a.pdf\">Hibbard (2011)</a>; <a href=\"http://arxiv.org/pdf/1109.5951.pdf\">Legg & Veness (2011)</a>; <a href=\"http://www.cis.temple.edu/~wangp/Publication/AI_Definitions.pdf\">Wang (2008)</a>; <a href=\"http://arxiv.org/pdf/1109.1314.pdf\">Schaul et al. (2011)</a>; <a href=\"http://www.csse.monash.edu.au/~dld/Publications/2012/Dowe%2BHernandez-Orallo_2012_IQ_tests_are_not_for_machines_comma_yet_IN_PRESS.pdf\">Dowe & Hernandez-Orallo (2012)</a>; <a href=\"http://agi-conf.org/2010/wp-content/uploads/2009/06/paper_14.pdf\">Goertzel (2010)</a>; <a href=\"http://www.cse.buffalo.edu/faculty/shapiro/Papers/hlai.pdf\">Adams et al. (2011)</a>.</small></p>
<p>The post <a href=\"http://intelligence.org/2013/06/19/what-is-intelligence-2/\">What is Intelligence?</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/06/19/what-is-intelligence-2/#comments" "68187908540eeacf83aa68ce4cd0b05e") (5 (20954 62970 754960) "http://intelligence.org/2013/06/07/miris-july-2013-workshop/?utm_source=rss&utm_medium=rss&utm_campaign=miris-july-2013-workshop" "=?utf-8?Q?MIRI=E2=80=99s?= July 2013 Workshop" "Luke Muehlhauser" "Fri, 07 Jun 2013 22:15:07 +0000" "<p><a href=\"http://intelligence.org/get-involved/#workshop\"><img class=\"aligncenter size-full wp-image-10250\" alt=\"Mihaly at April workshop\" src=\"http://intelligence.org/wp-content/uploads/2013/06/Mihaly-at-April-workshop.jpg\" width=\"500\" height=\"308\" /></a></p>
<p>From July 8-14, MIRI will host its <strong>3rd Workshop on Logic, Probability, and Reflection</strong>. The focus of this workshop will be the <a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/\">Löbian obstacle to self-modifying systems</a>.</p>
<p>Participants confirmed so far include:</p>
<ul>
<li><a href=\"http://acritch.com/\">Andrew Critch</a> (just finished his math PhD at UC Berkeley, now working at <a href=\"http://rationality.org/\">CFAR</a>)</li>
<li><a href=\"https://plus.google.com/111568410659864255951\">Abram Demski</a> (USC)</li>
<li><a href=\"http://lesswrong.com/user/Benja/submitted/\">Benja Fallenstein</a> (Bristol U)</li>
<li><a href=\"http://www.linkedin.com/pub/marcello-herreshoff/0/8b4/51a\">Marcello Herreshoff </a>(Google)</li>
<li>Jonathan Lee (Cambridge)</li>
<li><a href=\"http://www.ctpost.com/news/article/Wisdom-beyond-his-years-1390299.php\">Will Sawin</a> (Princeton)</li>
<li><a href=\"http://math.berkeley.edu/~qchu/\">Qioachu Yuan</a> (UC Berkeley)</li>
<li><a href=\"http://yudkowsky.net/\">Eliezer Yudkowsky</a> (MIRI)</li>
</ul>
<p>If you have a strong mathematics background and might like to attend this workshop, it’s not too late to <a href=\"http://intelligence.org/get-involved/#workshop\">apply</a>! And even if <em>this</em> workshop doesn’t fit your schedule, please <strong>do apply</strong>, so that we can notify you of other workshops (long before they are announced publicly).</p>
<p>Information on past workshops:</p>
<ul>
<li><span style=\"line-height: 13px;\">Our<strong> 1st Workshop</strong> (Nov. 11-18, 2012; 4 participants) resulted in Christiano’s <a href=\"http://lesswrong.com/lw/h1k/reflection_in_probabilistic_logic/\">probabilistic logic</a>, an attack on the <a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/\">Löbian obstacle for self-modifying systems</a>.<br />
</span></li>
<li>Our <strong>2nd Workshop</strong> (Apr. 3-24, 2013; 12 participants coming in and out) resulted in (1) some as-yet unpublished progress on Christiano’s probabilistic logic, (2) some progress on program equilibrium recorded in <a href=\"http://intelligence.org/files/RobustCooperation.pdf\">LaVictoire et al. (2013)</a>, and some progress on the Löbian obstacle resulting in <a href=\"http://intelligence.org/files/TilingAgents.pdf\">Yudkowsky & Herreschoff (2013)</a>.</li>
</ul>
<p>The post <a href=\"http://intelligence.org/2013/06/07/miris-july-2013-workshop/\">MIRI’s July 2013 Workshop</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/06/07/miris-july-2013-workshop/#comments" "5dd8c96ab7d5dee064f1bc9f95e46e40") (4 (20954 62970 754206) "http://intelligence.org/2013/06/06/new-research-page-and-two-new-articles/?utm_source=rss&utm_medium=rss&utm_campaign=new-research-page-and-two-new-articles" "New Research Page and Two New Articles" "Luke Muehlhauser" "Fri, 07 Jun 2013 06:54:02 +0000" "<p><a href=\"/research/\"><img class=\"aligncenter size-full wp-image-10246\" alt=\"research page\" src=\"http://intelligence.org/wp-content/uploads/2013/06/research-page.png\" width=\"500\" height=\"321\" /></a></p>
<p>Our new <a href=\"http://intelligence.org/research/\">Research</a> page has launched!</p>
<p>Our previous research page was a simple list of articles, but the new page describes the purpose of our research, explains four categories of research to which we contribute, and highlights the papers we think are most important to read.</p>
<p>We’ve also released drafts of two new research articles.</p>
<p><a href=\"http://intelligence.org/files/TilingAgents.pdf\">Tiling Agents for Self-Modifying AI, and the Löbian Obstacle</a> (discuss it <a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/\">here</a>), by Yudkowsky and Herreshoff, explains one of the key open problems in MIRI’s research agenda:</p>
<blockquote><p>We model self-modification in AI by introducing “tiling” agents whose decision systems will approve the construction of highly similar agents, creating a repeating pattern (including similarity of the offspring’s goals). Constructing a formalism in the most straightforward way produces a Gödelian difficulty, the “Löbian obstacle.” By technical methods we demonstrates the possibility of avoiding this obstacle, but the underlying puzzles of rational coherence are thus only partially addressed. We extend the formalism to partially unknown deterministic environments, and show a very crude extension to probabilistic environments and expected utility; but the problem of finding a fundamental decision criterion for self-modifying probabilistic agents remains open.</p></blockquote>
<p><a href=\"http://intelligence.org/files/RobustCooperation.pdf\">Robust Cooperation in the Prisoner’s Dilemma: Program Equilibrium via Provability Logic</a> (discuss it <a href=\"http://lesswrong.com/lw/hmw/robust_cooperation_in_the_prisoners_dilemma/\">here</a>), by LaVictoire et al., explains some progress in program equilibrium made by MIRI research associate Patrick LaVictoire and several others during MIRI’s April 2013 workshop:</p>
<blockquote><p>Rational agents defect on the one-shot prisoner’s dilemma even though mutual cooperation would yield higher utility for both agents. Moshe Tennenholtz showed that if each program is allowed to pass its playing strategy to all other players, some programs can then cooperate on the one-shot prisoner’s dilemma. Program equilibria is Tennenholtz’s term for Nash equilibria in a context where programs can pass their playing strategies to the other players.</p>
<p>One weakness of this approach so far has been that any two programs which make different choices cannot “recognize” each other for mutual cooperation, even if they are functionally identical. In this paper, provability logic is used to enable a more flexible and secure form of mutual cooperation.</p></blockquote>
<p>Participants of MIRI’s April workshop also made progress on <a href=\"http://lesswrong.com/lw/h1k/reflection_in_probabilistic_logic/\">Christiano’s probabilistic logic</a> (an attack on the Löbian obstacle), but that work is not yet ready to be released.</p>
<p>We’ve also revamped the <a href=\"http://intelligence.org/get-involved/\">Get Involved</a> page, which now includes an <a href=\"/get-involved/#workshop\">application form</a> for forthcoming workshops. If you <em>might</em> like to work with MIRI on some of its open research problems sometime in the next 18 months, <a href=\"/get-involved/#workshop\">please apply</a>! Likewise, if you know someone who might enjoy attending such a workshop, please encourage <em>them</em> to apply.</p>
<p>The post <a href=\"http://intelligence.org/2013/06/06/new-research-page-and-two-new-articles/\">New Research Page and Two New Articles</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/06/06/new-research-page-and-two-new-articles/#comments" "3d723ea88e5053947a7e516ffa4dcb75") (3 (20954 62970 752799) "http://intelligence.org/2013/06/05/friendly-ai-research-as-effective-altruism/?utm_source=rss&utm_medium=rss&utm_campaign=friendly-ai-research-as-effective-altruism" "Friendly AI Research as Effective Altruism" "Luke Muehlhauser" "Thu, 06 Jun 2013 00:55:23 +0000" "<p><iframe src=\"http://embed.ted.com/talks/peter_singer_the_why_and_how_of_effective_altruism.html\" width=\"560\" height=\"315\" frameborder=\"0\" scrolling=\"no\" webkitAllowFullScreen mozallowfullscreen allowFullScreen></iframe></p>
<p>MIRI was founded in 2000 on the premise that creating<sup>1</sup> Friendly AI might be a particularly efficient way to do as much good as possible.</p>
<p>Some developments since then include:</p>
<ul>
<li>The field of “<a href=\"http://www.ted.com/talks/peter_singer_the_why_and_how_of_effective_altruism.html\">effective altruism</a>” — trying not just to do good but to do <em>as much good as possible</em><sup>2</sup> — has seen more publicity and better research than ever before, in particular through the work of <a href=\"http://www.givewell.org/\">GiveWell</a>, the <a href=\"http://centreforeffectivealtruism.org/\">Center for Effective Altruism</a>, the philosopher <a href=\"http://en.wikipedia.org/wiki/Peter_Singer\">Peter Singer</a>, and the community at <a href=\"http://lesswrong.com/\">Less Wrong</a>.<sup>3</sup></li>
<li>In his recent <a href=\"https://sites.google.com/site/nbeckstead/research/Beckstead%2C%20Nick--On%20the%20Overwhelming%20Importance%20of%20Shaping%20the%20Far%20Future.pdf?attredirects=0&d=1\">PhD dissertation</a>, <a href=\"https://sites.google.com/site/nbeckstead/\">Nick Beckstead</a> has clarified the assumptions behind the claim that shaping the far future (e.g. via Friendly AI) is overwhelmingly important.</li>
<li>Due to research performed by MIRI, the <a href=\"http://www.fhi.ox.ac.uk/\">Future of Humanity Institute</a> (FHI), and others, our strategic situation with regard to machine superintelligence is more clearly understood, and FHI’s <a href=\"http://nickbostrom.com/\">Nick Bostrom</a> has organized much of this work in a <a href=\"http://lesswrong.com/lw/bd6/ai_risk_opportunity_a_timeline_of_early_ideas_and/91zl\">forthcoming book</a>.<sup>4</sup></li>
<li>MIRI’s Eliezer Yudkowsky has <a href=\"http://lesswrong.com/lw/hmt/tiling_agents_for_selfmodifying_ai_opfai_2/\">begun</a> to describe in more detail which open research problems constitute “Friendly AI research,” in his view.</li>
</ul>
<p>Given these developments, we are in a better position than ever before to assess the value of Friendly AI research as effective altruism.</p>
<p>Still, this is a difficult question. It is challenging enough to evaluate the cost-effectiveness of <a href=\"http://blog.givewell.org/2012/10/18/revisiting-the-case-for-insecticide-treated-nets-itns/\">anti-malaria nets</a> or <a href=\"http://www.givewell.org/international/top-charities/give-directly\">direct cash transfers</a>. Evaluating the cost-effectiveness of attempts to shape the far future (e.g. via Friendly AI) is even more difficult than that. Hence, <strong>this short post sketches an argument that can be given in favor of Friendly AI research as effective altruism, to enable future discussion</strong>, and is <strong>not intended as a thorough analysis.</strong></p>
<p><span id=\"more-10240\"></span></p>
<h3>An argument for Friendly AI research as effective altruism</h3>
<p><a href=\"https://sites.google.com/site/nbeckstead/research/Beckstead%2C%20Nick--On%20the%20Overwhelming%20Importance%20of%20Shaping%20the%20Far%20Future.pdf?attredirects=0&d=1\">Beckstead (2013)</a> argues<sup>5</sup> for the following thesis:</p>
<blockquote><p>From a global perspective, what matters most (in expectation) is that we do what is best (in expectation) for the general trajectory along which our descendants develop over the coming millions, billions, and trillions of years.</p></blockquote>
<p>Why think this? Astronomical facts suggest that humanity (including “post-humanity”) could survive for billions or trillions of years (<a href=\"http://books.google.com/books?id=X5jdMyJKNL4C&pg=PT77&lpg=PT77#v=onepage&q&f=false\">Adams 2008</a>), and could thus produce enormous amounts of good.<sup>6</sup> But the value produced by our future depends on our <em>development trajectory</em>. If humanity destroys itself with powerful technologies in the 21st century, then nearly all that future value is lost. And if we survive but develop along a trajectory dominated by conflict and poor decisions, then the future could be much less good than if our trajectory is dominated by altruism and wisdom. Moreover, some of our actions today can have “ripple effects”<sup>7</sup> which determine the trajectory of human development, because many outcomes are <a href=\"http://en.wikipedia.org/wiki/Path_dependence\">path-dependent</a>. Hence, actions which directly or indirectly precipitate particular trajectory changes (e.g. mitigating existential risks) can have vastly more value (in expectation) than actions with merely proximate benefits (e.g. saving the lives of 20 wild animals). Beckstead calls this the “rough future-shaping argument.”</p>
<p>If we accept the normative assumptions lurking behind this argument (e.g. <a href=\"http://en.wikipedia.org/wiki/Risk_neutral\">risk neutrality</a>; see Beckstead’s dissertation), then the far future is enormously valuable (if it goes at least as well on average as the past century), and existential risk reduction is much more important than producing proximate benefits (e.g. global health, poverty reduction) or speeding up development (which could in fact increase existential risks, and even if it doesn’t, has lower expected value than existential risk reduction).</p>
<p>However, Beckstead’s conclusion is not necessarily that existential risk reduction should be our global priority, because</p>
<blockquote><p>there may be other ways to have a large, persistent effect on the far future without reducing existential risk… Some persistent changes in values and social norms could make the future [some fraction] better or worse… Sure, succeeding in preventing an existential catastrophe would be better than making a smaller trajectory change, but creating a small positive trajectory change may be significantly easier.</p></blockquote>
<p>Instead, Beckstead’s arguments suggest that “what matters most for shaping the far future is producing positive trajectory changes and avoiding negative ones.” Existential risk reduction is one important kind of positive trajectory change that could turn out to be the intervention with the highest expected value.</p>
<p>One important clarification is in order. It could turn out to be that working toward proximate benefits or development acceleration does more good than “direct” efforts for trajectory change, if working toward proximate benefits or development acceleration turns out to have major ripple effects which produce important trajectory change. For example, perhaps an “ordinary altruistic effort” like solving India’s <a href=\"http://www.dnaindia.com/india/1593586/report-71-million-hit-by-iodine-deficiency-in-india\">iodine deficiency problem</a> would cause there to be thousands of “extra” world-class elite thinkers two generations from now, which could increase humanity’s chances of intelligently navigating the crucial 21st century and spreading to the stars. (I don’t think this is likely; I suggest it merely for illustration.)</p>
<p>For the sake of argument, suppose you agree with Beckstead’s core thesis that “what matters most (in expectation) is that we do what is best (in expectation) for the general trajectory along which our descendants develop.” Suppose you also think, as I do, that machine superintelligence is probably inevitable.<sup>8</sup></p>
<p>In that case, you might think that Friendly AI research is a uniquely foreseeable and impactful way to shape the far future in an enormously positive way, <a href=\"http://lesswrong.com/lw/hjb/a_proposed_adjustment_to_the_astronomical_waste/91zq\">because</a> “our effects on the far future must almost entirely pass through our effects on the development of machine superintelligence.” All other developing trends might be overridden by the overwhelming effectiveness of machine superintelligence — and specifically, by the values that were (explicitly or implicitly, directly or indirectly) written into the machine superintelligence(s).</p>
<p>If that’s right, our situation is a bit like sending an interstellar probe to <a href=\"http://commonsenseatheism.com/wp-content/uploads/2013/05/Armstrong-Sandberg-Eternity-in-six-hours-intergalactic-spreading-of-intelligent-life-and-sharpening-the-Fermi-paradox.pdf\">colonize distant solar systems</a> before they recede beyond the <a href=\"http://en.wikipedia.org/wiki/Observable_universe#Particle_horizon\">cosmological horizon</a> and can thus never be reached from Earth again due to <a href=\"https://en.wikipedia.org/wiki/Metric_expansion_of_space\">the expansion of the universe</a>. Anything on Earth that doesn’t affect the content of the probe will have no impact on those solar systems. (See also <a href=\"http://lesswrong.com/lw/hjb/a_proposed_adjustment_to_the_astronomical_waste/921r\">this comment</a>.)</p>
<p> </p>
<h3>Potential defeaters</h3>
<p>The rough argument above — in favor of Friendly AI research as an efficient form of effective altruism — deserves to be “fleshed out” in more detail.<sup>9</sup></p>
<p>Potential defeaters should also be examined:</p>
<ul>
<li>Perhaps we ought to reject one or more of the normative assumptions behind Beckstead’s rough future-shaping argument.</li>
<li>Perhaps it’s not true that “our effects on the far future must almost entirely pass through our effects on the development of machine superintelligence.”</li>
<li>Perhaps Friendly AI research is not (today) a particularly efficient way to positively affect the development of machine superintelligence. Competing interventions may include: (1) <a href=\"http://lesswrong.com/lw/bd6/ai_risk_opportunity_a_timeline_of_early_ideas_and/91zl\">AI risk strategy research</a>, (2) improving <a href=\"http://intelligence.org/2013/05/15/when-will-ai-be-created/\">technological forecasting</a>, (3) <a href=\"http://www.vannevargroup.org/\">improving science in general</a>, (4) <a href=\"http://rationalaltruist.com/2013/06/03/my-outlook/\">improving and spreading effective altruism and rationality</a>, and (5) many others.</li>
</ul>
<p>In future blog posts, members of the effective altruist community (including myself) will expand on the original argument and examine potential defeaters.</p>
<h4></h4>
<h4>Notes</h4>
<p><small>My thanks to those who provided feedback on this post: Carl Shulman, Nick Beckstead, Jonah Sinick, and Eliezer Yudkowsky.</small></p>
<p><sup>1</sup> <small>In this post, I talk about the value of <em>humanity in general</em> creating Friendly AI, though MIRI co-founder Eliezer Yudkowsky usually talks about <em>MIRI in particular</em> — or at least, a functional equivalent — creating Friendly AI. This is because I am not as confident as Yudkowsky that it is best for MIRI to attempt to build Friendly AI. When updating MIRI’s bylaws in early 2013, Yudkowsky and I came to a compromise on the language of MIRI’s mission statement, which now reads: “[MIRI] exists to ensure that the creation of smarter-than-human intelligence has a positive impact. Thus, the charitable purpose of [MIRI] is to: (a) perform research relevant to ensuring that smarter-than-human intelligence has a positive impact; (b) raise awareness of this important issue; (c) advise researchers, leaders and laypeople around the world; and (d) <em>as necessary</em>, implement a smarter-than-human intelligence with humane, stable goals” (emphasis added). My own hope is that it will not be necessary for MIRI (or a functional equivalent) to attempt to build Friendly AI itself. But of course I must remain open to the possibility that this will be the wisest course of action as the first creation of AI <a href=\"http://intelligence.org/2013/05/15/when-will-ai-be-created/\">draws nearer</a>. There is also the question of capability: few people think that a non-profit research organization has much chance of being the first to build AI. I worry, however, that the world’s elites will not find it fashionable to take this problem seriously until the creation of AI is only a few decades away, at which time it will be especially difficult to develop the mathematics of Friendly AI in time, and humanity will be forced to take a gamble on its very survival with powerful AIs we have little reason to trust.</small></p>
<p><sup>2</sup> <small>One might think of effective altruism as a straightforward application of <a href=\"http://lesswrong.com/lw/gu1/decision_theory_faq/\">decision theory</a> to the subject of philanthropy. Philanthropic agents of all kinds (individuals, groups, foundations, etc.) ask themselves: “How can we choose philanthropic acts (e.g. donations) which (in expectation) will do as much good as possible, given what we care about?” The consensus recommendation for <em>all</em> kinds of choices under uncertainty, including philanthropic choices, is to maximize expected utility (<a href=\"http://books.google.com/books?id=S1-K4AT3zXYC&lpg=PA11&ots=wjavib87qF&dq=normative%20systems%3A%20logic%2C%20probability%2C%20and%20rational%20choice&lr&pg=PA11#v=onepage&q&f=false\">Chater & Oaksford 2012</a>; <a href=\"http://commonsenseatheism.com/wp-content/uploads/2012/05/Peterson-From-Outcomes-to-Acts-a-non-standard-axiomatization-of-the-expected-utility-principle.pdf\">Peterson 2004</a>; <a href=\"http://www.amazon.com/Without-Good-Reason-Rationality-Philosophy/dp/0198235747/\">Stein 1996</a>; <a href=\"http://www.amazon.com/Axiomatic-Utility-Theory-under-Risk/dp/3540643192\">Schmidt 1998</a>:19). Different philanthropic agents value different things, but decision theory suggests that each of them can get the most of what they want if they each maximize their expected utility. Choices which maximize expected utility are in this sense “optimal,” and thus another term for effective altruism is “<a href=\"http://lesswrong.com/lw/da4/what_is_optimal_philanthropy/\">optimal philanthropy</a>.” Note that effective altruism in this sense is not too dissimilar from earlier approaches to philanthropy, including <a href=\"http://en.wikipedia.org/wiki/High_impact_philanthropy\">high-impact philanthropy</a> (making “<a href=\"http://www.impact.upenn.edu/faq/\">the biggest difference possible, given the amount of capital invested</a>“), <a href=\"http://www.amphilsoc.org/sites/default/files/490202.pdf\">strategic philanthropy</a>, <a href=\"http://www.hudson.org/files/pdf_upload/Stanley_Katz_APS_Proceedings_Piece_June_2005.pdf\">effective philanthropy</a>, and <a href=\"http://wisephilanthropy.com/\">wise philanthropy</a>. Note also that effective altruism does not say that a philanthropic agent should specify complete utility and probability functions over outcomes and then compute the philanthropic act with the highest expected utility — that is impractical for bounded agents. We must keep in mind the distinction between normative, descriptive, and prescriptive models of decision-making (Baron 2007): “normative models tell us how to evaluate… decisions in terms of their departure from an ideal standard. Descriptive models specify what people in a particular culture actually do and how they deviate from the normative models. Prescriptive models are designs or inventions, whose purpose is to bring the results of actual thinking into closer conformity to the normative model.” The <em>prescriptive</em> question — about what bounded philanthropic agents should do to maximize expected utility with their philanthropic choices — tends to be extremely complicated, and is the subject of most of the research performed by the effective altruism community.</small></p>
<p><sup>3</sup> <small>See, for example: <a href=\"http://lesswrong.com/lw/37f/efficient_charity/\">Efficient Charity</a>, <a href=\"http://lesswrong.com/lw/3gj/efficient_charity_do_unto_others/\">Efficient Charity: Do Unto Others</a>, <a href=\"http://lesswrong.com/lw/2qq/politics_as_charity/\">Politics as Charity</a>, <a href=\"http://lesswrong.com/lw/aid/heuristics_and_biases_in_charity/\">Heuristics and Biases in Charity</a>, <a href=\"http://lesswrong.com/lw/2hv/public_choice_and_the_altruists_burden/\">Public Choice and the Altruist’s Burden</a>, <a href=\"http://lesswrong.com/lw/44c/on_charities_and_linear_utility/\">On Charities and Linear Utility</a>, <a href=\"http://lesswrong.com/lw/6py/optimal_philanthropy_for_human_beings/\">Optimal Philanthropy for Human Beings</a>, <a href=\"http://lesswrong.com/lw/6z/purchase_fuzzies_and_utilons_separately/\">Purchase Fuzzies and Utilons Separately</a>, <a href=\"http://lesswrong.com/lw/65/money_the_unit_of_caring/\">Money: The Unit of Caring</a>, <a href=\"http://lesswrong.com/lw/3kl/optimizing_fuzzies_and_utilons_the_altruism_chip/\">Optimizing Fuzzies and Utilons: The Altruism Chip Jar</a>, <a href=\"http://lesswrong.com/lw/684/efficient_philanthropy_local_vs_global_approaches/\">Efficient Philanthropy: Local vs. Global Approaches</a>, <a href=\"http://lesswrong.com/lw/2pq/the_effectiveness_of_developing_world_aid/\">The Effectiveness of Developing World Aid</a>, <a href=\"http://lesswrong.com/lw/2kh/against_cryonics_for_costeffective_charity/\">Against Cryonics & For Cost-Effective Charity</a>, <a href=\"http://lesswrong.com/lw/gzq/bayesian_adjustment_does_not_defeat_existential/\">Bayesian Adjustment Does Not Defeat Existential Risk Charity</a>, <a href=\"http://lesswrong.com/lw/373/how_to_save_the_world/\">How to Save the World</a>, and <a href=\"http://lesswrong.com/lw/da4/what_is_optimal_philanthropy/\">What is Optimal Philanthropy?</a></small></p>
<p><sup>4</sup> <small>I believe Beckstead and Bostrom have done the research community an enormous service in creating a <em>framework</em>, a <em>shared language</em>, for discussing trajectory changes, existential risks, and machine superintelligence. When discussing these topics with my colleagues, it has often been the case that the first hour of conversation is spent merely trying to understand what the other person is saying — how they are using the terms and concepts they employ. Beckstead’s and Bostrom’s recent work should enable clearer and more efficient communication between researchers, and therefore greater research productivity. Though I am not aware of any controlled, experimental studies on the effect of shared language on research productivity, a shared language is widely considered to be of great benefit for any field of research, and I shall provide a few examples of this claim which appear in print. <a href=\"http://atmos-chem-phys.net/6/2017/2006/acp-6-2017-2006.pdf\">Fuzzi et al. (2006)</a>: “The use of inconsistent terms can easily lead to misunderstandings and confusion in the communication between specialists from different [disciplines] of atmospheric and climate research, and may thus potentially inhibit scientiﬁc progress.” <a href=\"http://www.pik-potsdam.de/research/transdisciplinary-concepts-and-methods/projects/project-archive/favaia/pubs/hinkel-knowledge-integration.pdf\">Hinkel (2008)</a>: “Technical languages enable their users, e.g. members of a scientiﬁc discipline, to communicate eﬃciently about a domain of interest.” <a href=\"http://www.cs.cofc.edu/~bowring/classes/csis%20633/readings/madin-etal-tree-2008.pdf\">Madin et al. (2007)</a>: “terminological ambiguity slows scientiﬁc progress, leads to redundant research efforts, and ultimately impedes advances towards a uniﬁed foundation for ecological science.”</small></p>
<p><sup>5</sup> <small>In addition to Beckstead’s thesis, see also <a href=\"http://lesswrong.com/lw/hjb/a_proposed_adjustment_to_the_astronomical_waste/\">A Proposed Adjustment to the Astronomical Waste Argument</a>.</small></p>
<p><sup>6</sup> <small>Beckstead doesn’t mention this, but I would like to point out that moral realism is not required for Beckstead’s arguments to go through. In fact, I generally accept Beckstead’s arguments even though most philosophers would not consider me a moral realist, though to some degree that is a semantic debate (<a href=\"http://lesswrong.com/lw/5u2/pluralistic_moral_reductionism/\">Muehlhauser 2011</a>; <a href=\"http://www.victoria.ac.nz/staff/richard_joyce/acrobat/joyce_metaethical.pluralism.pdf\">Joyce 2012</a>). If you’re a moral realist and you believe your intuitive moral judgments are data about what is morally true, then Beckstead’s arguments (if successful) have something to say about what is morally true, and about what you should do if you want to act in morally good ways. If you’re a moral anti-realist but you think your intuitive judgments are data about what you value — or about what you would value if you had more time to think about your values and how to resolve the contradictions among them — then Beckstead’s arguments (if successful) have something to say about what you value, and about what you should do if you want to help achieve what you value.</small></p>
<p><sup>7</sup> <small>Karnofsky calls these “<a href=\"http://blog.givewell.org/2013/05/15/flow-through-effects/\">flow-through effects</a>.”</small></p>
<p><sup>8</sup> <small>See <a href=\"http://lesswrong.com/lw/bd6/ai_risk_opportunity_a_timeline_of_early_ideas_and/91zl\">Bostrom (forthcoming)</a> for an extended argument. Perhaps the most likely defeater for machine superintelligence is that global catastrophe may halt scientific progress before human-level AI is created.</small></p>
<p><sup>9</sup> <small>Beckstead, in personal communication, suggested (but didn’t necessarily endorse) the following formalization of the rough argument sketched in the main text of the blog post: “(1) To a first approximation, the future of humanity is all that matters. (2) To a much greater extent than anything else, the future of humanity is highly sensitive to how machine intelligence unfolds. (3) Therefore, there is a very strong presumption in favor of working on any project which makes machine intelligence unfold in a better way. (4) FAI research is the most promising route to making machine intelligence unfold in a better way. (5) Therefore, there is a very strong presumption in favor of doing FAI research.” <a href=\"https://sites.google.com/site/nbeckstead/research/Beckstead%2C%20Nick--On%20the%20Overwhelming%20Importance%20of%20Shaping%20the%20Far%20Future.pdf?attredirects=0&d=1\">Beckstead (2013)</a> examines the case for (1). <a href=\"http://lesswrong.com/lw/bd6/ai_risk_opportunity_a_timeline_of_early_ideas_and/91zl\">Bostrom (forthcoming)</a>, in large part, examines the case for (2). Premise (3) informally follows from (1) and (2), and the conclusion (5) informally follows from (3) and (4). Premise (4) appears to me to be the most dubious part of the argument, and the least explored in the extant literature.</small></p>
<p>The post <a href=\"http://intelligence.org/2013/06/05/friendly-ai-research-as-effective-altruism/\">Friendly AI Research as Effective Altruism</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/06/05/friendly-ai-research-as-effective-altruism/#comments" "f792fa2e4c21633edd56dfd2669717e0") (2 (20954 62970 749118) "http://intelligence.org/2013/05/30/miri-may-newsletter-intelligence-explosion-microeconomics-and-other-publications/?utm_source=rss&utm_medium=rss&utm_campaign=miri-may-newsletter-intelligence-explosion-microeconomics-and-other-publications" "MIRI May Newsletter: Intelligence Explosion Microeconomics and Other Publications" "jake" "Thu, 30 May 2013 18:48:11 +0000" "<p><!-- // End Template Preheader \\\\ --></p>
<table id=\"templateContainer\" style=\"border: 0px solid #e5e5e5; background-color: #ffffff;\" width=\"575\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">
<tbody>
<tr>
<td style=\"border-collapse: collapse;\" align=\"center\" valign=\"top\"><!-- // Begin Template Header \\\\ --></p>
<table id=\"templateHeader\" style=\"background-color: #ffffff; border-bottom: 0px none; padding: 0px;\" width=\"575\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">
<tbody>
<tr>
<td class=\"headerContent\" style=\"border-collapse: collapse; color: #ffffff; font-family: Arial; font-size: 34px; font-weight: bold; line-height: 100%; padding: 0; text-align: center; vertical-align: middle;\"><!-- // Begin Module: Standard Header Image \\\\ --><img id=\"headerImage campaign-icon\" style=\"border: 1px; border-style: none; border-width: px; height: auto; width: 575px; margin: 0; padding: 1px; max-width: 575; line-height: 100%; outline: none; text-decoration: none;\" alt=\"\" src=\"http://gallery.mailchimp.com/353906382677fa789a483ba9e/images/newsletterheader_sm_c.jpg\" width=\"575\" height=\"178\" border=\"0\" /><!-- // End Module: Standard Header Image \\\\ --></td>
</tr>
</tbody>
</table>
<p><!-- // End Template Header \\\\ --></td>
</tr>
<tr>
<td style=\"border-collapse: collapse;\" align=\"center\" valign=\"top\"><!-- // Begin Template Body \\\\ --></p>
<table id=\"templateBody\" width=\"575\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">
<tbody>
<tr>
<td style=\"border-collapse: collapse;\" valign=\"top\" width=\"575\">
<table width=\"575\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">
<tbody>
<tr>
<td style=\"border-collapse: collapse;\" valign=\"top\">
<table width=\"575\" border=\"0\" cellspacing=\"0\" cellpadding=\"0\">
<tbody>
<tr>
<td class=\"bodyContent\" style=\"border-collapse: collapse; background-color: #ffffff; border: 1px solid #e5e5e5;\" valign=\"top\"><!-- // Begin Module: Standard Content \\\\ --></p>
<table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"20\">
<tbody>
<tr>
<td style=\"border-collapse: collapse;\" valign=\"top\">
<div mc:repeatable=\"repeat_1\" mc:repeatindex=\"0\" style=\"color: #4a535d; font-family: sans-serif; font-size: 14px; line-height: 1.5em; text-align: left; border-bottom: solid 1px #e5e5e5; padding: 13px 24px 10px 23px;\">
<h2 class=\"h2 title\" style=\"color: #003b65; display: block; font-family: sans-serif; font-size: 22px; font-weight: bold; line-height: 100%; text-align: left; padding-top: 5px; margin-top: 0 !important; margin-right: 0 !important; margin-bottom: 10px !important; margin-left: 0 !important;\"><span class=\"mc-toc-title\">Greetings From the Executive Director</span></h2>
<p><img style=\"max-width: 160px; height: auto; line-height: 100%; outline: none; padding: 1px; text-decoration: none; display: inline; width: 165px; border: 1px; border-style: solid; border-width: 1px; border-color: Black;\" alt=\"\" src=\"http://intelligence.org/files/luke.jpeg\" width=\"165\" height=\"250\" align=\"right\" />Dear friends,</p>
<p>It’s been a busy month!</p>
<p>Mostly, we’ve been busy <em>publishing</em> things. As you’ll see below, <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://www.amazon.com/Singularity-Hypotheses-Scientific-Philosophical-Assessment/dp/3642325599/\" target=\"_self\"><em>Singularity Hypotheses</em></a> has now been published, and it includes four chapters by MIRI researchers or research associates. We’ve also published two new technical reports — one on decision theory and another on intelligence explosion microeconomics — and several new blog posts analyzing various issues relating to the future of AI. Finally, we added <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/2013/05/24/four-articles-added-to-research-page/\" target=\"_self\">four older articles</a> to the research page, including <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/files/IdealAdvisorTheories.pdf\" target=\"_self\">Ideal Advisor Theories and Personal CEV</a> (2012).</p>
<p>In our <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/2013/04/18/miri-april-newsletter-relaunch-celebration-and-a-new-math-result/\" target=\"_self\">April newsletter</a> we spoke about our April 11th party in San Francisco, celebrating our relaunch as the Machine Intelligence Research Institute and our transition to mathematical research. Additional photos from that event are now available as a <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"https://www.facebook.com/media/set/?set=a.520720301298692.1073741826.170446419659417&type=3\" target=\"_self\">Facebook photo album</a>. We’ve also uploaded a video from the event, in which I spend 2 minutes explaining MIRI’s relaunch and some tentative results from the April workshop. After that, visiting researcher <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://qchu.wordpress.com/\" target=\"_self\">Qiaochu Yuan</a> spends 4 minutes explaining one of MIRI’s core research questions: the Löbian obstacle to self-modifying systems.</p>
<p>Some of the research from our April workshop will be published in June, so if you’d like to read about those results right away, you might like to <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/blog/\" target=\"_self\">subscribe</a> to <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/blog\" target=\"_self\">our blog</a>.</p>
<p>Cheers!</p>
<p>Luke Muehlhauser</p>
<p>Executive Director</p>
<p><span id=\"more-10217\"></span></p>
</div>
<div mc:repeatable=\"repeat_1\" mc:repeatindex=\"1\" style=\"color: #4a535d; font-family: sans-serif; font-size: 14px; line-height: 1.5em; text-align: left; border-bottom: solid 1px #e5e5e5; padding: 13px 24px 10px 23px;\">
<h2 class=\"h2 title\" style=\"color: #003b65; display: block; font-family: sans-serif; font-size: 22px; font-weight: bold; line-height: 100%; text-align: left; padding-top: 5px; margin-top: 0 !important; margin-right: 0 !important; margin-bottom: 10px !important; margin-left: 0 !important;\"><span class=\"mc-toc-title\">Intelligence Explosion Microeconomics</span></h2>
<p>Our largest new publication this month is Yudkowsky’s 91-page <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/files/IEM.pdf\" target=\"_self\">Intelligence Explosion Microeconomics</a> (discuss <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://lesswrong.com/lw/hbd/new_report_intelligence_explosion_microeconomics/\" target=\"_self\">here</a>). In this article, Yudkowsky takes some initial steps toward tackling the key quantitative issue in the intelligence explosion, “reinvestable returns on cognitive investments”: what kind of returns can you get from an investment in cognition, can you reinvest it to make yourself even smarter, and does this process die out or blow up? The article can be thought of as a compact and hopefully more coherent successor to the <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://wiki.lesswrong.com/wiki/The_Hanson-Yudkowsky_AI-Foom_Debate\" target=\"_self\">AI Foom Debate</a> of 2009, featuring Yudkowsky and GMU economist <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://hanson.gmu.edu/\" target=\"_self\">Robin Hanson</a>.</p>
<p>Here is the abstract:</p>
<blockquote><p>I. J. Good’s thesis of the ‘intelligence explosion’ is that a sufficiently advanced machine intelligence could build a smarter version of itself, which could in turn build an even smarter version of itself, and that this process could continue enough to vastly exceed human intelligence. As Sandberg (2010) correctly notes, there are several attempts to lay down return-on-investment formulas intended to represent sharp speedups in economic or technological growth, but very little attempt has been made to deal formally with I. J. Good’s intelligence explosion thesis as such.</p>
<p>I identify the key issue as <em>returns on cognitive reinvestment</em> — the ability to invest more computing power, faster computers, or improved cognitive algorithms to yield cognitive labor which produces larger brains, faster brains, or better mind designs. There are many phenomena in the world which have been argued as evidentially relevant to this question, from the observed course of hominid evolution, to Moore’s Law, to the competence over time of machine chess-playing systems, and many more. I go into some depth on the sort of debates which then arise on how to interpret such evidence. I propose that the next step forward in analyzing positions on the intelligence explosion would be to formalize return-on-investment curves, so that each stance can say formally which possible microfoundations they hold to be <em>falsified</em> by historical observations already made. More generally, I pose multiple open questions of ‘returns on cognitive reinvestment’ or ‘intelligence explosion microeconomics’. Although such questions have received little attention thus far, they seem highly relevant to policy choices affecting the outcomes for Earth-originating intelligent life.</p></blockquote>
<p>The dedicated mailing list will be small and restricted to technical discussants: apply for it <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"https://docs.google.com/forms/d/1KElE2Zt_XQRqj8vWrc_rG89nrO4JtHWxIFldJ3IY_FQ/viewform\" target=\"_self\">here</a>.</p>
</div>
<div mc:repeatable=\"repeat_1\" mc:repeatindex=\"2\" style=\"color: #4a535d; font-family: sans-serif; font-size: 14px; line-height: 1.5em; text-align: left; border-bottom: solid 1px #e5e5e5; padding: 13px 24px 10px 23px;\">
<h2 class=\"h2 title\" style=\"color: #003b65; display: block; font-family: sans-serif; font-size: 22px; font-weight: bold; line-height: 100%; text-align: left; padding-top: 5px; margin-top: 0 !important; margin-right: 0 !important; margin-bottom: 10px !important; margin-left: 0 !important;\">When Will AI Be Created?</h2>
<p>In part, intelligence explosion microeconomics seeks to answer the question “How quickly will human-level AI self-improve to become superintelligent?” Another major question in AI forecasting is, of course: “When will we create human-level AI?”</p>
<p>This is another difficult question, and Luke Muehlhauser surveyed those difficulties in a recent (and quite detailed) blog post: <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/2013/05/15/when-will-ai-be-created/\" target=\"_self\">When Will AI Be Created?</a> He concludes:</p>
<blockquote><p>Given these considerations, I think the most appropriate stance on the question “When will AI be created?” is something like this:</p>
<p>“We can’t be confident AI will come in the next 30 years, and we can’t be confident it’ll take more than 100 years, and anyone who is confident of either claim is pretending to know too much.”</p>
<p>How confident is “confident”? Let’s say 70%. That is, I think it is unreasonable to be 70% confident that AI is fewer than 30 years away, and I also think it’s unreasonable to be 70% confident that AI is more than 100 years away.</p>
<p>This statement admits my inability to predict AI, but it also constrains my probability distribution over “years of AI creation” quite a lot.</p>
<p>I think the considerations above justify these constraints on my probability distribution, but I haven’t spelled out my reasoning in great detail. That would require more analysis than I can present here. But I hope I’ve at least summarized the basic considerations on this topic, and those with different probability distributions than mine can now build on my work here to try to justify them.</p></blockquote>
<p>Muehlhauser also explains four methods for reducing our uncertainty about AI timelines: <em>explicit quantification</em>, <em>leveraging aggregation</em>, <em>signposting the future</em>, and <em>decomposing the phenomena</em>.</p>
<p>As it turns out, <strong>you can participate</strong> in the first two methods for improving our AI forecasts by <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/2013/05/24/sign-up-for-daggre-to-improve-science-technology-forecasting/\" target=\"_self\">signing up for GMU’s DAGGRE program</a>. Muehlhauser himself has signed up.</p>
<p>Muehlhauser also wrote a 400-word piece on the difficulty of AI forecasting for <em>Quartz</em> magazine: <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://qz.com/85825/robots-may-take-our-jobs-but-its-hard-to-say-when/\" target=\"_self\">Robots will take our jobs, but it’s hard to say when</a>. Here’s a choice quote:</p>
<blockquote><p>We’ve had the computing power of a honeybee’s brain for quite a while now, but that doesn’t mean we know how to build tiny robots that fend for themselves outside the lab, find their own sources of energy, and communicate with others to build their homes in the wild.</p></blockquote>
<p> </p>
</div>
<div mc:repeatable=\"repeat_1\" mc:repeatindex=\"3\" style=\"color: #4a535d; font-family: sans-serif; font-size: 14px; line-height: 1.5em; text-align: left; border-bottom: solid 1px #e5e5e5; padding: 13px 24px 10px 23px;\">
<h2 class=\"h2 title\" style=\"color: #003b65; display: block; font-family: sans-serif; font-size: 22px; font-weight: bold; line-height: 100%; text-align: left; padding-top: 5px; margin-top: 0 !important; margin-right: 0 !important; margin-bottom: 10px !important; margin-left: 0 !important;\"><span class=\"mc-toc-title\">Singularity Hypothesis Published</span></h2>
<p><a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://www.amazon.com/Singularity-Hypotheses-Scientific-Philosophical-Assessment/dp/3642325599/\"><img style=\"max-width: 160px; height: auto; line-height: 100%; outline: none; padding: 1px; text-decoration: none; display: inline; width: 131px; border: 1px; border-style: solid; border-width: 1px; border-color: Black;\" alt=\"\" src=\"http://intelligence.org/wp-content/uploads/2013/04/singularity-hypotheses.jpg\" width=\"131\" height=\"200\" align=\"right\" /></a></p>
<p><a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://www.amazon.com/Singularity-Hypotheses-Scientific-Philosophical-Assessment/dp/3642325599/\" target=\"_self\"><em>Singularity Hypotheses: A Scientific and Philosophical Assessment</em></a> has now been published by Springer, in hardcover and ebook forms.</p>
<p>The book contains 20 chapters about the prospect of machine superintelligence, including 4 chapters by MIRI researchers and research associates:</p>
<ul>
<li><a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/files/IE-EI.pdf\" target=\"_self\">Intelligence Explosion: Evidence and Import</a> by Luke Muehlhauser and Anna Salamon</li>
<li><a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/files/IE-ME.pdf\" target=\"_self\">Intelligence Explosion and Machine Ethics</a> by Luke Muehlhauser and Louie Helm</li>
<li>Friendly Artificial Intelligence by Eliezer Yudkowsky, a shortened version of <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/files/AIPosNegFactor.pdf\" target=\"_self\">Yudkowsky (2008)</a></li>
<li><a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/files/AGI-HMM.pdf\" target=\"_self\">Artificial General Intelligence and the Human Mental Model</a> by Roman Yampolskiy and (MIRI research associate) Joshua Fox</li>
</ul>
<p>For more details, see <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/2013/04/25/singularity-hypotheses-published/\" target=\"_self\">the blog post</a>.</p>
</div>
<div mc:repeatable=\"repeat_1\" mc:repeatindex=\"4\" style=\"color: #4a535d; font-family: sans-serif; font-size: 14px; line-height: 1.5em; text-align: left; border-bottom: solid 1px #e5e5e5; padding: 13px 24px 10px 23px;\">
<h2 class=\"h2 title\" style=\"color: #003b65; display: block; font-family: sans-serif; font-size: 22px; font-weight: bold; line-height: 100%; text-align: left; padding-top: 5px; margin-top: 0 !important; margin-right: 0 !important; margin-bottom: 10px !important; margin-left: 0 !important;\"><span class=\"mc-toc-title\">Timeless Decision Theory Paper Published</span></h2>
<p><a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/files/Comparison.pdf\"><img style=\"max-width: 160px; height: auto; line-height: 100%; outline: none; padding: 1px; text-decoration: none; display: inline; width: 203px; border: 1px; border-style: solid; border-width: 1px; border-color: Black;\" alt=\"\" src=\"http://intelligence.org/wp-content/uploads/2013/04/Altair-paper-front.png\" width=\"203\" height=\"200\" align=\"right\" /></a> During his time as a research fellow for MIRI, Alex Altair wrote an article on <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://wiki.lesswrong.com/wiki/Timeless_decision_theory\" target=\"_self\">Timeless Decision Theory</a> (TDT) that has now been published: “<a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/files/Comparison.pdf\" target=\"_self\">A Comparison of Decision Algorithms on Newcomblike Problems</a>.”</p>
<p>Altair’s article is both more succinct and also more precise in its formulation of TDT than Yudkowsky’s earlier paper “<a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/files/TDT.pdf\" target=\"_self\">Timeless Decision Theory</a>.” Thus, Altair’s paper should serve as a handy introduction to TDT for philosophers, computer scientists, and mathematicians, while Yudkowsky’s paper remains required reading for anyone interested to develop TDT further, for it covers more ground than Altair’s paper.</p>
<p>For a gentle introduction to the entire field of normative decision theory (including TDT), see Muehlhauser and Williamson’s <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://lesswrong.com/lw/gu1/decision_theory_faq/\" target=\"_self\">Decision Theory FAQ</a>.</p>
</div>
<div mc:repeatable=\"repeat_1\" mc:repeatindex=\"5\" style=\"color: #4a535d; font-family: sans-serif; font-size: 14px; line-height: 1.5em; text-align: left; border-bottom: solid 1px #e5e5e5; padding: 13px 24px 10px 23px;\">
<h2 class=\"h2 title\" style=\"color: #003b65; display: block; font-family: sans-serif; font-size: 22px; font-weight: bold; line-height: 100%; text-align: left; padding-top: 5px; margin-top: 0 !important; margin-right: 0 !important; margin-bottom: 10px !important; margin-left: 0 !important;\"><span class=\"mc-toc-title\">AGI Impacts Experts and Friendly AI Experts</span></h2>
<p>In <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/2013/05/01/agi-impacts-experts-and-friendly-ai-experts/\" target=\"_self\">AGI Impacts Experts and Friendly AI Experts</a>, Luke Muehlhauser explains the two types of experts MIRI hopes to cultivate.</p>
<p><em>AGI impacts experts</em> develop skills related to predicting technological development (e.g. building <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/files/ChangingTheFrame.pdf\">computational models</a> of AI development or reasoning about <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/files/IEM.pdf\">intelligence explosion microeconomics</a>), predicting AGI’s likely impacts on society, and identifying which interventions are most likely to increase humanity’s chances of safely navigating the creation of AGI. For overviews, see <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/files/EthicsofAI.pdf\">Bostrom & Yudkowsky (2013)</a>; <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/files/IE-EI.pdf\">Muehlhauser & Salamon (2013)</a>.</p>
<p><em>Friendly AI experts</em> develop skills useful for the development of mathematical architectures that can enable AGIs to be <em>trustworthy</em> (or “human-friendly”). This work is carried out at <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/2013/03/07/upcoming-miri-research-workshops/\">MIRI research workshops</a> and in various publications, e.g. <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://lesswrong.com/lw/h1k/reflection_in_probabilistic_set_theory/\">Christiano et al. (2013)</a>; <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://arxiv.org/pdf/1111.3934v2.pdf\">Hibbard (2013)</a>. Note that the term “Friendly AI” was selected (in part) to avoid the suggestion that we understand the subject very well — a phrase like “Ethical AI” might sound like the kind of thing one can learn a lot about by looking it up in an encyclopedia, but our present understanding of trustworthy AI is too impoverished for that.</p>
<p>For more details on which skills these kinds of experts should develop, <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/2013/05/01/agi-impacts-experts-and-friendly-ai-experts/\" target=\"_self\">read the blog post</a>.</p>
</div>
<div mc:repeatable=\"repeat_1\" mc:repeatindex=\"6\" style=\"color: #4a535d; font-family: sans-serif; font-size: 14px; line-height: 1.5em; text-align: left; border-bottom: solid 1px #e5e5e5; padding: 13px 24px 10px 23px;\">
<h2 class=\"h2 title\" style=\"color: #003b65; display: block; font-family: sans-serif; font-size: 22px; font-weight: bold; line-height: 100%; text-align: left; padding-top: 5px; margin-top: 0 !important; margin-right: 0 !important; margin-bottom: 10px !important; margin-left: 0 !important;\"><span class=\"mc-toc-title\">MIRI’s Mission in Five Theses and Two Lemmas</span></h2>
<p>Yudkowsky sums up MIRI’s research mission in the blog post <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/2013/05/05/five-theses-two-lemmas-and-a-couple-of-strategic-implications/\" target=\"_self\">Five theses, two lemmas, and a couple of strategic implications</a>. The five theses are:</p>
<ul>
<li>Intelligence explosion</li>
<li>Orthogonality of intelligence and goals</li>
<li>Convergent instrumental goals</li>
<li>Complexity of value</li>
<li>Fragility of value</li>
</ul>
<p>According to Yudkowsky, these theses imply two important lemmas:</p>
<ul>
<li>Indirect normativity</li>
<li>Large bounded extra difficulty of Friendliness</li>
</ul>
<p>In turn, these two lemmas have two important strategic implications:</p>
<ol>
<li>We have a lot of work to do on things like indirect normativity and stable self-improvement. At this stage a lot of this work looks really foundational — that is, we can’t describe how to do these things using infinite computing power, let alone finite computing power.  We should get started on this work as early as possible, since basic research often takes a lot of time.</li>
<li>There needs to be a Friendly AI project that has some sort of boost over competing projects which don’t live up to a (very) high standard of Friendly AI work — a project which can successfully build a stable-goal-system self-improving AI, before a less-well-funded project hacks together a much sloppier self-improving AI. Giant supercomputers may be less important to this than being able to bring together the smartest researchers… but the required advantage cannot be left up to chance. Leaving things to default means that projects less careful about self-modification would have an advantage greater than casual altruism is likely to overcome.</li>
</ol>
<p>For more details on the theses and lemmas, <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/2013/05/05/five-theses-two-lemmas-and-a-couple-of-strategic-implications/\" target=\"_self\">read the blog post</a> and its linked articles.</p>
</div>
<div mc:repeatable=\"repeat_1\" mc:repeatindex=\"7\" style=\"color: #4a535d; font-family: sans-serif; font-size: 14px; line-height: 1.5em; text-align: left; border-bottom: solid 1px #e5e5e5; padding: 13px 24px 10px 23px;\">
<h2 class=\"h2 title\" style=\"color: #003b65; display: block; font-family: sans-serif; font-size: 22px; font-weight: bold; line-height: 100%; text-align: left; padding-top: 5px; margin-top: 0 !important; margin-right: 0 !important; margin-bottom: 10px !important; margin-left: 0 !important;\"><span class=\"mc-toc-title\">Our Final Invention available for preorder</span></h2>
<p><a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://www.amazon.com/Our-Final-Invention-Artificial-Intelligence/dp/0312622376/\"><img style=\"max-width: 160px; height: auto; line-height: 100%; outline: none; padding: 1px; text-decoration: none; display: inline; width: 131px; border: 1px; border-style: solid; border-width: 1px; border-color: Black;\" alt=\"\" src=\"http://ecx.images-amazon.com/images/I/71t4FkxPNqL._SL1500_.jpg\" width=\"131\" height=\"200\" align=\"right\" /></a><a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://www.jamesbarrat.com/author/\" target=\"_self\">James Barrat</a>, a documentary filmmaker for National Geographic, Discovery, PBS, and other broadcasters, has written a wonderful new book about the intelligence explosion called <em>Our Final Invention: Artificial Intelligence and the End of the Human Era</em>. It will be released October 1st, and is <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://www.amazon.com/Our-Final-Invention-Artificial-Intelligence/dp/0312622376/?tag=r601000000-20\" target=\"_self\">available for pre-order on Amazon</a>.</p>
<p>Here are some blurbs from people who have read an advance copy:</p>
<p>“A hard-hitting book about the most important topic of this century and possibly beyond — the issue of whether our species can survive. I wish it was science fiction but I know it’s not.”</p>
<p>—Jaan Tallinn, co-founder of Skype</p>
<p>“The compelling story of humanity’s most critical challenge. A Silent Spring for the 21st Century.”</p>
<p>—Michael Vassar, former MIRI president</p>
<p>“<em>Our Final Invention</em> is a thrilling detective story, and also the best book yet written on the most important problem of the 21st century.”</p>
<p>—Luke Muehlhauser, MIRI executive director</p>
<p>“An important and disturbing book.”</p>
<p>—Huw Price, co-founder, Cambridge University Center for the Study of Existential Risk</p>
</div>
<div mc:repeatable=\"repeat_1\" mc:repeatindex=\"8\" style=\"color: #4a535d; font-family: sans-serif; font-size: 14px; line-height: 1.5em; text-align: left; border-bottom: solid 1px #e5e5e5; padding: 13px 24px 10px 23px;\">
<h2 class=\"h2 title\" style=\"color: #003b65; display: block; font-family: sans-serif; font-size: 22px; font-weight: bold; line-height: 100%; text-align: left; padding-top: 5px; margin-top: 0 !important; margin-right: 0 !important; margin-bottom: 10px !important; margin-left: 0 !important;\"><span class=\"mc-toc-title\">MIRI Needs Advisors</span></h2>
<p>MIRI currently has a few dozen volunteer advisors on a wide range of subjects, but we need more! We’re especially hoping for additional advisors in mathematical logic, theoretical computer science, artificial intelligence, economics, and game theory.</p>
<p>If you <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/2013/05/15/advise-miri-with-your-domain-specific-expertise/\" target=\"_self\"><strong>sign up</strong></a>, we will occasionally ask you questions, or send you early drafts of upcoming writings for feedback.</p>
<p>We don’t always want technical advice (“Well, you can do that with a relativized arithmetical hierarchy…”); often, we just want to understand how different groups of experts respond to our writing (“The tone of this paragraph rubs me the wrong way because…”).</p>
<p>Even if you don’t have much time to help, <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://intelligence.org/2013/05/15/advise-miri-with-your-domain-specific-expertise/\" target=\"_self\"><strong>please sign up</strong></a>! We will of course respect your own limits on availability.</p>
</div>
<div mc:repeatable=\"repeat_1\" mc:repeatindex=\"9\" style=\"color: #4a535d; font-family: sans-serif; font-size: 14px; line-height: 1.5em; text-align: left; border-bottom: solid 1px #e5e5e5; padding: 13px 24px 10px 23px;\">
<h2 class=\"h2 title\" style=\"color: #003b65; display: block; font-family: sans-serif; font-size: 22px; font-weight: bold; line-height: 100%; text-align: left; padding-top: 5px; margin-top: 0 !important; margin-right: 0 !important; margin-bottom: 10px !important; margin-left: 0 !important;\">Featured Volunteer – Florian Blumm</h2>
<p><img style=\"max-width: 160px; height: auto; line-height: 100%; outline: none; padding: 1px; text-decoration: none; display: inline; width: 184px; border: 1px; border-style: solid; border-width: 1px; border-color: Black;\" alt=\"\" src=\"http://gallery.mailchimp.com/353906382677fa789a483ba9e/images/tauchen2.jpg\" width=\"184\" height=\"200\" align=\"right\" />Florian Bumm has been one of our most active translators. Florian translates materials from English into his native tongue, German. Historically a software engineer, he is now on a traveling vacation in Bolivia progressively extended by remote contract labor, which he has found conducive to his volunteering for MIRI. After leaving a position as a Java engineer for a financial services company, he has decided that he would rather contribute directly to a cause of some sort, and has determined that there is nothing more important than mitigating existential risks from artificial intelligence.</p>
<p>Thanks, Florian!</p>
<p>To join Florian and dozens of other volunteers, visit <a style=\"color: #4b71a7; text-decoration: none; font-weight: normal;\" href=\"http://mirivolunteers.org/\" target=\"_self\">MIRIvolunteers.org</a>.</p>
</div>
</td>
</tr>
</tbody>
</table>
<p><!-- // End Module: Standard Content \\\\ --></td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>
</td>
</tr>
</tbody>
</table>
<p><!-- // End Template Body \\\\ --></td>
</tr>
<tr>
<td style=\"border-collapse: collapse;\" align=\"center\" valign=\"top\"><!-- // Begin Template Footer \\\\ --></p>
<table id=\"templateFooter\" style=\"background-color: #aeb6c3; border-top: 0;\" width=\"575\" border=\"0\" cellspacing=\"0\" cellpadding=\"10\">
<tbody>
<tr>
<td class=\"footerContent\" style=\"border-collapse: collapse;\" valign=\"top\"><!-- // Begin Module: Standard Footer \\\\ --></p>
<table width=\"100%\" border=\"0\" cellspacing=\"0\" cellpadding=\"10\">
<tbody>
<tr>
<td id=\"social\" style=\"border-collapse: collapse; background-color: #aeb6c3; border: 0;\" colspan=\"2\" valign=\"middle\"></td>
</tr>
<tr>
<td style=\"border-collapse: collapse;\" valign=\"top\" width=\"350\">
<div style=\"color: #707070; font-family: Arial; font-size: 12px; line-height: 125%; text-align: left;\">
<p>You’re receiving this because you have subscribed to content from the Machine Intelligence Research Institute.</p>
<p><em>Copyright © Machine Intelligence Research Institute, All rights reserved.</em></p>
</div>
</td>
<td id=\"monkeyRewards\" style=\"border-collapse: collapse;\" valign=\"top\" width=\"190\"></td>
</tr>
<tr>
<td id=\"utility\" style=\"border-collapse: collapse; background-color: #aeb6c3; border: 0;\" colspan=\"2\" valign=\"middle\"></td>
</tr>
</tbody>
</table>
<p><!-- // End Module: Standard Footer \\\\ --></td>
</tr>
</tbody>
</table>
<p><!-- // End Template Footer \\\\ --></td>
</tr>
</tbody>
</table>
<p>The post <a href=\"http://intelligence.org/2013/05/30/miri-may-newsletter-intelligence-explosion-microeconomics-and-other-publications/\">MIRI May Newsletter: Intelligence Explosion Microeconomics and Other Publications</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/05/30/miri-may-newsletter-intelligence-explosion-microeconomics-and-other-publications/#comments" "7cc602d3bd8b10e26c4cbe521833f0ed") (1 (20954 62970 744909) "http://intelligence.org/2013/05/29/new-transcript-yudkowsky-and-aaronson/?utm_source=rss&utm_medium=rss&utm_campaign=new-transcript-yudkowsky-and-aaronson" "New Transcript: Yudkowsky and Aaronson" "Luke Muehlhauser" "Thu, 30 May 2013 03:21:29 +0000" "<p><a href=\"https://docs.google.com/document/d/1JIqzTGNvdLukR0Ce5T2eiv_vxtPO53N3KGCbxVvREtU/pub\"><img class=\"aligncenter size-full wp-image-10214\" alt=\"Yudkowsky-Aaronson\" src=\"http://intelligence.org/wp-content/uploads/2013/05/Yudkowsky-Aaronson.png\" width=\"454\" height=\"289\" /></a></p>
<p>In <a href=\"http://intelligence.org/2013/05/15/when-will-ai-be-created/\">When Will AI Be Created?</a>, I referred to a <a href=\"http://bloggingheads.tv/videos/2220\">bloggingheads.tv conversation</a> between Eliezer Yudkowsky and Scott Aaronson. A transcript of that dialogue is <a href=\"https://docs.google.com/document/d/1JIqzTGNvdLukR0Ce5T2eiv_vxtPO53N3KGCbxVvREtU/pub\">now available</a>, thanks to MIRI volunteers Ethan Dickinson, Daniel Kokotajlo, and Rick Schwall.</p>
<p>See also <a href=\"http://intelligence.org/2013/01/09/new-transcript-eliezer-yudkowsky-and-massimo-pigliucci-on-the-singularity/\">the transcript</a> for a <a href=\"http://bloggingheads.tv/videos/2561\">bloggingheads.tv conversation</a> between Eliezer Yudkowsky and Massimo Pigliucci.</p>
<p>To join these volunteers in assisting our cause, visit <a href=\"http://mirivolunteers.org/\">MIRIvolunteers.org</a>!</p>
<p>The post <a href=\"http://intelligence.org/2013/05/29/new-transcript-yudkowsky-and-aaronson/\">New Transcript: Yudkowsky and Aaronson</a> appeared first on <a href=\"http://intelligence.org\">Machine Intelligence Research Institute</a>.</p>" nil "http://intelligence.org/2013/05/29/new-transcript-yudkowsky-and-aaronson/#comments" "9eaa76e114204e134c9c3bbacd5c579e")))