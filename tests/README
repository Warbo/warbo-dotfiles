# Simple test suite #

I use this as a high-level test harness, testing as many of my projects as
possible; like a poor man's continuous integration server.

It's really simple:

 - Anything in scripts/ is a test
 - A test passes if its exit code is zero
 - A test fails if its exit code is non-zero
 - To execute a single test, just run it, since they're just regular scripts
 - To run the suite, run ./once.sh

The once.sh script is meant to be simple, so it doesn't take any parameters.
It behaviour is this:

 - Each non-empty line in results/failures is a "previous failure"
    - A previous failure denotes the filename of something in scripts/
    - If previous failures are found, we will only test those
    - If no previous failures are found, we will test everything in scripts/
 - Run each test "foo", with stderr piped to "results/foo.stderr" and stdout
   piped to "results/foo.stdout"
 - If any test fails, our overall exit code is 1; else it is 0
 - During testing, a status message is written to status.txt

We "back up" the contents of results/ (failures and the stderr and stdout files)
so we can read the previous results while a test is still running. Backing up
"foo" means deleting "foo.old", if it exists, then moving "foo" to "foo.old".

Some things to notice about the behaviour of ./once.sh and results/failures:

 - By writing a set of filenames to results/failures, we can have ./once.sh run
   just that set
 - To see if a new test works, without having to re-test the world, just append
   its name to results/failures
 - Just because ./once.sh exits successfully doesn't mean our whole suite passes
   since it may have only run the previous failures. Two successes in a row
   definitely means a passing suite.

Also note that tests will be run in lexicographic order, so we can prioritise
some over others; usually in a case of "fail fast".

Existing tests roughly follow this ordering:

 - Fast tests should be higher than slow tests, so we can fail fast
 - Unstable versions should be higher than stable versions, as they're more
   likely to change, we're more likely to be working with them, and that's where
   we'll make any fixes (producing a replacement stable version)
 - Tests for my projects should be higher than other peoples', since they're
   more likely to fail due to me making changes
 - Tests for programs I use regularly should be higher than those I merely
   maintain, since maintenance requires my tools to be working!
